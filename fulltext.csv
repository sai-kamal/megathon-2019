,paper text
0,"A Probabilistic Programming Approach To Probabilistic Data Analysis Feras Saad MIT Probabilistic Computing Project fsaad@mit.edu Vikash Mansinghka MIT Probabilistic Computing Project vkm@mit.edu  with nonparametric Bayes in 50 lines of probabilistic code. Second, it reports the lines of code and accuracy of CGPMs compared with baseline solutions from standard machine learning libraries. 1 Introduction Probabilistic techniques are central to data analysis, but can be difficult to apply, combine, and compare. Such difficulties arise because families of approaches such as parametric statistical modeling, machine learning and probabilistic programming are each associated with different formalisms and assumptions. The contributions of this paper are (i) a way to address these challenges by defining CGPMs, a new family of composable probabilistic models; (ii) an integration of this family into BayesDB [10], a probabilistic programming platform for data analysis; and (iii) empirical illustrations of the efficacy of the framework for analyzing a real-world database of Earth satellites. We introduce composable generative population models (CGPMs), a computational formalism that generalizes directed graphical models. CGPMs specify a table of observable random variables with a finite number of columns and countably infinitely many rows. They support complex intra-row dependencies among the observables, as well as inter-row dependencies among a field of latent random variables. CGPMs are described by a computational interface for generating samples and evaluating densities for random variables derived from the base table by conditioning and marginalization. This paper shows how to package discriminative statistical learning techniques, dimensionality reduction methods, arbitrary probabilistic programs, and their combinations, as CGPMs. We also describe algorithms and illustrate new syntaxes in the probabilistic Metamodeling Language for building composite CGPMs that can interoperate with BayesDB. The practical value is illustrated in two ways. First, we describe a 50-line analysis that identifies satellite data records that probably violate their theoretical orbital characteristics. The BayesDB script builds models that combine non-parametric Bayesian structure learning with a causal probabilistic program that implements a stochastic variant of Kepler?s Third Law. Second, we illustrate coverage and conciseness of the CGPM abstraction by quantifying the improvement in accuracy and reduction in lines of code achieved on a representative data analysis task. 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. 2 Composable Generative Population Models A composable generative population model represents a data generating process for an exchangeable sequence of random vectors (x1 , x2 , . . . ), called a population. Each member xr is T -dimensional, and element x[r,t] takes values in an observation space Xt , for t ? [T ] and r ? N. A CGPM G is formally represented by a collection of variables that characterize the data generating process: G = (?, ?, Z = {zr : r ? N}, X = {xr : r ? N}, Y = {yr : r ? N}). ? ?: Known, fixed quantities about the population, such as metadata and hyperparameters. ? ?: Population-level latent variables relevant to all members of the population. ? zr = (z[r,1] , . . . z[r,L] ): Member-specific latent variables that govern only member r directly. ? xr = (x[r,1] , . . . x[r,T ] ): Observable output variables for member r. A subset of these variables may be observed and recorded in a dataset D. ? yr = (y[r,1] , . . . y[r,I] ): Input variables, such as ?feature vectors? in a purely discriminative model. A CGPM is required to satisfy the following conditional independence constraint: ?r 6= r0 ? N, ?t, t0 ? [T ] : x[r,t] ?? x[r0 ,t0 ] | {?, ?, zr , zr0 }. (1) Eq (1) formalizes the notion that all dependencies across members r ? N are completely mediated by the population parameters ? and member-specific variables zr . However, elements x[r,i] and x[r,j] within a member are generally free to assume any dependence structure. Similarly, the memberspecific latents in Z may be either uncoupled or highly-coupled given population parameters ?. CGPMs differ from the standard mathematical definition of a joint density in that they are defined in terms of a computational interface (Listing 1). As computational objects, they explicitly distinguish between the sampler for the random variables from their joint distribution, and the assessor of their joint density. In particular, a CGPM is required to sample/assess the joint distribution of a subset of output variables x[r,Q] conditioned on another subset x[r,E] , and marginalizing over x[r,[T ]\(Q?E)] . Listing 1 Computational interface for composable generative population models. ? s ? simulate (G, member: r, query: Q = {qk }, evidence : x[r,E] , input : yr ) Generate a sample from the distribution s ?G x[r,Q] |{x[r,E] , yr , D}. ? c ? logpdf (G, member: r, query : x[r,Q] , evidence : x[r,E] , input : yr ) Evaluate the log density log pG (x[r,Q] |{x[r,E] , yr , D}). ? G 0 ? incorporate (G, measurement : x[r,t] or yr ) Record a measurement x[r,t] ? Xt (or yr ) into the dataset D. ? G 0 ? unincorporate (G, member : r) Eliminate all measurements of input and output variables for member r. ? G 0 ? infer (G, program : T ) Adjust internal latent state in accordance with the learning procedure specified by program T . 2.1 Primitive univariate CGPMs and their statistical data types The statistical data type (Figure 1) of a population variable xt generated by a CGPM provides a more refined taxonomy than its ?observation space? Xt . The (parameterized) support of a statistical type is the set in which samples from simulate take values. Each statistical type is also associated with a base measure which ensures logpdf is well-defined. In high-dimensional populations with heterogeneous types, logpdf is taken against the product measure of these base measures. The statistical type also identifies invariants that the variable maintains. For instance, the values of a NOMINAL variable are permutation-invariant. Figure 1 shows statistical data types provided by the Metamodeling Language from BayesDB. The final column shows some examples of primitive CGPMs that are compatible with each statistical type; they implement logpdf directly using univariate probability density functions, and algorithms for simulate are well known [4]. For infer their parameters may be fixed, or learned from data using, e.g., maximum likelihood [2, Chapter 7] or Bayesian priors [5]. We refer to an extended version of this paper [14, Section 3] for using these primitives to implement CGPMs for a broad collection of model classes, including non-parametric Bayes, nearest neighbors, PCA, discriminative machine learning, and multivariate kernel methods. 2 Statistical Data Type BINARY NOMINAL COUNT/RATE CYCLIC MAGNITUDE NUMERICAL NUMERICAL-RANGED Nominal Parameters symbols: S base: b period: p ? ? low: l, high:h Count Measure/?-Algebra {0, 1} {0 . . . S?1} {0, 1b , 2b , . . .} (0, p) (0, ?) (??, ?) (l, h) ? R Magnitude Poisson Geometric {0,1} (#, 2 ) (#, 2[S] ) (#, 2N ) (?, B(R)) (?, B(R)) (?, B(R)) (?, B(R)) BERNOULLI CATEGORICAL POISSON, GEOMETRIC VON-MISES LOGNORMAL, EXPON NORMAL BETA, NORMAL-TRUNC Cyclic Numerical Von-Mises Lognormal Exponential Primitive CGPM Numerical-Ranged Normal NormalTrunc Beta Frequency Categorical Support Figure 1: Statistical data types for population variables generated by CGPMs available in the BayesDB Metamodeling Language, and samples from their marginal distributions. 2.2 Implementing general CGPMs as probabilistic programs in VentureScript In this section, we show how to implement simulate and logpdf (Listing 1) for composable generative models written in VentureScript [8], a probabilistic programming language with programmable inference. For simplicity, this section assumes a stronger conditional independence constraint, ?l, l0 ? [L] such that (r, t) 6= (r0 , t0 ) =? x[r,t] ?? x[r0 ,t0 ] | {?, ?, z[r,l] , z[r0 ,l0 ] , yr , yr0 }. (2) In words, for every observable element x[r,t] , there exists a latent variable z[r,l] which (in addition to ?) mediates all coupling with other variables in the population. The member latents Z may still exhibit arbitrary dependencies. The approach for simulate and logpdf described below is based on approximate inference in tagged subparts of the Venture trace, which carries a full realization of all random choices (population and member-specific latent variables) made by the program. The runtime system carries a set of K traces {(? k , Z k )}K k=1 sampled from an approximate posterior pG (?, Z|D). These traces are assigned weights depending on the user-specified evidence x[r,E] in the simulate/logpdf function call. G represents the CGPM as a probabilistic program, and the input yr and latent variables Z k are treated as ambient quantities in ? k . The distribution of interest is Z pG (x[r,Q] |x[r,E] , D) = pG (x[r,Q] |x[r,E] , ?, D)pG (?|x[r,E] , D)d? ?   Z pG (x[r,E] |?, D)pG (?|D) = pG (x[r,Q] |x[r,E] , ?, D) d? (3) pG (x[r,E] |D) ? ? PK K X 1 k k=1 w pG (x[r,Q] |x[r,E] , ? k , D)wk where ? k ?G |D. (4) k=1 The weight wk = pG (x[r,E] |? k , D) of trace ? k is the likelihood of the evidence. The weighting scheme (4) is a computational trade-off avoiding the requirement to run posterior inference on population parameters ? for a query about member r. It suffices to derive the distribution for only ? k , Z pG (x[r,Q] |x[r,E] , ? k , D) = pG (x[r,Q] , zrk |x[r,E] , ? k , D)dzrk (5) zrk Z = Y zrk q?Q J  1X Y pG (x[r,q] |zrk , ? k ) pG (zrk |x[r,E] , ? k , D)dzrk ? pG (x[r,q] |zrk,j , ? k ), J j=1 (6) q?Q where zrk,j ?G |{x[r,E] , ? k , D}. Eq (5) suggests that simulate can be implemented by sampling (x[r,Q] , zrk ) ?G |{x[r,E] , ? k , D} from the joint local posterior, then returning elements x[r,Q] . Eq (6) shows that logpdf can be implemented by first sampling the member latents zrk ?G |{x[r,E] , ? k , D} from the local posterior; using the conditional independence constraint (2), the query x[r,Q] then factors into a product of density terms for each element x[r,q] . 3 To aggregate over {? k }K k=1 , for simulate the runtime obtains the queried sample by first drawing k ? C ATEGORICAL({w1 , . . . , wK }), then returns the sample x[r,Q] drawn from trace ? k . Similarly, logpdf is computed using the weighted Monte Carlo estimator (6). Algorithms 2a and 2b summarize implementations of simulate and logpdf in a general probabilistic programming environment. Algorithm 2a simulate for CGPMs in a probabilistic programming environment. 1: function S IMULATE(G, r, Q, x[r,E] , yr ) 2: for k = 1, . . . , K do 3: if zrk ? 6 Z k then k 4: zr ?G |{? k , Z k , D} Q k 5: w ? e?E pG (x[r,e] |? k , zrk ) 6: k ? C ATEGORICAL ({w1 , . . . , wk }) 7: {x[r,Q] , zrk } ?G |{? k , Z k , D ? {yr , x[r,E] }} 8: return x[r,Q] . for each trace k . if member r has unknown local latents . sample them from the prior . weight the trace by likelihood of evidence . importance resample the traces . run a transition operator leaving target invariant . select query variables from the resampled trace Algorithm 2b logpdf for CGPMs in a probabilistic programming environment. 1: function L OG P DF(G, r, x[r,Q] , x[r,E] , yr ) 2: for k = 1, . . . , K do 3: Run steps 2 through 5 from Algorithm 2a 4: for j = 1, . . . , J do 5: zrk,j ?G |{? k , Z k , D ? {yr , x[r,E] }} Q 6: hk,j ? q?Q pG (x[r,q] |? k , zrk,j ) P 7: rk ? J1 Jj=1 hk,j k k 8: q k ? r w  P  PK K k k 9: return log ? log k=1 q k=1 w 2.3 . for each trace k . retrieve the trace weight . obtain J samples of latents in scope of member r . run a transition operator leaving target invariant . compute the density estimate . aggregate density estimates by simple Monte Carlo . importance weight the estimate . weighted importance sampling over all traces Inference in a composite network of CGPMs This section shows how CGPMs are composed by applying the output of one to the input of another. This allows us to build complex probabilistic models out of simpler primitives directly as software. Section 3 demonstrates surface-level syntaxes in the Metamodeling Language for constructing these composite structures. We report experiments including up to three layers of composed CGPMs. Let G a be a CGPM with output xa? and input y?a , and G b have output xb? and input y?b (the symbol ? a indexes all members r ? N). The composition GBb ? GA applies the subset of outputs xa[?,A] of G a to b the inputs y[?,B] of G b , where |A| = |B| and the variables are type-matched (Figure 1). This operation b results in a new CGPM G c with output xa? ? xb? and input y?a ? y[?,\B] . In general, a collection k {G : k ? [K]} of CGPMs can be organized into a generalized directed graph G [K] , which itself is a CGPM. Node k is an ?internal? CGPM G k , and the labeled edge aA ? bB denotes the composition a GA ? GBb . The directed acyclic edge structure applies only to edges between elements of different CGPMs in the network; elements xk[?,i] , xk[?,j] within G k may satisfy the more general constraint (1). Algorithms 3a and 3b show sampling-importance-resampling and ratio-likelihood weighting algorithms that combine simulate and logpdf from each individual G k to compute queries against network G [K] . The symbol ? k = {(p, t) : xp[?,t] ? y?k } refers to the set of all output elements from upstream CGPMs connected to the inputs of G k , so that {? k : k ? [K]} encodes the graph adjacency matrix. Subroutine 3c generates a full realization of all unconstrained variables, and weights forward samples from the network by the likelihood of constraints. Algorithm 3b is based on ratio-likelihood weighting (both terms in line 6 are computed by unnormalized importance sampling) and admits an analysis with known error bounds when logpdf and simulate of each G k are exact [7]. Algorithm 3a simulate in a directed acyclic network of CGPMs. 1: function S IMULATE(G k , r, Qk , xk[r,E k ] , yrk , for k ? [K]) 2: for j = 1, . . . , J do 3: (sj , wj ) ? W EIGHTED -S AMPLE ({xk[r,E k ] : k ? [K]}) 4: 5: m ? C ATEGORICAL ({w1 , . . . , wJ }) return {xk[r,Qk ] ? sm : k ? [K]} . generate J importance samples . retrieve jth weighted sample . resample by importance weights . return query variables from the selected sample 4 Algorithm 3b logpdf in a directed acyclic network of CGPMs. 1: function S IMULATE(G k , r, xkQ , xk[r,E k ] , yrk , for k ? [K]) 2: for j = 1, . . . , J do 3: (sj , wj ) ? W EIGHTED -S AMPLE ({xk[r,Qk ?E k ] : k ? [K]}) 4: 5: 6: . generate J importance samples . joint density of query/evidence for j = 1, . . . , J 0 do . generate J 0 importance samples (s0j , w0j ) ? W EIGHTED -S AMPLE ({xk[r,E k ] : k ? [K]}) . marginal density of evidence P  j P 0j 0 return log ? log(J/J ) . return likelihood ratio importance estimate [J] w / [J 0 ] w Algorithm 3c Weighted forward sampling in a directed acyclic network of CGPMs. 1: function W EIGHTED -S AMPLE (constraints: xk[r,C k ] , for k ? [K]) 2: (s, log w) ? (?, 0) . initialize empty sample with zero weight 3: for k ? T OPO S ORT ({? 1 , . . . , ? K }) do . topologically sort CGPMs using adjacency matrix 4: y?rk ? yrk ? {xp[r,t] ? s : (p, t) ? ? k } . retrieve required inputs at node k 5: log w ? log w + logpdf (G k , r, xk[r,C k ] , ?, y?rk ) . update weight by likelihood of constraint 6: xk[r,\C k ] ? simulate (G k , r, \C k , xk[r,C k ] , y?rk ) . simulate unconstrained nodes 7: s ? s ? xk[r,C k ?\C k ] . append all node values to sample 8: 3 return (s, w) . return the overall sample and its weight Analyzing satellites using CGPMs built from causal probabilistic programs, discriminative machine learning, and Bayesian non-parametrics This section outlines a case study applying CGPMs to a database of 1163 satellites maintained by the Union of Concerned Scientists [12]. The dataset contains 23 numerical and categorical features of each satellite such as its material, functional, physical, orbital and economic characteristics. The list of variables and examples of three representative satellites are shown in Table 1. A detailed study of this database using BayesDB provided in [10]. Here, we compose the baseline CGPM in BayesDB, CrossCat [9], a non-parametric Bayesian structure learner for high dimensional data tables, with several CGPMs: a classical physics model written in VentureScript, a random forest classifier, factor analysis, and an ordinary least squares regressor. These composite models allow us to identify satellites that probably violate their orbital mechanics (Figure 2), as well as accurately infer the anticipated lifetimes of new satellites (Figure 3). We refer to [14, Section 6] for several more experiments on a broader set of data analysis tasks, as well as comparisons to baseline machine learning solutions. Name Country of Operator Operator Owner Users Purpose Class of Orbit Type of Orbit Perigee km Apogee km Eccentricity Period minutes Launch Mass kg Dry Mass kg Power watts Date of Launch Anticipated Lifetime Contractor Country of Contractor Launch Site Launch Vehicle Source Used for Orbital Data longitude radians of geo Inclination radians International Space Station Multinational NASA/Multinational Government Scientific Research LEO Intermediate 401 422 0.00155 92.8 NaN NaN NaN 36119 30 Boeing Satellite Systems/Multinational Multinational Baikonur Cosmodrome Proton www.satellitedebris.net 12/12 NaN 0.9005899 AAUSat-3 Denmark Aalborg University Civil Technology Development LEO NaN 770 787 0.00119 100.42 0.8 NaN NaN 41330 1 Aalborg University Denmark Satish Dhawan Space Center PSLV SC - ASCR NaN 1.721418241 Advanced Orion 5 (NRO L-32, USA 223) USA National Reconnaissance Office (NRO) Military Electronic Surveillance GEO NaN 35500 35500 0 NaN 5000 NaN NaN 40503 NaN National Reconnaissance Laboratory USA Cape Canaveral Delta 4 Heavy SC - ASCR 1.761037215 0 Table 1: Variables in the satellite population, and three representative satellites. The records are multivariate, heterogeneously typed, and contain arbitrary patterns of missing data. 5 CREATE TABLE satellites_ucs FROM 'satellites.csv'; CREATE POPULATION satellites FOR satellites_ucs WITH SCHEMA ( GUESS STATTYPES FOR (*) ); 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 CREATE METAMODEL satellites_hybrid FOR satellites WITH BASELINE CROSSCAT ( OVERRIDE GENERATIVE MODEL FOR type_of_orbit GIVEN apogee_km, perigee_km, period_minutes, users, class_of_orbit USING RANDOM_FOREST (num_categories = 7); OVERRIDE GENERATIVE MODEL FOR launch_mass_kg, dry_mass_kg, power_watts, perigee_km, apogee_km USING FACTOR_ANALYSIS (dimensionality = 2); OVERRIDE GENERATIVE MODEL FOR period_minutes AND EXPOSE kepler_cluster_id CATEGORICAL, kepler_noise NUMERICAL GIVEN apogee_km, perigee_km USING VENTURESCRIPT (program = ' define dpmm_kepler = () -> { // Definition of DPMM Kepler model program. assume keplers_law = (apogee, perigee) -> { (GM, earth_radius) = (398600, 6378); a = .5*(abs(apogee) + abs(perigee)) + earth_radius; 2 * pi * sqrt(a**3 / GM) / 60 }; // Latent variable priors. assume crp_alpha = gamma(1,1); assume cluster_id_sampler = make_crp(crp_alpha); assume noise_sampler = mem((cluster) -> make_nig_normal(1, 1, 1, 1)); // Simulator for latent variables (kepler_cluster_id and kepler_noise). assume sim_cluster_id = mem((rowid, apogee, perigee) -> { cluster_id_sampler() #rowid:1 }); assume sim_noise = mem((rowid, apogee, perigee) -> { cluster_id = sim_cluster_id(rowid, apogee, perigee); noise_sampler(cluster_id)() #rowid:2 }); // Simulator for observable variable (period_minutes). assume sim_period = mem((rowid, apogee, perigee) -> { keplers_law(apogee, perigee) + sim_noise(rowid, apogee, perigee) }); assume outputs = [sim_period, sim_cluster_id, sim_noise]; // List of output variables. }; // Procedures for observing the output variables. define obs_cluster_id = (rowid, apogee, perigee, value, label) -> { $label: observe sim_cluster_id( $rowid, $apogee, $perigee) = atom(value); }; define obs_noise = (rowid, apogee, perigee, value, label) -> { $label: observe sim_noise( $rowid, $apogee, $perigee) = value; }; define obs_period = (rowid, apogee, perigee, value, label) -> { theoretical_period = run(sample keplers_law($apogee, $perigee)); obs_noise( rowid, apogee, perigee, value - theoretical_period, label); }; define observers = [obs_period, obs_cluster_id, obs_noise]; // List of observer procedures. define inputs = [""apogee"", ""perigee""]; // List of input variables. define transition = (N) -> { default_markov_chain(N) }; // Transition operator. ')); INITIALIZE 10 MODELS FOR satellites_hybrid; ANALYZE satellites_hybrid FOR 100 ITERATIONS; INFER name, apogee_km, perigee_km, period_minutes, kepler_cluster_id, kepler_noise FROM satellites; Clusters Identified by Kepler CGPM Period [mins] 4000 Geotail 3000 28 Cluster 1 Cluster 2 Cluster 3 Cluster 4 Theoretically Feasible Orbits 26 2000 Amos5 NavStar 1000 Meridian4 20000 30000 Perigee [km] Geotail Negligible Noticeable Large Extreme 24 23 Amos5 Meridian4 Orion6 21 0 10000 25 22 Orion6 0 Empirical Distribution of Orbital Deviations 27 Number of Satellites 5000 20 1e-10 40000 1e-5 1e0 1e5 Magntiude of Deviation from Keplers? Law [mins2 ] 1e10 Figure 2: A session in BayesDB to detect satellites whose orbits are likely violations of Kepler?s Third Law using a causal composable generative population model written in VentureScript. The dpmm_kepler CGPM (line 17) learns a DPMM on the residuals of each satellite?s deviation from its theoretical orbit. Both the cluster identity and inferred noise are exposed latent variables (line 14). Each dot in the scatter plot (left) is a satellite in the population, and its color represents the latent cluster assignment learned by dpmm_kepler. The histogram (right) shows that each of the four detected clusters roughly translates to a qualitative description of the deviation: yellow (negligible), magenta (noticeable), green (large), and blue (extreme). 6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 CREATE TABLE data_train FROM 'sat_train.csv'; .nullify data_train 'NaN'; def dummy_code_categoricals(frame, maximum=10): def dummy_code_categoricals(series): categories = pd.get_dummies( series, dummy_na=1) if len(categories.columns) > maximum-1: return None if sum(categories[np.nan]) == 0: del categories[np.nan] categories.drop( categories.columns[-1], axis=1, inplace=1) return categories CREATE POPULATION satellites FOR data_train WITH SCHEMA( GUESS STATTYPES FOR (*) ); CREATE METAMODEL crosscat_ols FOR satellites WITH BASELINE CROSSCAT( OVERRIDE GENERATIVE MODEL FOR anticipated_lifetime GIVEN type_of_orbit, perigee_km, apogee_km, period_minutes, date_of_launch, launch_mass_kg USING LINEAR_REGRESSION ); def append_frames(base, right): for col in right.columns: base[col] = pd.DataFrame(right[col]) numerical = frame.select_dtypes([float]) categorical = frame.select_dtypes([object]) INITIALIZE 4 MODELS FOR crosscat_ols; ANALYZE crosscat_ols FOR 100 ITERATION WAIT; categorical_coded = filter( lambda s: s is not None, [dummy_code_categoricals(categorical[c]) for c in categorical.columns]) CREATE TABLE data_test FROM 'sat_test.csv'; .nullify data_test 'NaN'; .sql INSERT INTO data_train SELECT * FROM data_test; joined = numerical CREATE TABLE predicted_lifetime AS INFER EXPLICIT PREDICT anticipated_lifetime CONFIDENCE prediction_confidence FROM satellites WHERE _rowid_ > 1000; for sub_frame in categorical_coded: append_frames(joined, sub_frame) return joined (a) Full session in BayesDB which loads the training and test sets, creates a hybrid CGPM, and runs the regression using CrossCat+OLS. (b) Ad-hoc Python routine (used by baselines) for coding nominal predictors in a dataframe with missing values and mixed data types. Mean Squared Error 102 101 ridge ols lasso kernel forest bayesdb(crosscat+ols) bayesdb(crosscat) 100 1 10 102 Lines of Code Figure 3: In a high-dimensional regression problem with mixed data types and missing data, the composite CGPM improves prediction accuracy over purely generative and purely discriminative baselines. The task is to infer the anticipated lifetime of a held-out satellite given categorical and numerical features such as type of orbit, launch mass, and orbital period. As feature vectors in the test set have missing entries, purely discriminative models (ridge, lasso, OLS) either heuristically impute missing features, or ignore the features and predict the anticipated lifetime using the mean in the training set. The purely generative model (CrossCat) can impute missing features from their joint distribution, but only indirectly mediates dependencies between the predictors and response through latent variables. The composite CGPM (CrossCat+OLS) in panel (a) combines advantages of both approaches; statistical imputation followed by regression on the features leads to improved predictive accuracy. The reduced code size is a result of using SQL, BQL, & MML, for preprocessing, model-building and predictive querying, as opposed to collections of ad-hoc scripts such as panel (b). Figure 2 shows the MML program for constructing the hybrid CGPM on the satellites population. In terms of the compositional formalism from Section 2.3, the CrossCat CGPM (specified by the MML BASELINE keyword) learns the joint distribution of variables at the ?root? of the network (i.e., all variables from Table 1 which do not appear as arguments to an MML OVERRIDE command). The dpmm_kepler CGPM in line 16 of the top panel in Figure 2 accepts apogee_km and perigee_km as input variables y = (A, P ), and produces as output the period_minutes x = (T ). These variables characterize the ellipticalporbit of a satellite and are constrained by the relationships e = (A ? P )/(A + P ) and T = 2? ((A + P )/2))3 /GM where e is the eccentricity and GM 7 is a physical constant. The program specifies a stochastic version of Kepler?s Law using a Dirichlet process mixture model for the distribution over errors (between the theoretical and observed period), P ? DP(?, N ORMAL -I NVERSE -G AMMA(m, V, a, b)), (?r , ?r2 )|P ? P r |{?r , ?r2 , yr } ? N ORMAL(?|?r , ?r2 ), where r := Tr ? K EPLER(Ar , Pr ). The lower panels of Figure 2 illustrate how the dpmm_kepler CGPM clusters satellites based on the magnitude of the deviation from their theoretical orbits; the variables (deviation, cluster identity, etc) in these figures are obtained from the BQL query on line 50. For instance, the satellite Orion6 shown in the right panel of Figure 2, belongs to a component with ?extreme? deviation. Further investigation reveals that Orion6 has a recorded period 23.94 minutes, most likely a data entry error for the true period of 24 hours (1440 minutes); we have reported such errors to the maintainers of the database. The data analysis task in Figure 3 is to infer the anticipated_lifetime xr of a new satellite, given a set of features yr such as its type_of_orbit and perigee_km. A simple OLS regressor with normal errors is used for the response pG ols (xr |yr ). The CrossCat baseline learns a joint generative model for the covariates pG crosscat (yr ). The composite CGPM crosscat_ols built Figure 3 (left panel) thus carries the full joint distribution over the predictors and response pG (xr , yr ), leading to more accurate predictions. Advantages of this hybrid approach are further discussed in the figure. 4 Related Work and Discussion This paper has shown that it is possible to use a computational formalism in probabilistic programming to uniformly apply, combine, and compare a broad class of probabilistic data analysis techniques. By integrating CGPMs into BayesDB [10] and expressing their compositions in the Metamodeling Language, we have shown it is possible to combine CGPMs synthesized by automatic model discovery [9] with custom probabilistic programs, which accept and produce multivariate inputs and outputs, into coherent joint probabilistic models. Advantages of this hybrid approach to modeling and inference include combining the strengths of both generative and discriminative techniques, as well as savings in code complexity from the uniformity of the CGPM interface. While our experiments have constructed CGPMs using VentureScript and Python implementations, the general probabilistic programming interface of CGPMs makes it possible for BayesDB to interact with a variety systems such as BUGS [15], Stan [1], BLOG [11], Figaro [13], and others. Each of these systems provides varying levels of model expressiveness and inference capabilities, and can be used to be construct domain-specific CGPMs with different performance properties based on the data analysis task on hand. Moreover, by expressing the data analysis tasks in BayesDB using the model-independent Bayesian Query Language [10, Section 3], CGPMs can be queried without necessarily exposing their internal structures to end users. Taken together, these characteristics help illustrate the broad utility of the BayesDB probabilistic programming platform and architecture [14, Section 5], which in principle can be used to create and query novel combinations of black-box machine learning, statistical modeling, computer simulation, and probabilistic generative models. Our applications have so far focused on CGPMs for analyzing populations from standard multivariate statistics. A promising area for future work is extending the computational abstraction of CGPMs, as well as the Metamodeling and Bayesian Query Languages, to cover analysis tasks in other domains such longitudinal populations [3], statistical relational settings [6], or natural language processing and computer vision. Another extension, important in practice, is developing alternative compositional algorithms for querying CGPMs (Section 2.3). The importance sampling strategy used for compositional simulate and logpdf may only be feasible when the networks are shallow and the constituent CGPMs are fairly noisy; better Monte Carlo strategies or perhaps even variational strategies may be needed for deeper networks. Additional future work for composite CGPMs include (i) algorithms for jointly learning the internal parameters of each individual CGPM, using, e.g., imputations from its parents, and (ii) new meta-algorithms for structure learning among a collection of compatible CGPMs, in a similar spirit to the non-parametric divide-and-conquer method from [9]. We hope the formalisms in this paper lead to practical, unifying tools for data analysis that integrate these ideas, and provide abstractions that enable the probabilistic programming community to collaboratively explore these research directions. 8 References [1] B. Carpenter, A. Gelman, M. Hoffman, D. Lee, B. Goodrich, M. Betancourt, M. A. Brubaker, J. Guo, P. Li, and A. Riddell. Stan: A probabilistic programming language. J Stat Softw, 2016. [2] G. Casella and R. Berger. Statistical Inference. Duxbury advanced series in statistics and decision sciences. Thomson Learning, 2002. [3] M. Davidian and D. M. Giltinan. Nonlinear models for repeated measurement data, volume 62. CRC press, 1995. [4] L. Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pages 260?265. ACM, 1986. [5] D. Fink. A compendium of conjugate priors. 1997. [6] N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. Learning probabilistic relational models. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, IJCAI 99, Stockholm, Sweden, July 31 - August 6, 1999. 2 Volumes, 1450 pages, pages 1300?1309, 1999. [7] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009. [8] V. Mansinghka, D. Selsam, and Y. Perov. Venture: a higher-order probabilistic programming platform with programmable inference. CoRR, abs/1404.0099, 2014. [9] V. Mansinghka, P. Shafto, E. Jonas, C. Petschulat, M. Gasner, and J. B. Tenenbaum. Crosscat: A fully bayesian nonparametric method for analyzing heterogeneous, high dimensional data. arXiv preprint arXiv:1512.01272, 2015. [10] V. Mansinghka, R. Tibbetts, J. Baxter, P. Shafto, and B. Eaves. Bayesdb: A probabilistic programming system for querying the probable implications of data. arXiv preprint arXiv:1512.05006, 2015. [11] B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, and A. Kolobov. 1 blog: Probabilistic models with unknown objects. Statistical relational learning, page 373, 2007. [12] U. of Concerned Scientists. UCS Satellite Database, 2015. [13] A. Pfeffer. Figaro: An object-oriented probabilistic programming language. Charles River Analytics Technical Report, 137, 2009. [14] F. Saad and V. Mansinghka. Probabilistic data analysis with probabilistic programming. arXiv preprint arXiv:1608.05347, 2016. [15] D. J. Spiegelhalter, A. Thomas, N. G. Best, W. Gilks, and D. Lunn. Bugs: Bayesian inference using gibbs sampling. Version 0.5,(version ii) http://www. mrc-bsu. cam. ac. uk/bugs, 19, 1996. 9 "
1," Introduction
Recommendation systems are routinely used to suggest items based on customers’ past preferences. These systems have proven useful for music, movies, news, and retail in general [1]. In
contrast, to find new scientific literature, researchers rely mostly on author-provided keywords,
titles, author names, and references. These procedures are likely to be biased [2] and also provide an unintended rich get richer (or Matthew’s) effects [3]. This problem is more pronounced
during conferences where appropriate keywords may not even exist, let alone citations. An
application of recommendation systems to suggest scholarly material based on the researcher’s
preferences thus promises to speed up literature search and increase relevance.
There are multiple recommendation systems that use either the personal preferences of a
new user (e.g., content-based recommendations) or exploit the similarity between the new
users’ preferences and previous users’ preferences (e.g., collaborative filtering). Most such
PLOS ONE | DOI:10.1371/journal.pone.0158423 July 6, 2016 1 / 11
a11111
OPEN ACCESS
Citation: Achakulvisut T, Acuna DE, Ruangrong T,
Kording K (2016) Science Concierge: A Fast
Content-Based Recommendation System for
Scientific Publications. PLoS ONE 11(7): e0158423.
doi:10.1371/journal.pone.0158423
Editor: Peter van den Besselaar, VU University
Amsterdam, NETHERLANDS
Received: December 4, 2015
Accepted: June 15, 2016
Published: July 6, 2016
Copyright: © 2016 Achakulvisut et al. This is an
open access article distributed under the terms of the
Creative Commons Attribution License, which permits
unrestricted use, distribution, and reproduction in any
medium, provided the original author and source are
credited.
Data Availability Statement: The dataset is owned
by Society for Neuroscience and is not publicly
available. However, an academic license may be
requested directly through media@sfn.org or the
society’s website http://www.sfn.org/about/careersand-staff/staff-list.
Funding: TA received Royal Thai Government
Scholarship #50AC002. DA and KK received support
by the John Templeton Foundation grant through the
Metaknowledge Research Network. The funders had
no role in study design, data collection and analysis,
decision to publish, or preparation of the manuscript.
system are available for commercial software, such as news [4], movie [5], and music [6] applications. In contrast, few projects address scientific literature search.
Scientific literature is different from other applications of recommendation systems. Scientist are very specialized and they tend to become even more specialized as they progress in their
careers. Therefore, understanding the differences in such fine grained topics is challenging. For
example, for the Society for Neuroscience conference, there are more than 500 hand-curated
areas of research (github.com/titipata/science_concierge/wiki/Topic-in-SfN-2015). The entire
library of medicine contains more than 25K hand-curated vocabultary terms to denote
research areas [7]. By comparison, there are around 400 genres and sub-genres of music. Additionally, the priority discovery credit system of science makes scientists hide their discoveries
until full publication. This means that entirely new fields may appear overnight, forcing scientific discovery systems to quickly adapt. Scientific literature is different from other fields and
therefore specific requirements need to be addressed.
There have been several attempts at producing recommendation systems for scientific literature. In [8], the authors presented a content-based recommendation system that works on
PubMed datasets. In [9], the Scienstein system combined a large set of criteria for providing literature recommendation. In [10], the authors presented a topic-based recommendation system
based on a Latent Dirichlet Allocation (LDA) model. Finally, [11] presents a recommendation
system based on citations. It is unclear how well these use the content of the manuscripts and
how they work internally since many of them are not open sourced.
Here we introduce Science Concierge (github.com/titipata/science_concierge), an open
source Python library that implements a recommendation system for literature search. Briefly,
the library uses a scalable vectorization of documents through online Latent Semantic Analysis
(LSA) [1]. For the recommendation part, it pairs the Rocchio Algorithm [12] with a large-scale
approximate nearest neighbor search based on ball trees [13]. The library aims at providing
responsive content-based recommendations utilizing only user’s votes. Science Concierge,
then, provides an open source solution to content-based scientific discovery.
We tuned and tested the algorithm on a collection of scientific posters from the largest Neuroscience conference in the world, Society for Neuroscience (SfN) 2015. First, we cross-validated the LSA model to capture most of the variance contained in the topics. Second, we tuned
the parameters of the algorithm to recommend posters that maximally resembled human
curated classifications into poster sessions. We showed that our algorithm significantly outperformed a popular alternative based on keywords, improving suggestions further as it learned
more from the user. A front-end interface that uses the algorithm in the back end is available at
http://www.scholarfy.net, where we used data from Society for Neuroscience (SfN) 2015
conference.
2 Materials and Methods
2.1 Conference dataset
Conferences are events that need a rapid understanding of trends. The time between submissions, acceptances, and presentations is typically much shorter than in journals. This makes it
crucial for recommendation systems to quickly scan the documents of the conference and let
scientists discover materials fast. This is one reason why we focus on testing our system using
conference data.
We obtained a license from the Society for Neuroscience (SfN) for the Neuroscience 2015
conference. This is the largest conference in Neuroscience in the world. This dataset included
14,718 posters and talks distributed over 500 sessions spanning 5 days. Not all the content of
the conference had abstracts available (e.g., talks) and therefore they were dropped from the
Science Concierge
PLOS ONE | DOI:10.1371/journal.pone.0158423 July 6, 2016 2 / 11
Competing Interests: The authors have declared
that no competing interests exist.
analysis. The dataset is not publicly available but an academic license may be requested directly
through media@sfn.org or the society’s website http://www.sfn.org/about/careers-and-staff/
staff-list
2.2 Content-based recommendation of scientific documents
There are three main design principles behind Science Concierge. First, it aims at using the
content of the documents rather than collaborative filter to prevent the Mathew’s effect, commonly found in recommendation systems [3]. This effect can be detrimental for scientific
exploration. Second, it aims at providing suggestions as fast as possible. This means that users
should get feedback as soon as they vote one item as relevant or irrelevant. Finally, it aims at
being validated using some external input. Below we describe the methods to achieve these
three goals.
2.3 Text preprocessing and term weighting schemes
Documents are tokenized by removing English stop words and stemming using the Porter
Stemming algorithm [14]. The terms in the documents are uni-grams and bi-grams. In the
experiment, terms which appear fewer than 3 times were removed or terms that have tf-idf
(explained below) greater than 0.8 were removed.
These terms can be weighted in different manners by considering the local structure of each
document as compared to the global structure of all documents. Different weighting schemes
exist in the literature and we try some of the most popular, including simple term frequency
(term frequency), term-frequency inverse document-frequency (tf-idf), and log-entropy [15,
16]. We additionally try a word vector representation that we explain in more detail later.
Term frequency, tf-idf, and log-entropy can all be computed based on the following term
statistics. Let fij be the frequency of term i in document j. Term frequency uses the matrix fij
directly. Tf-idf log transforms and reweights each term frequency by a global factor as follows
tf‐idf
ij ¼ ð1 þ log fi jÞ  log n
fi þ 1 ð1Þ
where fi is number of documents that contains term i and n is the total number of documents.
For the log-entropy transformation, we first find the global entropy for each term i as follows
gi ¼ 1 þ X
j
pij log2 pij
log2 n where pij ¼
fij Pj
fij ð2Þ
Finally, the log-entropy of term i in document j or lij can be written as
l
ij ¼ log2ð1 þ fijÞ  gi ð3Þ
Additionally, we use an alternative representation of a document based on word vectors.
The word vector represention [17] allows to approximate the co-ocurrence of terms, and therefore captures much better the meaning of words. This kind of representation has been used
successfully for synonym finding and translation tasks. In this article, we compute a 150
dimensional representation of the word vectors training from the abstracts of SfN 2015 and
average the words that appear in each document.
2.4 Latent Semantic Analysis
LSA is a simple topic modeling technique based on singular value decomposition [18]. The different weighting schemes describe in the previous section are transformed using Latent
Science Concierge
PLOS ONE | DOI:10.1371/journal.pone.0158423 July 6, 2016 3 / 11
Semantic Analysis (LSA) to reduce noise and improve smoothness. This technique decomposes
the weighting matrix X as a product of a left singular vectors (U), a diagonal singular value
matrix (S) and a right singular vectors (V) as X = USV>, where the diagonal singular values in
S are ranked from highest to lowest. The number of vectors to choose depends on how accurately we want to capture the matrix X [19], i.e. X  Ur Sr Vr and XLSA = Ur Sr where Ur, Sr, Vr
are truncated matrices. In our case, this number will be chosen by cross validation. The intuition behind the use of LSA is that we want to discover groups of words that are equivalent in
their meaning, as they should load onto the same singular vectors.
Even with the simple level of preprocessing provided by LSA, we can already start understanding how posters are separated into distinct topical areas. To visualize this, we transform
the LSA vectors using a two dimensional reduction by t-Distributed Stochastic Neighbor
Embedding (t-SNE) ([20], Fig 1C), coloring each poster with its curated topical area from letter
A through G. From the visualization, it is apparent that some topics are more clearly separable
than others, at least as long as we are in a very low dimensional space.
2.5 Poster representation based on keywords
The dataset used here also contained keywords. Searching by keywords is a popular way to discover topics at conferences. We compare our methods to this type of search. In this algorithm,
the LSA analysis is applied to the keyword vector of each poster and all the rest of the analysis
described below is the same.
2.6 Rocchio Algorithm and Nearest Neighbor Assignment
The Rocchio algorithm is used to produce recommendations based on relevant and non-relevant documents previously voted by the user [12]. The method can work with term frequency,
tf-idf, or any term weighting schemes. Given a set of documents vectors that are found to be relevant by the user {r1, r2,. . ., rn} and another set of non-relevant documents {u1, u2,. . ., um}, the
Rocchio algorithm finds a document vector that combines both sets of documents as follows
q ¼ q0 þ
a N
NXi¼1
r
i 
b M
MXj¼1
u
j ð4Þ
where q0 was originaly proposed as the mean document vector of an initial query made into the
system, such as a search for a particular keyword. The parameters α and β control how much
the relevant and non-relevant documents affect the final query vector, respectively. In the case
of our set up, there is no such initial query q0 and therefore the mean article replaces this term.
q ¼
a N
NXi¼1
r
i 
b M
MXj¼1
u
j ð5Þ
The parameters α and β will be found by cross validation using some external validation process.
Once the query vector q has been defined for a user, a nearest neighbor search will be performed to retrieve suggested elements. In our implementation, we use ball trees to approximate
a nearest neighbor search, which tradeoffs speed and accuracy relatively well [13].
2.7 Measuring suggestion performance by using human curated
classification
Each poster contained in this dataset has been manually classified into a topic by the organizers
of the conference. The topic classification is based on a tree with three levels. For example, a
Science Concierge
PLOS ONE | DOI:10.1371/journal.pone.0158423 July 6, 2016 4 / 11
Fig 1. Vector representation of documents. (A) Schematic of the workflow for converting abstracts into
vector representations (see Algorithm 1) (B) Schematic of Rocchio Algorithm (C) Projection of SfN abstract
vectors to 2-dimensional space using t-SNE color coded by human curated sessions from A to G.
doi:10.1371/journal.pone.0158423.g001
Science Concierge
PLOS ONE | DOI:10.1371/journal.pone.0158423 July 6, 2016 5 / 11
poster might be classified with topic F.01.r, where F is the broad area of study in Neuroscience,
01 is a subarea of F, and r is a further subdivision within 01. To validate many of the parameters
in our performance tests, we use this topic classification as an indirect measure of how correct
the suggestions are.
We implicitly assume that a random attendee would be highly interested in one topic only
and not interested in others. This means that users would tend to like poster from the same
area. While this assumption may not be accurate for a large number of attendees, we believe in
captures the intention behind the classification of topics in this particular SfN conference. To
measure the quality of suggestions, then, we ask the system to suggest ten posters based on a
set of“liked” posters from a particular topic. For each of the suggestions, we compute the distance between its topic and the topic of the liked posters, using as distance measure the lowest
common ancestor in the topic tree [21]. Minimizing the average of the humun topic distances
between the liked poster and the ten suggested posters will be set as the performance metric for
our comparisons.
Algorithm 1 Science Concierge workflow with Latent Semantic Analysis
1: Receive set of N documents D = {d1, d2,. . ., dN}
2: Preprocess documents D = preprocess({d1, d2,. . ., dN}) using stemming and
removing English stop words
3: Convert documents to matrix X using term frequency, tf-idf or logentropy. Remove rare words and words with high tf-idf
4: Apply Latent Semantic Analysis X = USV> ! X  Ur Sr Vr, XLSA = Ur Sr
5: Index nearest neighbor model to XLSA or for word vector represention, use
the average word vector of words in an abstract
6: Compute user’ s preference vector Eq (5)
q ¼
a N
NX i¼1
r
i 
b M
MX j¼1
u
j
7: Use indexed nearest neighbor model to suggest relevant documents
3 Results
3.1 Parameter optimization
Number of components for LSA. One parameter of the LSA algorithm is the number of
components of the SVD decomposition [19], which we find by cross validating on the distance
to human curated topic classification as described in Methods. We randomly sampled one
poster and liked it. Then, we ask the system to suggest ten posters and compute the average distance in human curated topic of those ten posters to the liked poster (Fig 2). In this way, we
were able to find that the appropriate number of components to be used should be around
between 100 to 200. We select 150 components for later experiments described in paper.
Weight of relevant and non-relevant documents for Rocchio algorithm. The Rocchio
algorithm differently weighs the importance of relevant documents with a parameter α and the
importance of non-relevant with a parameter β (see Eq 4). We tuned the parameters α and β
using a procedure similar to that used for finding the number of components in SVD.
The experiment this time is done by liking one poster from a random topic and voting as
non-relevant a poster from a different topic. We performed three experiments where we tested
three distances of the non-relevant voted posters: distance 1 (1 subdivision away), distance 2
(in a difference subarea), and distance 3 (in a different area of study). For each of these experiments, we tried a grid of α and β parameters and computed the average topic distance over a
set of 1,000 simulations (Fig 3). This is possible because we only have two free parameters here.
Science Concierge
PLOS ONE | DOI:10.1371/journal.pone.0158423 July 6, 2016 6 / 11
We found that voting as non-relevant those posters that are 1 distance away produces large
effects on performance. Interestingly, we found that the parameter β almost always makes the
performance decrease. This means that non-relevant posters should be used to filter out the
recommendations rather modifying a user’s preference vector. We also found that the best values for the parameter α are always larger than 1. Additionally, we found that non-relevant posters that are 3 distance away in topic space have little effect on performance (Fig 3). This is
intuitive because disliking posters that are far away gives little information about the topic that
a user may like. At the end, the best combination of parameters for non-relevant posters that
are away 1 distance is α = 1.8 and β = 0.
3.2 Algorithm comparison
In this section, we compare the performance of our algorithm against common alternatives
techniques and features. Our framework requires the use of abstracts, a significantly more complex data source than keywords, for example. Using LSA compromises speed in recommendation while still captures closeness in human curated distance after multiple votes. Moreover,
using the full abstract could capture variability that is not available in other simpler methods.
Fig 2. Number of SVD components vs. performance of the algorithm to capture human curated topics.
The number of LSA components vs the average human curated tree distance of suggested posters.
doi:10.1371/journal.pone.0158423.g002
Fig 3. Finding best parameters to weigh relevant and non-relevant votes. Performance of the system as
a function alpha and beta parameters for non-relevant documents that are 1 distance away (A), 2 dislike
distance away (B), and 3 dislike distance away (C) in human curated topics.
doi:10.1371/journal.pone.0158423.g003
Science Concierge
PLOS ONE | DOI:10.1371/journal.pone.0158423 July 6, 2016 7 / 11
To properly test the advantages of our approach, we cross validate the performance of our
method by using human curated topic distances as a performance metric (see section 2).
For each of the algorithms tested below, we do the following simulation to estimate their
performances. For each run of a simulation, we pick a poster at random and vote it as relevant.
Then, we ask the algorithm to suggest ten posters based on that vote. We compute the average
distance in human curated topic space between the suggestions and the liked poster. We then
vote for another poster randomly selected from the same human curated topic as the first
poster. We again ask the algorithm to suggest a set of ten posters and again we compute the
average distance. We repeat this process ten times to obtain the average distance to human
curated topics as a function of the number of votes. This simulation will help us understand
the performance of the algorithms as they gather more votes from a simulated user.
As a baseline for all comparisons, we compute the performance of a null model that suggests
posters at random. This is necessary because the distribution of human curated topics is not
uniform and therefore the distances could be distributed in unexpected ways. Not surprisingly,
the average distance to the human curated topic remained constant with the number of votes
(Fig 4, Random) but it was below 3, which is the farthest possible distance. This baseline will
allow us to compare performance against a proper null model.
Keywords are a common recommendation technique that relies on using human curated
“tags” for each document. In our dataset (see section 2), the authors themselves assigned multiple keywords by picking them from a predefined set, crafted by the organizers. We used these
keywords to produce recommendations by treating them as documents. We then model keyword counts by transforming them from the tf-idf matrix to 30 SVD dimensions. Preliminary
analysis revealed that 30 dimensions was an appropriate number to capture most of the variability. Suggestions using keywords perform significantly better than random suggestions
(paired t(9999) = 120.08, p < 0.0001) and keyword suggestion performance improves with the
number of votes, but not significantly (paired t(9998) = −1.851, p = 0.064, Fig 4). While keywords allow for better suggestions than the null model, authors would need to manually provide them.
Science Concierge uses the abstracts to automatically extract topics and provide suggestions.
We found that Science Concierge produces significantly better suggestions (Fig 4) than
Fig 4. Comparison of algorithms as they get more relevant documents from a simulated user. All term
weighting schemes except keywords improve recommendations with votes.
doi:10.1371/journal.pone.0158423.g004
Science Concierge
PLOS ONE | DOI:10.1371/journal.pone.0158423 July 6, 2016 8 / 11
keywords (Δ = 0.97 t(9999) = 138.26, p < 0.0001), term frequency (Δ = 0.3 t(9999) = 67.805,
p < 0.0001), log entropy (Δ = 0.04 t(9999) = 13.987, p < 0.0001), and word vector (Δ = 0.4
t(9999) = 86.662, p < 0.0001). For all techniques but keywords, we found that the human topic
distance of the recommendations significantly decreases with the number of votes.
Ideally, we want distances between documents induced by an algorithm to closely match the
distances induced by human curated topics. Here, we tested this idea using the keyword model
and the Science Concierge model. This is, we tested the correlation between the distance of two
random documents using a model and the distance of those same documents in human curated
topic space. Word vector had the highest correlation with human topics (ρ = 0.48), followed by
tf-idf (ρ = 0.43), log-entropy (ρ = 0.32), term frequency (ρ = 0.2), and keywords (ρ = 0.17). To
compare these relationships on a similar scale, we visualized the human curated distance vs.
the z-score of the model’s distances (Fig 5).
4 Discussion and Conclusion
Discovering new and relevant scholarly material progressively requires automation. Such discovery is already possible for commercial content such as movies, news, and music. However,
the same cannot be said about scientists. They commonly use legacy search systems that cannot
learn from previous searchers. In this article, we propose a system that can improve recommendations based on a scientist votes for relevant and irrelevant documents. We tested the system on a set of posters presented at the Society for Neuroscience 2015 conference. We found
that our system significantly improves on suggestions based on author-assigned keywords. We
also found that our system learns as the user provides more votes. The system returns a complete schedule of posters to visit within 100 ms for a conference of around 15K posters. We
publish the source code of the system so others can expand its functionality (github.com/
titipata/science_concierge). In sum, this article presents a system that can make scientific discovery faster and it is openly available for other scientists to expand.
One surprising finding in our analysis is that the posters voted as non-relevant were better
left not influencing the final preference vector. In particular, when voted non-relevant posters
were close in topic space to the liked posters, then the performance degraded. If those non-relevant posters were far away, then the performance remains unchanged. In the future, we want
Fig 5. Relationship between human curated distance and topic distance induced by the keyword and
Science Concierge models.
doi:10.1371/journal.pone.0158423.g005
Science Concierge
PLOS ONE | DOI:10.1371/journal.pone.0158423 July 6, 2016 9 / 11
to expand the experiment to domains in which a large number of votes is casted. This may
allow the algorithm to better understand topic preferences and therefore offer suggestions that
exploit the knowledge built in non-relevant votes.
The topic modeling technique used in our algorithm assumes that topics live on a linear
space. While this assumption provides significant computational advantages, there might be
cases where more complex modeling approaches may capture subtle topical relationships. In
the future, we will try non-linear probabilistic approaches such as Latent Dirichlet Allocation
(LDA), which recently has been shown to scale well [22]. To better capture entities embedded
in the text, future research will also investigate how to use deep learning modeling of words
and phrases [23, 24]. However, it is unclear how these methods cope with scalability. Our system may already provide an appropriate level of speed and accuracy for our domain.
It may appear that the averaging of preferences that our algorithm perform would mislead
recommendations for scientists with multiple, disparate interests. However, we found in the
usage data of the website that scientists tend to like few documents, all within similar topics.
More importantly, given the high dimensional representation of the documents, averaging
preferences from different clusters would still produce sensible recommendations. This happens because the nearest neighbor of the mean of two clusters is closer to those clusters than to
a random document. This phenomenon gets stronger with higher dimensions [25]. In our next
research, we will explore more directly how to improve recommendations for scientists with
diverse interests.
Future research could expand our system in many ways. The Rocchio algorithm’s dependency on nearest neighbors makes it inappropriate to exploit the potential richness available
on large number of votes [26]. With long term users who provide hundreds or even thousands
of votes, it may be more accurate to cast the recommendations as a classification problem [27].
It is unclear however when would be the right time to make such as switch.
Our system proposes a new way to discover scholarly material. Many similar systems (e.g.,
Google Scholar) do not release their algorithms, making them difficult to study. By opening
our algorithm, we will engage the scientific community to collaborate. Moreover, our algorithm
does not necessarily need to be constrained to scientific text as any document that can be represented as a vector can be fed into it (e.g., scientific figures, audio). In sum, our system provides
an open and fast approach to accurately discover new research."
2,"Dynamic programming is a powerful method for solving combinatorial optimization problems. By utilizing the properties of optimal substructures and overlapping subproblems,
dynamic programming can significantly reduce the search space and efficiently find an optimal solution. A representative example is the chain matrix multiplication problem (Cormen
et al., 2009). The dynamic programming algorithm takes only a polynomial time complexity
of O(n3) while the naive brute-force method takes at least exponential number of enumerations. Dynamic programming can even provide efficient algorithms for NP-hard problems.
For instance, the famous 0-1 knapsack problem can be solved in pseudo-polynomial time
complexity of O(cn) with sophisticated dynamic programming, which is much faster than
the naive enumeration with O(2n) time complexity.
c 2018 F. Yang, T. Jin, T.-Y. Liu, X. Sun & J. Zhang.
However, dynamic programming does not always work well, since the tabular method,
which will be discussed later, may take exponential space and time for some NP-hard
problems. For example, the Held-Karp algorithm, a dynamic programming algorithm to
solve the Travelling Salesman Problem (TSP) proposed independently by Bellman (Bellman,
1962) and by Held and Karp (Held and Karp, 1962), has the time complexity of O(2nn2)
and space complexity of O(2nn). Although it is faster than the brute-force method that
examines all O(n!) cycles, the algorithm is actually not able to solve relatively large problems
in practice more than 20 points because it still takes exponential time and space.
Recently, with success of deep learning, people consider an interesting idea of integrating
powerful machine learning methods on solving hard combinatorial optimization problems.
Neural network technology may be the most promising one for its powerful approximating
ability and flexibility. Vinyals et al. (Vinyals et al., 2015) designed a new recurrent neural
network (RNN) architecture called Pointer Network to learn a mapping from sequences to
index sequences for learning approximation algorithms to solve combinatorial optimization
problems such as convex hull, Delaunay triangulation, and TSP. Milan et al. (Milan et al.,
2017) further extended the Pointer Network architecture to another long short-term memory
(LSTM) model by considering the original objects. And they applied the method to solve
some NP-hard problems including the quadratic assignment problem and TSP. Their work
showed the interesting and strong capability of deep neural network to learn an algorithm
from input-output data.
However, there are some limitations in these work. First, these approaches do not utilize
the intrinsic properties of the problem, and they use neural network as a black box to learn
from a lot of labelled data generated from an approximation algorithm. That is, the learning
target is approximated solutions rather than optimal solutions and their performance are
reasonably behind the approximation algorithms, let alone the optimal solutions. Second,
because of the learning difficulty, these methods only learn instances of fixed sizes or limited
sizes, for example, TSP instances of 50 points to 100 points. For test cases of larger sizes, the
performance drops dramatically. In addition, the sequence to sequence learning frameworks
can only handle specific problem types. For example, they can only process planar or
Euclidean TSP instances with each element in the sequence being the coordination of a
point in the Euclidean space, rather than general TSP problems.
Another important field in machine learning closely related to dynamic programming
is reinforcement learning. Reinforcement learning is to learn the best interaction with an
environment for getting reward as much as possible. The core problem in reinforcement
learning is to solve a Markov decision process (MDP) and work out the optimal policy (action function). The typical way to derive the optimal policy is to learn the value function or
Q-function of the MDP, which look like the dynamic programming function in combinatorial optimization. For model-based scenarios (fully known information about the MDP), the
basic method is a recursive and iterative method, which is also called dynamic programming
approach. For model-free problems (unknown MDP), the method is similar with additionally employing Monte-Carlo simulation. In addition, for solving problems with large scale
or even continuous state space or action space, people propose the idea of value function
approximation by replacing the tabular value function representation with other more flexible function representation approaches (Sutton and Barto, 2017). Recently, considering
the universal approximating ability and success of deep neural networks, people propose
727
Yang Jin Liu Sun Zhang
the idea of representing the value function or Q-function with neural networks and design a
series of training algorithms (Sutton and Barto, 2017; Riedmiller, 2005; Mnih et al., 2013,
2015). This emerging direction is called deep reinforcement learning.
Inspired by the previous work, we consider if we can combine the advantages of dynamic
programming that utilizes the intrinsic properties of a problem and the strong approximating ability and flexibility of neural networks. Comparing with the rigid tabular method,
introducing the neural network technique can let the algorithm be more flexible and powerful. On the other hand, comparing with the previous black box usage of neural network,
utilizing the problem dependent properties can lead to a better performance.
In this paper, we propose an approach to boost the capability of dynamic programming
with neural network technology. This approach approximately represents a dynamic programming function with a neural network and it trains the neural network with an iterative
data-driven algorithm. Specifically, the main contributions of this paper are as follows.
• We propose the idea of approximately representing a dynamic programming function
with a neural network of polynomial size instead of the conventional memorization
table that may require exponential space. This method can significantly reduce the
space complexity and it is flexible in balancing the space, running time, and accuracy.
• We design an iterative update algorithm to train the neural network with data generated from a solution reconstruction process. This algorithm is flexible on running
time control with respect to the accuracy tolerance and an intermediate result can
also provide a fairly good suboptimal solution to the problem.
• We apply the approach to the Held-Karp algorithm to solve TSP and conduct a series
of experiments. The experimental results show that our method can solve larger problems that are intractable for conventional dynamic programming. The performances
are near optimal, outperforming the well-known approximation algorithms.
We would like to point out that there are differences between dynamic programming for
solving MDP and combinatorial optimization problems. First, in MDP, the state transition
is non-deterministic but the state transition in combinatorial optimization is usually deterministic. Second, though the state transition in MDP is uncertain, but one state can only
jump to another single state with one simulation step. On the contrary, one state may go
to multiple sub-states in combinatorial optimization. For example, in chain matrix multiplication problem, the algorithm will face two sub-problems after selecting one split point.
Therefore, our approach has to employ a data structure to organize the state visiting.
Another thing we would like to mention is that the intent of this work is not to challenge
the best solution of TSP. The value of this work is to provide an idea of leveraging the
powerful machine learning methods to solve a new real-world problem in the case that
one can design a dynamic programming but it has an extremely large state space. We
choose TSP as our experimental benchmark for some reasons. First, it is quite simple
to demonstrate our approach so that the readers are easy to follow the key idea but the
complicated business details. Second, TSP is a well-studied problem and the Held-Karp
dynamic programming algorithm is a natural baseline of our experiments.
The rest of this paper is organized as follows. The second section revisits the basic
knowledge of dynamic programming and the Travelling Salesman Problem. The third sec-
728
tion introduces and discusses our approach in detail. Specifically, one subsection proposes
the idea of representing a dynamic programming function with a neural network and the
other subsection introduces the training algorithm. In the fourth section, we apply our
approach to TSP and report the experimental results. In the last section, we summarize
our work and discuss some future exploration.
2. Preliminaries
2.1. Revisiting dynamic programming
To solve a combinatorial optimization problem with dynamic programming, we often consider it as a multi-step decision-making problem. At each step, we face a subproblem and
it turns to some smaller subproblems after making the current decision. Subproblems are
also called states as they actually form a search state space of a simple search algorithm
that inefficiently examines all solutions. One kind of such algorithms is to compute a state
value function f : S ! R, in which S is the state set, mapping states to the corresponding
optimal values. This function is also called dynamic programming function in the context
of dynamic programming algorithm design and analysis.
Dynamic programming can be considered as an improvement to the basic search method
under the condition that the search space has the properties of optimal substructures and
overlapping subproblems.
Like the divide-and-conquer method, the optimal substructures property means that the
solution to a problem could be constructed by combining the solutions to its subproblems.
This property gives us a top-down perspective to understand the relationship between a
problem and its subproblems and to formulate a recursive equation connecting the solutions
to a problem and its subproblems. Usually, suppose selecting a decision a for a state s will
turn to a set of sub-states δ(s; a), the dynamic programming equation for a minimization
problem looks like
f(s) = min
a
8<:
v(a) + X
s02δ(s;a)
f(s0)
9=;
; (1)
where a goes over all feasible decisions for state s and v(a) is the value of making this
decision.
Unlike the divide-and-conquer method in which subproblems to a problem are disjoint,
the overlapping subproblems property says that subproblems to a problem share some subsubproblems. A simple top-down divide-and-conquer method will cause a lot of unnecessary
computation by repeatedly solving the common subproblems. A key idea of dynamic programming is to solve each subproblem only once and then save the answer in a table. We
call such a method tabular method, which significantly reduces the computation time. Although dynamic programming starts from a top-down view, an algorithm often runs in a
bottom-up fashion. That is, the algorithm starts by filling the table with some edge cases
and then fills other cells according to the dynamic programming equation.
After the computation of a dynamic programming function, it is easy to find out an
optimal solution. We start from the state representing the original problem and make
decisions and go through the states with a depth first or breadth first order. At each
729
Yang Jin Liu Sun Zhang
step, we just make the decision that optimizes the current subproblem and turn to the
next corresponding subproblems accordingly. Specifically, for a current state s, we make a
decision a according to the equation
a = argmin
a0
8<:
v(a0) + X
s02δ(s;a0)
f(s0)
9=;
: (2)
Finally, all such decisions form the solution naturally.
2.2. Travelling Salesman Problem and Held-Karp algorithm
Travelling Salesman Problem (TSP) is a well-known combinatorial optimization problem
in computer science and graph theory. The problem is to find a minimum cost cycle in a
complete graph with nonnegative edges, visiting every vertex exactly once (Vazirani, 2001).
It is an important problem closely related to many practical problems.
The most direct solution is to examine all vertex permutations and find out the one
with the shortest length. The running time of this brute-force method is obviously O(n!)
and it is inefficient.
Actually, TSP is NP-hard and a reasonable approach is to find suboptimal solutions
with approximation algorithms. However, generally TSP cannot be approximated unless
P = NP. But for metric and symmetric TSP (with undirected graphs satisfying the triangle
inequality), the Christofides algorithm (Christofides, 1976) can solve it with O(n3) time and
produce a suboptimal solution less than 1.5 times the optimum. This is the best algorithm
with approximation ratio guarantee known so far. Asymmetric TSP is much harder and
the best known approximation ratio is O(log n=loglog n) (Asadpour et al., 2010).
Another early effort of solving TSP with dynamic programming is the Held-Karp algorithm (Bellman, 1962; Held and Karp, 1962). Without loss of generality, assume there are
n vertices and we always start from vertex 1. It is easy to understand that the construction
of a cycle is a multi-step decision-making process. We define a state with a pair (P; c),
in which P is a subset of [n] = f1; : : : ; ng denoting the vertices to be visited and c is the
current starting point. Let f(P; c) denote the shortest length of the path visiting all vertices
in P, starting from c and finally returning to vertex 1, as shown in figure 1. The original
problem corresponds to the state that P = [n] and c = 1.
Figure 1: An illustration to the Held-Karp algorithm, where P = f2;4;6g and c = 2. The
solid lines denote the visited path and the dashed lines denote a potential path
of visiting all vertices in P, starting from c and returning to vertex 1.
730
Based on the state presentation design, we have the following dynamic programming
equation,
f(P; c) = min
c02P ;c06=c
fd(c; c0) + f(P nfcg; c0)g; (3)
where d(c; c0) is the edge cost from c to c0. Because there are n vertices and 2n subsets of
vertex, the total number of possible states is O(2nn), which is the space complexity of the
dynamic programming algorithm. In addition, as it needs to examine all the left vertices
to compute one f(P; c), the time complexity is O(2nn2).
It is easy to construct a solution from a given f. Suppose c1; : : : ; ci are the first i vertices
of the solution and initially c1 = 1. The (i + 1)-th vertex of the solution is as follows except
some edge cases.
ci+1 = arg min
c = 2fc1;:::;cig
fd(ci; c) + f([n]nfc1; : : : ; cig; c)g: (4)
3. Approximating dynamic programming with neural networks
3.1. Approximately representing dynamic programming functions with neural
networks
As discussed in last section, the essence of the tabular method is to use a table to represent
a dynamic programming function. The advantage of this method is that it can precisely
save the information of a function if the state space is finite no matter how complicated
the function is. However, it may cost too much space if the state space is extremely large
though it is finite.
In addition to the tabular method, there are many ways to represent a function. Among
them, neural network is one powerful alternative. According to the universal approximation
theorem (Cybenko, 1989; Hornik, 1991), a feed-forward neural network with a single hidden
layer containing a finite number of neurons can approximate any continuous function on
compact subsets of Rn, under mild assumptions on the activation function (Bal´azs, 2001).
For representing a dynamic programming function with neural network, we first need
to encode states to numerical feature vectors as we usually do in machine learning. Since
a state may have several components, our idea is to concatenate different parts to form
a vector with each part corresponding to one component. The encoding method for each
component may be different according to its type.
• If the component is a numerical scale, we leave it as it is as a component in the vector;
• If the component is an id or order in a set of size n, the part is a one-hot binary vector
of size n with all 0s but the corresponding element as 1;
• If the component is a subset of a set of size n, the part is a binary vector of size n
having 1 denoting the corresponding element in the subset or 0 otherwise.
For the example in figure 1, the state (f2; 4; 6g; 2) could be encoded as a feature vector
(0; 1; 0; 1; 0; 1; 0; 1; 0; 0; 0; 0), with the first part (0; 1; 0; 1; 0; 1) corresponding to the subset
P = f2; 4; 6g and the last part (0; 1; 0; 0; 0; 0) corresponding to the starting vertex c = 2.
731
Yang Jin Liu Sun Zhang
After the quantization of the state (input) space, a dynamic programming function can
in principle be extended to a continuous function with interpolation techniques without
losing any precision though it is discrete. And then we can further approximately represent
the dynamic programming function with a neural network of finite size within any precision.
One may wonder if the size of the neural network will be even larger than the required
size of the tabular method. Despite it being possible, we argue that it is still practicable to
approximate a dynamic programming function with a much smaller neural network. First,
a dynamic programming function does not need to be absolutely precise as it is a tool to
construct the optimal solution whose value could be computed directly. Second, not all
states are equally useful for solving the problem. We may let the useful states to be more
precise than the useless states. That is, the neural network is not necessary to fit all states
uniformly. Third, a suboptimal solution is adequate in practice. It is valuable to find a
suboptimal solution efficiently with an inaccurate dynamic programming function. With
these reasons, the function could be much smoother and could be approximated with a
simple way. Comparing with the rigid tabular method, the neural network method is more
powerful and flexible. A much smaller neural network could approximate a large model
with tolerable accuracy. Certainly, for having a polynomial time algorithm, we employ a
neural network with polynomial size, trading off the space, running time, and accuracy.
3.2. An iterative algorithm and solution reconstruction process to train the
neural network
A conventional way of computing a dynamic programming function is to fill the table in a
specific topological order of states in a bottom-up fashion. However, it is difficult to fill a
value to a neural network without affecting other values as it is a parameterized function of
all states. In addition, it is also impractical to visit all states if the state space is extremely
large. The traditional scheme of training a neural network is supervised learning, which
trains the neural network with a lot of sampled (s; f(s)) pairs and hopes the generalization
works. However, this method is impractical because the computation of even a single exact
f(s) may take exponential time.
Similar to the algorithm of deep Q-learning (Mnih et al., 2013, 2015), we design an iterative algorithm to update the neural network with training data generated from a solution
reconstruction process, as shown in algorithm 1 for a minimization problem.
Rather than fitting the state values directly, we alternatively turn to the idea of making
the model look like a dynamic programming function as much as possible. Specifically,
suppose f(s; θ) is the neural network, where s is the input state vector and θ is the model
parameter. Ideally, if f is exactly the dynamic programming function, equation (1) should
hold for all states. However, there is usually a gap between the left term and the right
term. A natural idea is to make the gaps as close as possible and the learning object is to
minimize the loss function J(θ) defined as
J(θ) = X
s2S
0@
f(s; θ) − min
a
8<:
v(a) + X
s02δ(s;a)
f(s0; θ)
9=;
1A
2
; (5)
732
Algorithm 1 Training the neural network with solution reconstruction
1: Initialize neural network f(s; θ) with model parameter θ, maybe with pre-training on
edge cases
2: Initialize the data pool D
3: Initialize the exploration parameter ""t = 1:0 and the learning rate ηt appropriately
4: for each iteration t = 1; 2; : : : do
5: Initialize the state list S, the state scheduling data structure Q, and the state visitmarking data structure V
6: Add the state s0 representing the original problem to Q and V
7: while Q is not empty do
8: s = Q.pop()
9: S.Add(s)
10: With probability ""t:
select a random feasible decision a
with probability 1 − ""t:
select decision a according to equation (2)
11: for each sub-state s0 2 δ(s; a) do
12: if cannot find s0 in V then
13: Add s0 to Q and V
14: end if
15: end for
16: end while
17: Generate a mini-batch training data B=S+Sample(D)
18: Preform a gradient descent step on data B to close the gap according to equations
(6) and (7)
19: Add S to D with a weight being the value of the solution
20: Decay ""t and ηt if necessary
21: end for
where S in principle is the set of all states. A common solution to the optimization problem
is the gradient descent algorithm. The gradient with respect to θ is
rJ(θ) = 2X
s2S
0@
f(s; θ) − min
a
8<:
v(a) + X
s02δ(s;a)
f(s0; θ)
9=;
1A
0@
rf(s; θ) − r min
a
8<:
v(a) + X
s02δ(s;a)
f(s0; θ)
9=;
1A
: (6)
In each iteration the model parameter is updated by a gradient descent step with learning
rate ηt as
θt+1 = θt − ηtrJ(θt): (7)
However, it is impractical to calculate the gradient in a single iteration with all states.
Based on the idea of mini-batch technique in stochastic gradient descent, we use a small
sample of states instead to estimate the gradient. In our algorithm, we generate training
733
Yang Jin Liu Sun Zhang
states by an iterative solution reconstruction process. In each iteration, we construct a
solution based on current function f as if it is the correct dynamic programming function,
and we use the collected states as training data. This method has advantages over random
sampling. First, as f gets better and better, the generated states are more likely to be on
the optimal path or near the optimal path. That is, these states have likely more potential.
Second, we can give a weight to such states according to the generated solution. Such
weights could be further utilized for training data sampling, which is different from the
uniform data sampling in (Mnih et al., 2013, 2015).
For avoiding the model falls into local optima too early, particularly in the early stage
that f is inaccurate, we introduce the exploration strategy in solution construction. We
maintain an exploration parameter ""t. At each step in iteration t, we make the \optimal""
decision according to equation (2) with probability 1 − ""t, or choose a random feasible
decision with probability ""t. Parameter ""t decay over time, from 100% to a small number
such as 5%.
For collecting more data for training and breaking the correlation of consecutive iterations, we maintain a data pool. In each iteration, we sample moderate data according to
their weights from the pool along with the generated data as a mini-batch to update the
model.
State scheduling is another feature in our algorithm, which is a key difference from the
standard Q-learning. In the process of solution construction, one state may lead to multiple
sub-states, so we need to visit the states in a specific order with the help of some appropriate
data structures. For example, we may visit the states in the depth first order with a stack
or in the breadth first order with a queue. In addition, if sub-states will be overlapped, we
will need an extra data structure such as a hash set or binary set to mark if a state has been
visited otherwise it will cause a lot of unnecessary computation. Certainly, if the states to
a solution just form a chain, it is unnecessary to have such scheduling data structures.
4. Experiments in TSP
4.1. Experimental setting
We choose TSP as the experimental setting for some reasons. On one hand, among the
elegant NP-hard problems such as the satisfiability problem (SAT) and the set cover problem
in theoretical computer science, TSP is the most practical one as many real-world problems
can be modeled with TSP and it is easy to understand with less background knowledge. On
the other hand, among the NP-hard problems in the real world such as the vehicle routing
problem (VRP) and the bin packing problem, TSP is the simplest one and it is easy to
follow, avoiding complicated conditions.
We carry out experiments on a subset of TSPLIB (Reinelt, 1991), a data set of sample instances from real-world problems, including symmetric and asymmetric problems. TSPLIB
provides the best-known solutions to its problems that are from more than ten years’ efforts
of the human.
We implement four algorithms. The first is our approach, named as NNDP, short for
neural network dynamic programming. The second is the Held-Karp algorithm. The third
is the Christofides algorithm. And the last is the nearest neighbour greedy algorithm, which
734
is also a well-known approximation algorithm since it is quite simple, taking only O(n) or
O(n2) time, though it does not have the theoretical guarantee as good as Christofides does.
Following are some experimental details to our algorithm.
For a given instance of TSP with n vertices, we construct the dynamic programming
function as a fully connected feed-forward neural network. The input layer has 2n nodes,
separated into two parts. The first part is a binary vector of size n, corresponding to P ,
denoting the vertex subset to be visited. The other part is a one-hot binary vector of size
n, corresponding to c, denoting the starting vertex. The neural network has two hidden
layers, each having 4n hidden nodes with sigmoid activation function. We choose such a
setting for balancing the training time and the accuracy. And finally the output layer has
only one node and it is a linear combination of the outputs of the previous layer. Therefore,
there are O(n2) parameters in the neural network.
We develop our code based on Theano and run the experiments on a server with Intel
Xeon CPU (E5-2695 v2, 2.40GHz) and 128 GB memory. For each instance, we run the algorithm with 10000 iterations. We maintain the exploration parameter ""t with an exponential
decay that ""t+1 = maxf0:995""t; 0:05g. We keep a small constant learning rate η = 0:001.
The data pool is a limited queue that holds at most 1000 paths and the data too old will
be thrown away. A mini-batch contains 10 paths, with 1 path generated from the current
iteration and 9 ones sampled from the data pool.
4.2. Experimental results
The experimental results are shown in table 1. The number appended to the name of an
instance means the problem size. For example, Gr17 is an instance having 17 vertices. The
first 7 cases are symmetric TSP instances and the last 3 cases are asymmetric TSP instances.
In the table, we show the absolute value to a solution as well as the approximation ratio
over the best-known value for clear comparison.
Seeing from the experimental results, although the Held-Karp algorithm can guarantee
the optimal solution, it cannot extend to relatively large problems due to its space complexity issue. Our algorithm overcomes the space problem and it can solve larger problems
that are intractable for conventional dynamic programming.
Our approach also performs well. The relative errors of our approach are almost within
10%, outperforming the Christofides algorithm and the greedy algorithm. Our approach
even has good performance in asymmetric TSP. We do not directly compare our approach
with previous neural network based method in (Vinyals et al., 2015) and (Milan et al., 2017)
because they can only solve Euclidean TSP. But the instances in TSPLIB are not necessary
in such case. However, the experimental results in these papers show that their methods
do not catch up with the approximation algorithms, Christofides algorithm specifically, so
it is safe to conclude that our approach performs better than the previous work.
The running time depends on the problem size and the training data size. As we design
the neural network with O(n2) parameters, the total time complexity to one iteration is
O(n4). In our server, the running time to one iteration varies from 0.01 seconds to 0.1
seconds, for problems from about 20 vertices to about 100 vertices. Our approach converges
reasonably and it is flexible with respect to accuracy tolerance. Figure 2 illustrates an
735
Yang Jin Liu Sun Zhang
example of the convergence of the case Bayg29. It shows that the algorithm can reach a
nearly optimal solution with about 500 iterations.
Table 1: Experimental results in TSPLIB
Data Best NNDP Held-Karp Christofides Greedy
Gr17 2085 2085 1 2085 1 2287 1.1607 2178 1.0446
Bayg29 1610 1610 1 NA NA 1737 1.0789 1935 1.2019
Dantzig42 699 709 1.0143 NA NA 966 1.382 863 1.2346
HK48 11461 11539 1.0068 NA NA 13182 1.1502 12137 1.059
Att48 10628 10868 1.0226 NA NA 15321 1.4416 12012 1.1302
Eil76 538 585 1.0874 NA NA 651 1.1128 598 1.1115
Rat99 1211 1409 1.1635 NA NA 1665 1.3749 1443 1.1916
Br17 39 39 1 39 1 NA NA 56 1.435
Ftv33 1286 1324 1.0295 NA NA NA NA 1589 1.2002
Ft53 6905 7343 1.0634 NA NA NA NA 8584 1.169
Figure 2: Convergence of the case Bayg29. The function values are the values the model
outputs; the solution values are the exact values of the solutions constructed from
the model.
736
5. Conclusions and future work
Based on the neural network technology and the similar idea from reinforcement learning
method, in this paper we propose an approach to boost the capability of dynamic programming with neural networks for solving NP-hard combinatorial optimization problems. First,
we replace the tabular method with a neural network of polynomial size to approximately
represent a dynamic programming function. And then we design an iterative algorithm
to train the neural network with data generated from a solution reconstruction process.
Our method combines the approximating ability and flexibility of neural networks and the
advantage of dynamic programming in utilizing intrinsic properties of a problem. Our approach can significantly reduce the required space complexity for some NP-hard problems
and it is flexible in balancing space, running time, and accuracy.
There are differences in our approach from the standard deep reinforcement learning.
First, we aim at solving the NP-hard combinatorial optimization problems while reinforcement learning is to solve MDP. Moreover, we introduce some scheduling data structure
to handle the cases that one state has multiple subsequent substates. Last but not least,
we assign each data point a weight for sampling from the data pool rather than uniform
sampling.
We apply our approach to the Bellman-Held-Karp algorithm, a dynamic programming
algorithm for solving TSP. The experimental results show that our method can handle larger
problems that are intractable for the conventional dynamic programming algorithms. As an
approximation algorithm, our approach also outperforms other well-known approximation
algorithms.
There may be some future work along this direction. First, we may consider applying
this approach to more theoretical problems such as the weighted set cover problem and
practical problems such as the express delivering problem. Second, the neural network
dynamic programming function could be considered as a heuristic function in the search
process. So it is possible to generalize the idea to replace the handcrafted heuristic function
with a neural network and to automatically learn a better heuristic function in search
algorithm design though the search spaces do not have the good properties. In addition, a
possible extension of our approach is to solve combinatorial optimization problems under
non-deterministic environments."
3,"Deep learning is a powerful framework, and has been
successful in image classification [2], object detection [3],
pose estimation [4], and other areas. There is great interest
in applying deep learning to robotic applications, especially
reinforcement learning problems, i.e. learning behaviors with
partially or totally unknown dynamics. There are some
attempts [5], [6], but this is still an open problem.
We explore learning dynamics with neural networks, and
applying differential dynamic programming (DDP) to plan
behaviors. Recently we considered temporally decomposed
dynamics of a complicated task like pouring [7], modeled
them with locally weighted regression (LWR), and applied
DDP [1]. This method was verified in simulation experiments
of pouring, where the robot controlled a complicated flow to
achieve pouring. The contribution of [1] was showing that
learning decomposed dynamics and applying stochastic DDP
is a promising solution to reinforcement learning problems,
even in domains with complicated dynamics. By introducing deep neural networks into this framework, we expect:
(A) better modeling accuracy, and (B) automatic feature
selection.
In order to make use of neural networks with stochastic
DDP, we need extensions for: (1) modeling prediction error
1A. Yamaguchi and C. G. Atkeson are with The Robotics Institute,
Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh PA 15213,
United States info@akihikoy.net
and output noise, (2) computing an output probability distribution for a given input distribution, and (3) computing
gradients of output expectation with respect to an input.
Since neural networks have nonlinear activation functions,
these extensions were not easy. In this paper we provide an
analytic solution for these extensions using some assumptions for simplification.
We verified our method in simulation experiments of
pouring similar to those in [1]. Our stochastic DDP with
extended neural networks outperformed the LWR version.
When training with many samples, there was a small amount
of spilled materials in the LWR version. This was reduced
with the neural networks version. In addition, we used
redundant and/or non-informative state representations to
investigate the feature extraction ability of (deep) neural
networks. Although the state dimensionality changed from
16 to 47, the learning performance was preserved. We
also conducted preliminary robot experiments using a PR2.
Although the number of the experiments is not sufficient,
we obtained encouraging results. From the simulator results,
we think that learning dynamics with deep neural networks
and planning with stochastic DDP is a promising solution to
reinforcement learning problems. Furthermore, the extended
neural network is a general-purpose function approximator
and could be applied to learning various models used in
robotics such as dynamics and kinematics.
Related Work
There are two main approaches for reinforcement learning (RL) problems: a model-free approach and a modelbased approach. In recent robotics research, the model-free
approach seems to be more successful, for example in a
ball-in-cup task [8], flipping a pancake [9], and a crawling
motion [10]. A disadvantage of model-free methods is the
poor generalization ability compared to that of model-based
methods. Several attempts have been made to overcome this
issue [11], [12].
In a model-based method, dynamics of the system are
learned, and then planning, such as differential dynamic programming (DDP) [13], is used. Although there are successful
examples, such as [14], [15], this approach seems not popular
in RL for robotics, since (1) we need to gather many samples
to construct dynamics models, and (2) we need to solve two
problems, model learning and planning.
However we think that the model-based approach has
a large potential since learning dynamics would be more
robust than model-free RL because it is supervised learning,
and recently many good optimal control methods to solve
Fig. 1. Neural network architecture used in this paper. It has two networks
with the same input vector. The top part estimates an output vector, and the
bottom part models prediction error and output noise. Both use ReLU as
activation functions.
dynamic programming have been proposed (e.g. [16], [17]).
Recently Pan and Theodorou proposed a method: learning
dynamics models with a Gaussian process and applying
stochastic DDP [18]. We explore neural networks to model
complicated dynamics.
There are several examples of reinforcement learning
methods using neural networks, such as neural fitted Q iteration [19], and deep Q-learning [5]. These methods use a value
function based approach to RL with neural networks. Deep
Q-learning [5] makes use of the ability of deep networks,
namely, automatically learning features from images, but
would not be appropriate to learn behaviors with continuous
control signals. This method is using a discrete set of actions
like conventional RL methods [20]. Another approach is [12],
[6] where a trajectory optimization method was combined
with a local linear model learned from samples, and trained
neural network polices to increase generalization.
Another example of using neural networks in robot control
is [21] where a forward kinematics model between pneumatic
actuator displacements and feature point positions on an
android robot face was learned with neural networks, and
its inverse problem was solved with an optimizer. Although
the input and output dimensions were large (11 and 54
respectively), their method achieved an accurate inverse
kinematics controller of the android face.
In Section II we propose extensions of neural networks.
In Section III we describe our stochastic DDP. Section IV
describes the simulation, Section V describes the robot experiments, and Section VI concludes the paper.
II. NEURAL NETWORKS FOR REGRESSION WITH
PROBABILITY DISTRIBUTIONS
We explore extensions of neural networks for: (1) modeling prediction error and output noise, (2) computing an
output probability distribution for a given input distribution,
and (3) computing gradients of output expectation with respect to an input. Typical approaches for (2) are: (A) solving
analytically, (B) approximating the neural networks with
a local linear or quadratic model, and (C) computing numerically with random sampling. (A) is not easy since
neural networks include nonlinear activation functions, (B)
becomes inaccurate especially when learning a model with
discrete changes such as a step function, and (C) has a large
computation cost. We consider some assumptions to simplify
(A).
A. Definitions and Assumptions
We consider a neural network with rectified linear units
(ReLU; frelu(x) = max(0; x) for x 2 R) as activation
functions. Based on our preliminary experiments with neural
networks for regression problems, using ReLU as an activation function was the most stable and obtained the best
results. Fig. 1 shows the neural network architecture used in
this paper. For an input vector x, the neural network models
an output vector y with y = F(x), defined as:
h1 = f relu(W1x + b1); (1)
h2 = f relu(W2h1 + b2); (2)
: : :
hn = f relu(Wnhn−1 + bn); (3)
y = Wn+1hn + bn+1; (4)
where hi is a vector of hidden units at the i-th layer, Wi
and bi are parameters of a linear model, and f relu is an
element-wise ReLU function.
Even when an input x is deterministic, the output y
might have error due to: (A) prediction error, i.e. the error
between F(x) and the true function, caused by an insufficient
number of training samples, insufficient training iterations,
or insufficient modeling ability (e.g. small number of hidden
units), and (B) noise added to the output. We do not
distinguish (A) and (B). Instead, we consider additive noise
y = F(x) + ξ(x) where ξ(x) is a zero-mean normal
distribution N(0; Q(x)). Note that since the prediction error
changes with x and the output noise might change with x, Q
is a function of x. In order to model Q(x), we use another
neural network ∆y = ∆F(x) whose architecture is similar
to F(x), and approximate Q(x) = diag(∆F(x))2 where
diag is a diagonal function1. ∆y is an element-wise absolute
error between the training data y and the prediction F(x).
When x is a normal distribution N(µ; Σ), our purpose
is to approximate E[y] and cov[y]. The difficulty is the
use of the nonlinear ReLU operators f relu(p). We use the
approximation that when p is a normal distribution, f relu(p)
is also a normal distribution. We also assume that this
computation is done in an element-wise manner, that is, we
ignore non-diagonal elements of cov[p] and cov[f relu(p)].
Although the covariance Q(x) depends on x, we consider
its MAP estimate: Q(µ).
B. Expectation
Let us consider y = F(x) + ξ(x) where ξ(x) ∼
N(0; Q(x)). For x ∼ N(µ; Σ),
E[y] = E[F(x) + ξ(x)] = E[F(x)]; (5)
cov[y] = cov[F(x) + ξ(x)] = cov[F(x)] + cov[ξ(x)]
= cov[F(x)] + Q(µ): (6)
1We use diag for two meanings: exploiting diagonal elements as a vector
from a matrix, and converting a vector to a diagonal matrix.
Since Q(µ) = diag(∆F(µ))2, the difficulty is computing
E[F(x)] and cov[F(x)]. We solve these step by step.
The expectation and covariance of a linear function for an
input p ∼ N(µp; Σp) are:
E[Wp + b] = Wµp + b; (7)
cov[Wp + b] = WΣpW⊤: (8)
The expectation and covariance of the ReLU function for
an input p ∼ N(µp; Σp) where Σp is a diagonal matrix are:
E[f relu(p)] = Erelu(µp; Σp); (9)
cov[f relu(p)] = Vrelu(µp; Σp); (10)
where Erelu is an element-wise expectation of ReLU, and
Vrelu is a diagonal matrix of corresponding variances. For
µp = [µp1; µp2; : : :]⊤ and Σp = diag[σp 21; σp 22; : : :], each
element of Erelu(µp; Σp) and Vrelu(µp; Σp) is given by:
E[frelu(pi)] = ∫−1 1 max(0; pi)G(pi; µpi; σpi 2 )dpi
=
σpi
p2π X + µ2 pi (1 + E); (11)
var[frelu(pi)]
= ∫−1 1 (max(0; pi) − E[frelu(pi)])2G(pi; µpi; σpi 2 )dpi
=
1 4
µ
2
pi(1 − E2) + σpi 2
2
(1 + E − X2
π
) − σpip µpi 2π XE ; (12)
where G is a Gaussian function, X = exp(− 2 µ σ 2 pi 2
pi
), E =
erf( pµ 2pi σpi), and erf is an error function. Derivations are
described in the appendix.
The expectation and covariance of each layer’s output for
an input p ∼ N(µp; Σp) are:
E[f relu(Wp + b)] = Erelu(Wµp + b; diag(WΣpW⊤)); (13)
cov[f relu(Wp + b)] = Vrelu(Wµp + b; diag(WΣpW⊤)):
(14)
Thus the probability distribution of each hidden layer
hi ∼ N(µi; Σi) is given by: µi = Erelu(Wiµi−1 +
bi;diag(WiΣi−1W⊤ i )), and Σi = Vrelu(Wiµi−1 +
bi;diag(WiΣi−1W⊤ i )), where µ0 = µ and Σ0 = Σ (mean
and covariance of input x). We compute from i = 1 to n, and
obtain hn ∼ N(µn; Σn). Since the final activation function
is linear F(hn) = Wn+1hn + bn+1, we use Eq. (7), (8).
Finally assigning them into Eq. (5), (6), we obtain
E[y] = Wn+1µn + bn+1; (15)
cov[y] = Wn+1ΣnW⊤ n+1 + Q(µ): (16)
C. Gradient
The gradient of element-wise ReLU f relu(p) is given by:
@f relu(p)
@p = diag(f step(p)); (17)
where f step is an element-wise step function (1 if p is
positive, and 0 otherwise). Thus the gradient @ @y x for a
deterministic x is obtained by:
@y
@x =
@h1
@x
@h2
@h1 · · ·
@hn
@hn−1
@y
@hn ; (18)
where @hi
@hi−1 = W⊤ i diag(f step(Wihi−1+bi)), h0 = x, and
@y
@hn = W⊤ n+1.
For x ∼ N(µ; Σ), we use @@ E[ µ y] as a gradient for the
distribution of x. This gradient takes a similar form as above.
For N(µp; Σp) (Σp is a diagonal matrix), the gradient of
Erelu(µp; Σp) w.r.t. µp is given by:
@Erelu(µp; Σp)
@µp = diag(~ f step(µp;diag(Σp))); (19)
where ~ f step is an element-wise function similar to f step. For
µp = [µp1; µp2; : : :]⊤ and Σp = diag[σp 21; σp 22; : : :], each
element of ~ f step is given by:
f~ step(µpi; σpi) = 1
2
(1 + E); (20)
where E = erf( pµ 2pi σpi). Thus the gradient of E[y] w.r.t. µ is
given by:
@E[y]
@µ
=
@µ1
@µ
@µ2
@µ1
· · ·
@µn
@µn−1
@E[y]
@µn ; (21)
@µi
@µi−1
= W⊤
i diag(~ f step(Wiµi−1 + bi; diag(WiΣi−1W⊤ i )));
(22)
where µ0 = µ, Σ0 = Σ, and @ @E[ µy n] = W⊤ n+1.
D. Loss Function for the Error Model
The neural networks F(x) and ∆F(x) are trained with
a data set fx; yg. F(x) learns to predict from x to y, and
∆F(x) learns to predict from x to ∆y. First we train F(x),
and generate an error data set f∆yg = ff abs(y − F(x))g
where f abs is an element-wise absolute value function. f∆yg
is a set of positive value vectors. We expect ∆F(x) predicts
the envelope of f∆yg. For this purpose, using a standard
mean squared error is not adequate since the obtained curve
will be around the middle of the data. Thus we use a different
loss function to train ∆F(x), given by:
L = 1
ND
N∑ n
=1
D∑ d
=1
(max(0;∆ynd − ∆Fd(xn))2
+ β min(0;∆ynd − ∆Fd(xn))2); (23)
where N is the number of training samples, and D is
the number of output dimensions. The positive error and
the negative error are summed with a different weight β,
0 < β < 1. A typical value of β is 0:1. With this loss
function, the obtained curve shifts to a positive envelope.
E. Example
Fig. 2 shows the ReLU function, Erelu(x;1) and
Erelu(x;1) ± √Vrelu(x;1), and the numerically computed
expectation of ReLU with the variance 1.
Fig. 3 shows an example of learning a step function with
neural networks. We used two hidden layers; each hidden
Fig. 2. Expectation of ReLU.
Fig. 3. Learning a step function.
layer has 200 units. We used dropout [22] for each output
of the hidden layers during training where the dropout probability is 0:01. The dropout probability for regression works
better if it is smaller. In classification, typically 0.5 is used,
but such a large value is problematic in regression. Srivastava
et al. [22] analyzed the dropout in linear regression: “dropout
with linear regression is equivalent, in expectation, to ridge
regression”. Thus we chose 0:01. For training, we used
100 samples with adding a uniform random noise between
[−0:1; 0:1]. In Fig. 3, we are comparing the original step
function, samples for training, the neural network predictions
for N (x; 0) (there is variance of y because of the prediction
error model), and the predictions for N (x; 0:52). The gradients of each prediction are also plotted.
Fig. 4 shows an example of learning a linear function
y = 0:5x with neural networks. In this example, we used
output noise changing with x: a uniform random noise
between [−1; 1] for jxj < 2, and a uniform random noise
between [−0:25; 0:25] for other x. We used two hidden
layers; each hidden layer has 200 units. We used the dropout
with probability 0:01 for training. The above graph of Fig. 4
shows F (x) (x is a deterministic variable), and the graph
below shows ∆F (x).
III. STOCHASTIC DIFFERENTIAL DYNAMIC
PROGRAMMING
We use the same differential dynamic programming (DDP)
that we used in [1]. This DDP is stochastic; that is, we
consider probability distributions of states and expectations
of rewards. This design of evaluation functions is similar to
Fig. 4. Learning y = 0:5x with output noise depending on x.
Fig. 5. Temporal decomposition considered in this paper. The dotted box
denotes the whole process.
recent DDP methods (e.g. [18]). On the other hand, we use a
simple gradient descent method to optimize actions2, while
traditional DDP [13] and recent methods (e.g. [16], [17],
[18]) use a second-order algorithm like Newton’s method. In
terms of convergence speed, our DDP is inferior to secondorder DDP algorithms. The quality of the solution would
be the same as far as we use the same evaluation function.
Our algorithm is easier to implement. Since we use learned
dynamics models, there should be more local optima than
when using analytically simplified models. In order to avoid
poor local maxima, our DDP uses a multiple criteria gradient
descent method3.
We consider a system with N decomposed processes
illustrated in Fig. 5. The input of the n-th process is a state
xn and an action an, and its output is the next state xn+1.
Note that some actions may be omitted. A reward is given
to the outcome state. Let Fn denote the process model:
xn+1 = Fn(xn; an), and Rn denote the reward model:
rn = Rn(xn). Typically the most important reward is the
final reward RN (xN ) and Rn(xn) = 0 for n < N, but we
consider a general form Rn(xn) so that we can give rewards
(or penalties) for intermediate states. We assume that every
state is observable.
Each dynamics model Fn is learned from samples with the
neural networks mentioned in this paper where the prediction
2It works with any gradient descent methods.
3More specifically, in our DDP, we maintain “reference states” which are
optimized by ignoring dynamics constraints. These reference states are used
to create reference value functions. In our multiple criteria gradient descent
method, we switch the evaluation functions in these value functions and the
original one. Additionally, we also use multiple gradients computed from
these value functions and the original evaluation function, which are useful
to avoid local optima. See [1] for more details.
Fig. 6. Simulation environment. Right figure is after pouring material.
error is also modeled. At each step n, a state xn is observed,
and then an action an is planned with DDP so that an
evaluation function Jn(xn; fan; : : : ; aN−1g) is maximized.
Jn
is an expected sum of future rewards, defined as follows:
Jn
(xn; fan; : : : ; aN−1g) = E[∑N n′=n+1 Rn′(xn′)]
= ∑N n′=n+1 E[Rn′(xn′)]: (24)
In the DDP algorithm, first we generate an initial set of actions. Using this initial guess, we can predict the probability
distributions of future states with the neural networks. The
expected rewards are computed accordingly. Then we update
the set of actions iteratively with a gradient descent method.
In these calculations, we use the extensions of the neural
networks described in this paper to compute the expectations,
covariances, and the gradients. Thus we can replace LWR in
[1] by the neural networks, and apply our DDP.
IV. SIMULATION EXPERIMENTS
We verify our method in simulated experiments. The
problem setup is the same as in [1]. In Open Dynamics
Engine [23], we simulate source and receiving containers,
poured material, and a robot gripper grasping the source
container (Fig. 6).
The materials are modeled with many spheres, simulating the complicated behavior of material such as tomato
sauce during shaking. There are three typical failure cases:
(1) pushed the receiving container, (2) materials bounced
out, and (3) poured at wrong place. Note that even if we
use the same parameters in the same initial condition, the
result varies (we are not adding random noise); e.g. in a
case where the materials spill out of the receiving container,
but in another case there are no spilled materials. Thus
the dynamics is complicated as in real pouring, and the
simulation itself is a probabilistic process probably due
to contact simulation effects and/or process communication
delay (the simulator and the DDP program are connected
over ROS).
We use the same state machines for pouring as those
in [1], which are a simplified version of [7]. Those state
machines have some parameters: a grasping height and
pouring position. Those parameters are planned with DDP.
There are five separate processes. The initial or 0th state:
before grasping. x0: x and y positions of the receiving
container, a0: the grasping position. The 1st state: after
grasping. x1: x and y position of the receiving container, and
actual grasped position, a1: x and y position of the pouring
(a) Sum of rewards per episode.
(b) Penalty of spill per episode (y-axis is reversed).
Fig. 7. Learning curves of on-line learning. Moving average filter with 15
episode window is applied.
location. The 2nd state: after moving to the pouring location,
and before the flow control. x2: x and y displacement of the
receiving container, the speed of the receiving container, and
the actual pouring location relative to the receiving container,
a2: no action. The 3rd state: after the flow control. x3: the
average flow center (x and y), the flow variance, and zcoordinate of the pouring location. a3: no action. The 4th
state: after pouring. x4: the amount of materials poured in
the receiving container (arcv), and the amount of spilled
materials (aspill). The reward consists of the amount arcv,
the spilled penalty −aspill, and the penalty for the movement
of the receiving container given at x2.
A. Comparison of Models in On-line Learning
We compare LWR and neural networks as the models of
processes. Each LWR uses the same configuration as in [1].
Each neural network has three hidden layers; each hidden
layer has 200 units. We use dropout [22] for each output of
hidden layers during training where the dropout probability
is 0:01.
We apply the learning framework of [1] in an on-line
manner. After each state observation, we train the neural
networks, and plan actions with the models. In the early stage
of learning, we gather three samples with random actions,
and after that we use DDP.
We compared a method using LWR (LWR-Sum/Grad)
and one using neural networks (DNN-Sum/Grad). Fig. 7(a)
shows the average learning curves of 10 trials, and Fig. 7(b)
shows the penalty of spillage. DNN-Sum/Grad is better than
LWR-Sum/Grad. The difference in the learning curves is
Fig. 8. Learning curves of on-line learning. Moving average filter with 15
episode window is applied.
mainly due to the penalty of spillage; the amount of spilled
material is smaller in DNN-Sum/Grad. Since the dynamics
of flow in our simulation model is very complicated, there
was a small amount of spilled material even after the
dynamics model is learned. This spilled amount is reduced
when using neural networks, which indicates that the neural
networks has a better ability to learn dynamics than LWR.
B. Using Redundant States
In the previous experiments, we used carefully selected
states. Since we can expect automatic feature extraction with
(deep) neural networks, we test alternative state vectors that
have redundant and/or non-informative elements. Specifically, instead of the pose of the receiving container, we use
four corner points on its mouth (x; y; z respectively). We also
use the y-coordinate of the pouring position. Since we do not
use the pose of the receiving container, the relative position
of the flow center is computed with respect to the center of
the four corner points. Consequently the dimensions of the
state vectors x0, . . . , x4 are changed from (2, 3, 5, 4, 2) to
(12, 13, 16, 4, 2) respectively.
We used the same configuration as well as structure of
hidden layers. Fig. 8 shows the average learning curves of
10 trials where the previous result (DNN-Sum/Grad) is
plotted together with DNN-Sum/Grad X+ that uses the
expanded states. The learning curves of the two conditions
are almost the same. This result indicates that although
the total dimensionality changed from 16 to 47, the neural
networks could extract the features as we expected. The
two learning curves also show that the learning speeds are
almost the same. The reason would be that the complexities
of the dynamics are the same although the dimensionalities
are different.
V. PRELIMINARY ROBOT EXPERIMENTS
We apply the method to a pouring task of a PR2 robot.
Although the current results are preliminary, we think these
are valuable to mention in this paper.
This task is a simplified version of [7]. Fig. 9 shows
the setup. The robot starts pouring from an initial state
shown in the figure with holding a source container. The
position of the receiving container changes in each episode;
a human operator places the container differently before each
Fig. 9. Setup of the robot experiments.
pouring episode. The position of the receiving container
is measured by an external RGB-D sensor. First, the pose
of the AR-marker is measured. Then the base cylinder of
the receiving container is searched around the AR-marker
position. We use a template matching method with rendered
depth and surface-normal images using a ray-tracing method.
The robot plans a collision-free trajectory to reach a pouring
location, and executes flow control. Here we use only a
shaking skill for flow control (flow control with tipping does
not work in this case since the material jams inside the
container). Two RGB cameras are used to measure the flow,
the movement of the receiving container during the flow
control, the amount of materials poured into the receiving
container, and the amount of spilled materials. For the flow,
we use the Lucas-Kanade method [24] to obtain optical flow.
Then we compute the current flow amount, the flow center
position, an average flow position, and the flow variance
during flow control. The movement of the receiving container
is measured by detecting colors; for this purpose, the base
cylinder of the container is colored pink. The amount of
materials is measured by simple color detection; for this
purpose we use blue-colored materials. In order to decide
that the materials are poured into the container or spilled
out, we refer to the receiving container position measured
by color detection. The two cameras measure these data
independently. One is used as the x-axis data, and the other
is used as the y-axis data. The human operator also modifies
the camera positions when placing the receiving container to
adjust visual measurements. After pouring a target amount,
the robot moves the arm back to the initial pose.
The whole behavior is modeled with state machines.
In this scenario, the robot plans a feasible pouring location, feasible trajectories, and shaking parameters (“feasible”
means collision-free and IK-solvable). Although our final
goal is solving those plans under a single framework, in this
experiment we combine a classic method and the method
proposed in this paper. DDP plans parameters that are hard to
decide without learning the dynamics. The other parameters
are planned using a classic method. For the pouring location,
our DDP plans the z-coordinate and the distance from the
receiving container’s center. Other parameters (e.g. the orientation of the source container) are decided by an optimization
with CMA-ES [25]. Here the evaluation function takes a
better value if the source container is closer to the left
side of the receiving container, and if the pose is feasible.
Refer to [7] for the details of this optimization. The feasible
trajectories are also planned with CMA-ES as mentioned in
[7]. DDP also plans the amplitude of the shaking motion.
We use a fixed shaking direction.
Planning with CMA-ES considers only a partial situation,
while DDP plans the policy parameters considering the entire
pouring process. We consider three decomposed processes
for DDP. The initial or 0th state: at the initial state. x0: the
position of the receiving container in the robot frame, a0: the
pouring location parameters. The 1st state: after moving the
source container to the pouring location and before the flow
control. x1: the actual position of the pouring location, and
the position of the receiving container in the robot frame, a1:
the shaking amplitude. The 2nd state: at the end of the flow
control. x2: the movement of the receiving container in the
camera frame, the average flow position in the camera frame
(relative to the the receiving container position), and the flow
variance; the data from two cameras is used, a2: no action.
The 3rd state: at the end of the pouring. x3: the amount of
materials poured in the receiving container (arcv), and the
amount of spilled materials (aspill). The reward is mainly
given to (1−5(0:1−arcv))−100aspill where 0:1 is the target
amount. We use this small target amount in order to reduce
the experiment time. A penalty is given to the movement of
the receiving container at x2.
We conducted three runs with 38, 39, and 69 episodes
respectively. For the first two runs we used dry peas colored
blue, and for the third run we used blue plastic beads.
Fig. 10(a), 10(b), 10(c) show the learning curves where the
sum of rewards, the poured amount, and the spilled amount
are plotted per episode respectively. The sum of rewards is
improved with episodes. From Fig. 10(b) and 10(c) we can
see a strategy of the robot. After 10 episodes, the poured
amount curve is gradually decreasing in average. On the
other hand, between 0th to 20th episodes, the spilled amounts
are relatively large compared to the later part. Thus the robot
first tried actions that produced a large amount, and then
the robot adjusted the actions so that the spilled amount
is reduced. This tendency would be changed by modifying
the weights of poured and spilled amounts in the reward
function.
Fig. 11 shows part of the dynamics learned with neural
networks. The graphs in Fig. 11 show the prediction from
x2 to the reward for the poured and the spilled amounts.
Since x2 has eight elements, only two elements are varied,
and the median values of the samples are used for the other
elements. From the 3D plot (a), we can see the samples are
noisy, and there are some outliers. From the top view of the
same graph (b), we can see a peak is located where the flow-y
is around zero and the flow-x is around −0:1. Since the flow
position is a relative value from the receiving container, this
result is almost correct. On the other hand, the graph (c) is
different from what we expected. The peak should be located
where the flow-y is zero and the flow variance is also close
to zero, but actually the peak is at a different position. Since
DDP is used with the dynamics where (b) and (c) (and other
not plotted elements) are unified, the problem in (c) is not
a big issue now. However this might be a potential problem
(a) Sum of rewards per episode.
(b) Poured amount per episode.
(c) Spilled amount per episode.
Fig. 10. Learning curves (mean±1-SD) of the robot experiment. Moving
average filter with 5 episode window is used.
Fig. 11. Plot of the part of dynamics models. The predictions from x2
to the reward for the poured and the spilled amounts are plotted. (a) and
(b) are plots over the flow position (x and y). (c) is a plot over the flow
y-position and the flow y-variance. (b) and (c) also show the contours where
the reference states used in our DDP are also plotted. (zoom in using your
PDF viewer)
in future.
Conducting better-controlled experiments is planned for
future work. In the above experiments, the manual placement
of the RGB cameras might produce noise although the
stochastic extensions of neural networks could handle them.
This should be improved. We also consider comparing neural
networks with LWR and other function approximators in
robot experiments as well as variations of neural networks
such as different activation functions.
VI. CONCLUSION
As a model-based reinforcement learning method, we explored learning dynamics with neural networks and applying
differential dynamic programming (DDP) to plan behaviors.
Based on our recent work [1] where we used locally weighted
regression to model temporally decomposed dynamics, we
made use of neural networks with stochastic DDP. For this
purpose, we extended neural networks with ReLU activation
functions in terms of probability computations. We verified
this method in pouring simulation experiments. The learning
performance with neural networks outperformed that of
locally weighted regression. The amount of spilled materials
was reduced. In addition, we used redundant and/or noninformative states to investigate the feature extraction property of (deep) neural networks. Although the state dimensionality changed from 16 to 47, the learning performance did
not change. The early results of the robot experiments using
a PR2 were also positive. Therefore we think this framework,
learning dynamics with deep neural networks and planning
with stochastic DDP, is a promising solution to reinforcement
learning problems"
4,"Recommender System (RS) for research articles is a very important
application that helps researchers keeping track of their eld of
study. Moreover, it can aid the scientists (e.g., physicians) as a
decision support tool. One way by which researchers nd articles is
following citations in other articles that they are interested in, but
this limits them to specic citation communities, and it is biased
towards heavily cited papers. Another method of nding articles is
keyword-based search, which is a powerful approach, but it is also
limited as it can be dicult to form queries to search with. This has
opened the door to using recommendation methods as a way to
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specic permission and/or a
fee. Request permissions from permissions@acm.org.
UMAP’17, July 9–12, 2017, Bratislava, Slovakia
© 2017 ACM. 978-1-4503-4635-1/17/07...$15.00
DOI: http://dx.doi.org/10.1145/3079628.3079708
help researchers nding interesting articles. Most of the current RSs
are available for commercial applications, such as news, movies and
music applications. On the contrary, few projects address scientic
literature recommendation.
The existing RSs approaches can be classied into three types:
content-based ltering, collaborative ltering and hybrid approaches.
Content-based ltering uses the content of items which are highly
rated by a user in order to nd her preferences [17]. On the other
hand, collaborative ltering utilizes the similarity between user’s
preferences and other similar users’ preferences in order to recommend new items [7]. Hybrid RSs use a combination of contentbased and collaborative ltering techniques [5]. Most of the current
research paper recommendation systems are based on the bag-ofwords model, that represents the number of times each word occurs
in a document. The context of the words and the semantic similarity between words are not considered during the extraction and
representation of the document features.
Recent advances in articial neural networks (ANNs) have shown
that continuous word vectors can be learned as a probability distribution over the words of a document. Deep learning architectures
are basically ANNs of multiple non-linear layers. A key benet of
deep learning is the analysis and learning of massive amounts of
unsupervised data, making it a valuable tool for big data analytics
where raw data is largely unstructured. Using deep learning techniques to extract meaningful data representations makes it possible
to obtain a semantic and relational understanding of the data from
such high-dimensional textual data.
We intend to use deep learning techniques for recommending
relevant research papers in PubMed1 database, based on the semantic content of the papers that match user’s preferences. User
proles will be built based on some basic information that a user
may provide, such as the papers she is interested in, to be selected as
input from a graphical user interface. Moreover, the proles will be
improved using user interaction information that can be extracted
from users’ logs.
The rst part of this paper briey presents the research goals,
some of the research literature related to the existing approaches
of designing research paper RSs. The other parts introduce the
proposed system architecture, the technologies that we will use,
and the evaluation methods. The paper is concluded by presenting
the current research progress.
1https://www.ncbi.nlm.nih.gov/pubmed
UMAP 2017 Doctoral Consortium
UMAP’17, July 9-12, 2017, Bratislava, Slovakia
327
2 RESEARCH GOALS
This main research goals can be summarized as follows:
• To propose a novel personalized research paper recommendation approach that learns and utilizes the semantic
representation of papers’ titles and abstracts for matching
users’ interests.
• To study the eect of using deep neural networks, in particular word2vec2 and long short-term memory (LSTM)
techniques in extracting the semantic representation of the
scientic papers’ titles and abstracts for the recommendation task.
• To propose a comprehensive user modeling framework that
combines user explicit and implicit feedback by allowing
users to specify their preferred papers and by analyzing
users behaviors.
3 RELATED WORK
There have been some attempts to develop recommendation systems for scientic literature. Citation databases such as CiteSeerX3
apply citation analysis in order to identify papers that are similar to
an input paper. Scholarly search engines such as Google Scholar4 focus on classic text mining and citation counts. The research of [20]
has also presented a recommendation system based on citations.
Other work was done based on articles content; many dierent types of continuous representations techniques such as Latent
Dirichlet Allocation (LDA) [19] and Latent Semantic Analysis (LSA)
[1, 9] have been used to describe the content of a document as a
probability distribution of latent variables known as topics. The
assumption behind those methods is that words that are related to
each other will often appear in the same documents. The system
illustrated in [1] used TF-IDF and LSA methods to discover groups
of words that are equivalent in their meaning.
The authors of [19] presented a topic-based recommendation
system that combines traditional collaborative ltering with topic
modeling based on LDA model. Nascimento et al. [16] provided
another example of a content-based RS for scientic articles recommendation. Their proposed solution utilized the n-grams models
to generate queries from a particular article that is presented by
the user, and then submit the generated queries on publicly available web sources of scientic papers. Their method used the titles
and abstracts of the articles, and the similarity of the papers was
calculated through the cosine similarity method.
In [21], the authors presented PURE, a content-based recommendation system that works on documents’ titles and abstracts
of the PubMed dataset. It automatically captures user preferences
by using her response to the presented papers. Furthermore, PURE
uses the well-known TF-IDF method and learns probabilistic model
for computing relevant documents based on selected documents
added by the user.
Docear [3] is an academic literature suite to search, organize,
and create research articles. Its recommender system uses contentbased methods to recommend articles. It builds a user model using
the mind maps created by the user, and match it with Docear Digital
2https://code.google.com/archive/p/word2vec/
3http://citeseerx.ist.psu.edu/index
4https://scholar.google.it/
Library. The authors claimed to have achieved good results based
on the number of clicks gained, through around thirty thousand
tested recommendation results.
Some authors suggested using collaborative ltering and ratings
[6]. Ratings can be generated by considering citations as ratings.
They can also be implicitly inferred by monitoring user’s actions
such as downloading or bookmarking a paper. The system proposed in [6], called Scienstein, combines dierent methods for providing literature recommendation. Scienstein integrated the traditional keyword-based search with citation analysis, author analysis,
source analysis, implicit and explicit ratings. Instead of entering
just keywords for searching documents, a user may provide entire
documents as an input, include reference lists, and provides implicit and explicit ratings in order to improve the recommendation
process.
In [10] a personalized academic research paper RS is presented.
It recommends articles relevant to the research eld of the users,
supposing that researchers like their own articles. Based on this
assumption papers similar to the ones previously written by users
are recommended as relevant to them. This system uses a web
crawler to retrieve research papers from IEEE Xplore5 and ACM
digital library6. It measures text similarity using bag-of-words and
KNN methods to determine the similarity between two research
papers and uses collaborative ltering methods to recommend the
items.
Finally, the research of [14] proposed a novel method for integrating structural and contextual information to build a context specic
network for generating recommendations for similar PubMed articles.
Some of the used methods have drawbacks, which limit their ability to deliver recommendations. For example, in the citation-based
approaches, not all research papers are cited and hence cannot be
recommended. Also, reference lists can contain irrelevant citations
just because the author believes that well-known papers should be
cited, or in order to promote other publications although they are
irrelevant for the citing paper [6]. In addition, text-based RSs cannot
identify related papers if dierent terms are used. Moreover, the
basic topic modeling methods which are based on the traditional
bag-of-words techniques, have the disadvantage that topics are
probability distributions over a collection of words that represent
a document, it does not consider the semantic relations between
words. Thus, it may result in redundant topics that contain dierent
words, but with the same meaning. In addition, these techniques
don’t take the context of the words into consideration.
Collaborative ltering in the research paper recommender systems domain would be ineective as there is a huge number of
papers compared with the number of users, and only few users
rated the same papers. In domains such as movie recommendations,
there are few items and many users such as in MovieLens7 recommender system, and most movies have been watched and rated by
at least some users [2]. Therefore, like-minded users can be found
and recommendations can be given eectively.
The use of deep neural networks for Natural Language Processing (NLP) has recently received much attention; it provides high
5http://ieeexplore.ieee.org/Xplore/home.jsp
6http://dl.acm.org/
7https://grouplens.org/datasets/movielens/
UMAP 2017 Doctoral Consortium
UMAP’17, July 9-12, 2017, Bratislava, Slovakia
328
quality semantic word representations. These models are usually
trained on large amounts of data. In the last few years, deep neural
network models have been applied to tasks ranging from machine
translation to question answering, but not much attention is paid to
the RSs area. For instance, in [15] and [12], the authors showed that
LSTM can be used to build a language model and assess semantic
similarity between sentences. To the best of our knowledge, there
have been no work done before for recommending scientic articles
based on LSTM.
4 RESEARCH METHODOLOGY
4.1 Data Collection
PubMed is one of the largest public databases in biological and
medical sciences. It contains more than 26 million citations for
biomedical literature from MEDLINE8, life science journals, and
online books. Each paper in MEDLINE is indexed using a controlled
vocabulary, called Medical Subject Headings (MeSH), which is used
to describe the main topics discussed. The set of MeSH terms is
manually assigned by biomedical experts who scan each article.
We will use BioPython [4] library to crawl the PubMed database
through PubMed Central (PMC) APIs9, and download titles and
abstracts of sample papers. Then, we will concatenate the title and
abstract for each paper.
We will use only the titles and abstracts of the papers to calculate
the similarity between the papers and recommend similar articles
since they will be always publicly available. In addition, fetching
and analyzing the full text of every paper would signicantly slow
down the process.
4.2 Language Modeling
Recurrent Neural Networks (RNNs) are deep models that are widely
used when dealing with sequential data, unlike the traditional neural networks which assume that all inputs and outputs are independent of each other. RNNs have shown great promise in image
and video captioning, time series prediction, NLP, text and music
generation and much more tasks [11]. LSTM networks are a type of
RNNs [8], that allows the model to learn longer-term dependencies
than a traditional RNN.
Word2vec is one of state-of-the-art word embedding techniques,
published by Google in 2013 [13], that learns distributed representations for words. It converts text into a numerical form that deep
nets can understand. The idea of the word vectors is to represent
a word by a dense vector in a semantic space, and other vectors
close by should be semantically similar. Other deep recurrent neural
network architectures had been previously proposed for learning
word representations, but the major problem with those methods
was the long time required to train the models. Word2vec learns
quickly compared to other models. In general, parallelization is
used to speed up the training process, so that larger models can be
trained in a reasonable amount of time.
We will create a language model using the word2vec and LSTM
techniques, in order to be used for measuring the relatedness of
the scientic publications. Word2vec will be used for computing
individual word representation for all the words from the collected
8https://www.nlm.nih.gov/bsd/pmresources.html
9https://www.ncbi.nlm.nih.gov/home/develop/api.shtml
papers. We intend to use gensim Python package [18] for this purpose. Then, we will train a LSTM model using the vectors of words
resulted from the word2vec step, in order to learn the semantic
content of the research papers. This model will be used for the
purpose of document embedding. In other words, the LSTM model
will be utilized for representing an article according to the semantic
representation of its words.
4.3 User Prole Creation
In order to recommend papers to users, we will model users’ interests in user proles. These proles will represent users’ tastes and
opinions about papers. Such proles could contain both long-term
and short-term interests, gathered explicitly or implicitly.
One of the explicit feedback forms is to ask the user to explicitly
specify papers which are relevant to her (which are satisfying information need). Implicit feedback is to collect data about user’s
preferences based on observations of the user’s behavior from the
transactions log like viewing abstracts or full-text articles.
In our study, we will build user proles from users’ explicit and
implicit short-term interests; user can explicitly add or delete the
preferred articles as an input to the recommender system, from
which the title and abstract will be extracted and the semantic
vectors will be calculated. In addition, we will infer users’ interests
based on viewing abstracts and clicking links to full-text articles
while querying the PubMed Central in the same query session.
We will assign high weights to the vectors that characterize the
topics which the user is interested in, based on the user’s actions;
for instance, viewing abstract only should be weighted lower than
viewing the whole paper.
4.4 Recommendation of papers
Using the constructed user prole and the feature vectors of the set
of the candidate papers to recommend, the system will compute the
cosine similarities between the papers in the user prole and the
ones in the corpus, considering the weights that take into account
the user feedback. Highly relevant papers will be ranked rst for
presentation. Therefore, the correctness of an item in the ranking
list should be weighted by its position in the ranking. Figure 1
shows the high level architecture of the proposed method.
4.5 Evaluation Methods
We will compare the results from our recommendation engine with
PubMed MeSH-based baseline. Similar to the PubMed recommender
system proposed in [14], we will use MeSH-based paper similarities
as the gold standard to evaluate the dierent methods. In the MeSHbased paper similarities, the quality of predictions for each paper is
dened based on its distance compared to similar predictions from
MeSH scores.
In addition to the described oine evaluation, we intend to conduct user studies in order to evaluate our proposed method. The
participants will be asked to read and indicate how the recommended papers are relevant to their research.
5 CURRENT PROGRESS
The thesis is now in the initial phase. We are currently studying the
dierent recommendation methodologies, deep learning techniques,
UMAP 2017 Doctoral Consortium
UMAP’17, July 9-12, 2017, Bratislava, Slovakia
329
Figure 1: The proposed method
and reviewing the literature of the research paper recommender
systems. The next steps will be the implementation of the proposed
approach and the evaluation of its performance in comparison with
other approaches in the literature"
5,"INTRODUCTION
In the age of ever-increasing volume and complexity of the
internet, millions of users have unrestricted access to vast
amounts of content that allows for privileges unimaginable
several decades ago, such as access to knowledge bases or
latest news within just a few clicks. However, due to internet’s non-restrictive nature and, in certain countries, legal
protection of free speech which also includes hate speech
[4], some users misuse the medium to promote offensive and
hateful language, which mars experience of regular users,
affects business of online companies, and may even have severe real-life consequences [1]. To mitigate these detrimental
effects, many companies (including Yahoo, Facebook, and
YouTube) strictly prohibit hate speech on websites they own
and operate, and implement algorithmic solutions to discern
hateful content. However, scale and multifacetedness of the
task renders it a difficult endeavour, and hate speech still
remains a problem in online user comments.
Curiously, despite prevalence and large impact of online
hate speech, to the best of our knowledge there exist only
a few published works addressing this problem. In [1] (see
also references therein) authors extract linguistic and bagof-words (BOW) features and explore several classifiers to
detect hateful tweets following the 2013 incident in WoolPermission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage, and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact
the owner/author(s). Copyright is held by the author/owner(s).
WWW 2015 Companion, May 18–22, 2015, Florence, Italy.
ACM 978-1-4503-3473-0/15/05.
http://dx.doi.org/10.1145/2740908.2742760.
wich, UK. In [6] authors use BOW representation of user
comments and train Support Vector Machine to filter antisemitic content. Motivated by [6], authors of [2] use BOW
and Na¨ ıve Bayes to flag racist comments. Interestingly, in all
these works authors comment on limitations of BOW-based
representation of text. This especially holds in the context
of hate speech where offenders often use simple yet effective
tricks to obfuscate their comments and make them more difficult for automatic detection (such as replacing or removing
characters of offensive words), while still keeping the intent
clear to a human reader. This results in high-dimensionality
and large sparsity of the problem, making models susceptible to overfitting [6]. To address these issues, in this work
we propose an approach that learns low-dimensional, distributed representations of user comments, allowing for efficient training of effective hate speech detectors.
We note that the task is different from, albeit related to,
sentiment analysis [5] as there are no shades of hate speech
and, unlike hate speech, even negative sentiment provides
useful and actionable insights. Related work also includes
attempts to remove offensive words without modifying the
underlying meaning of comments [7]. This approach is however not applicable to hate speech detection as the conveyed
message itself is considered harmful and should be removed.
2. PROPOSED APPROACH
We propose a two-step method for hate speech detection.
First, we use paragraph2vec [3] for joint modeling of comments and words, where we learn their distributed representations in a joint space using the continuous BOW (CBOW)
neural language model. This results in low-dimensional text
embedding, where semantically similar comments and words
reside in the same part of the space. Then, we use the embeddings to train a binary classifier to distinguish between
hateful and clean comments. During inference, for newly
observed comment, we infer representation by olding in""
using already learned word embeddings, as detailed in [3].
2.1 Neural language model
Neural language models take advantage of word order,
and state the same assumption of n-gram language models
that words that are close in a sentence are also statistically
more dependent. In this work, we use the CBOW model
as a component of paragraph2vec [3], which, based on the
surrounding words, tries to predict the central word, as well
as the user comment the words belong to.
More formally, let us assume we are given a set D of M
documents, D = fd1; d2; : : : ; dMg, where each document dm
Figure 1: Nearest neighbors for swearword ck""
is a sequence of Tm words, dm = (wm1; wm2; : : : ; wm;Tm).
We aim to simultaneously learn low-dimensional representations of documents and words in a common vector space, and
represent each document and word as a continuous feature
vector of dimensionality D. Then, the objective of paragraph2vec is to maximize the data log-likelihood,
L = X
dm2D
log P(dmjwm1; wm2; : : : ; wm;Tm)
+ X
dm2D
X
wmt2dm
log P(wmtjwm;t−c; : : : ; wm;t+c; dm);
(1)
where c is the length of the context for word sequences.
When modeling the probability of a document and the probability of a word, we define both models using a softmax
function. Probability of the central word wmt is defined as
P(wmtjwm;t−c; : : : ; wm;t+c; dm) = Pexp( V w=1 v exp( ¯>vw 0v ¯mt >v )w 0 ); (2)
where v0
wmt is the output vector representation of wmt, V is
vocabulary size, and v ¯ is an averaged vector representation
of the context (including the containing comment dm),
v ¯ =
1
2c + 1
(vdm + X
−c≤i≤c;i6=0
vw
m;t+i): (3)
We similarly define P(dmjwm1; : : : ; wm;Tm), probability of a
comment, by replacing appropriate variables in (2) and (3).
We use stochastic gradient ascent to maximize (1). However, compute time of r log P in (1) is proportional to vocabulary size, which may be expensive in practice. As an alternative we use hierarchical soft-max [3], which significantly
reduces time complexity and allows for efficient training.
3. EMPIRICAL ANALYSIS
We evaluated our approach on a large-scale data set of user
comments collected on Yahoo Finance website. The data
set comprises 56;280 comments containing hate speech and
895;456 clean comments generated by 209;776 anonymized
users, collected and editorially labeled over a 6-month period. We preprocessed the text by lowercasing and removing
stopwords and special characters, resulting in a vocabulary
size of V = 304;427. This makes the used data the largest
hate speech data set considered thus far in the literature.
Table 1: AUC of various methods
Algorithm AUC
BOW (tf) 0:7889
BOW (tf-idf) 0:6933
paragraph2vec 0:8007
We compared our method to the current state-of-the-art
methods employing BOW representation, using tf and tfidf encodings. We set D = 200 and c = 5 for paragraph2vec,
while training on the entire data for 5 iterations. Once we
learned vector representations we trained logistic regression
classifier, and report the classification performance of competing methods after 5-fold cross-validation.
We first validated that the paragraph2vec representations
are meaningful, and that semantically similar words are close
to each other in the embedding space. This is illustrated in
Figure 1, where we show a wordcloud of nearest neighbors
in terms of cosine distance to obscured swearword ck"". We
can see that using paragraph2vec resulted in this word, its
variations, as well as semantically related swearwords having
similar low-dimensional representations, grouping them in
the same part of the vector space. Interestingly, the model
even found some non-obvious swearwords, such as \chit"".
Next, we validated utility of the learned vectors on the
hate speech classification task. To this end, in Table 1 we
report Area under the Curve (AUC), where we see that the
proposed method outperformed the competing approaches.
Interestingly, tf encoding achieved better performance than
tf-idf and obtained very competitive AUC, which explains
why many of the existing approaches use BOW representation. Nevertheless, paragraph2vec obtained higher AUC
than either BOW model, while requiring less memory and
training time to learn very effective hate speech detectors.
The results clearly indicate the benefits of the proposed approach, and constitute a step towards solution of the problem of hate speech detection in online user comments."
6,"Introduction
What constitutes hate speech and when does it differ from
offensive language? No formal definition exists but there is
a consensus that it is speech that targets disadvantaged social groups in a manner that is potentially harmful to them
(Jacobs and Potter 2000; Walker 1994). In the United States,
hate speech is protected under the free speech provisions of
the First Amendment, but it has been extensively debated in
the legal sphere and with regards to speech codes on college
campuses. In many countries, including the United Kingdom, Canada, and France, there are laws prohibiting hate
speech, which tends to be defined as speech that targets minority groups in a way that could promote violence or social
disorder. People convicted of using hate speech can often
face large fines and even imprisonment. These laws extend
to the internet and social media, leading many sites to create their own provisions against hate speech. Both Facebook
and Twitter have responded to criticism for not doing enough
to prevent hate speech on their sites by instituting policies
to prohibit the use of their platforms for attacks on people
Copyright  c 2017, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
based on characteristics like race, ethnicity, gender, and sexual orientation, or threats of violence towards others.1
Drawing upon these definitions, we define hate speech
as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate,
or to insult the members of the group. In extreme cases this
may also be language that threatens or incites violence, but
limiting our definition only to such cases would exclude a
large proportion of hate speech. Importantly, our definition
does not include all instances of offensive language because
people often use terms that are highly offensive to certain
groups but in a qualitatively different manner. For example
some African Americans often use the term n*gga2 in everyday language online (Warner and Hirschberg 2012), people
use terms like h*e and b*tch when quoting rap lyrics, and
teenagers use homophobic slurs like f*g as they play video
games. Such language is prevalent on social media (Wang
et al. 2014), making this boundary condition crucial for any
usable hate speech detection system .
Previous work on hate speech detection has identified this
problem but many studies still tend to conflate hate speech
and offensive language. In this paper we label tweets into
three categories: hate speech, offensive language, or neither.
We train a model to differentiate between these categories
and then analyze the results in order to better understand
how we can distinguish between them. Our results show that
fine-grained labels can help in the task of hate speech detection and highlights some of the key challenges to accurate
classification. We conclude that future work must better account for context and the heterogeneity in hate speech usage.
Related Work
Bag-of-words approaches tend to have high recall but lead
to high rates of false positives since the presence of offensive words can lead to the misclassification of tweets as
hate speech (Kwok and Wang 2013; Burnap and Williams
2015). Focusing on anti-black racism, Kwok and Wang find
1Facebook’s policy can be found here: www.facebook.com/
communitystandards#hate-speech. Twitter’s policy can be found
here: support.twitter.com/articles/20175050.
2Where present, the “*” has been inserted by us and was not part
of the original text. All tweets quoted have been modified slightly
to protect user’s identities while retaining their original meaning.
Proceedings of the Eleventh International AAAI Conference on Web and Social Media (ICWSM 2017)
512
that 86% of the time the reason a tweet was categorized as
racist was because it contained offensive words. Given the
relatively high prevalence of offensive language and “curse
words” on social media this makes hate speech detection
particularly challenging (Wang et al. 2014). The difference
between hate speech and other offensive language is often
based upon subtle linguistic distinctions, for example tweets
containing the word n*gger are more likely to be labeled as
hate speech than n*gga (Kwok and Wang 2013). Many can
be ambiguous, for example the word gay can be used both
pejoratively and in other contexts unrelated to hate speech
(Wang et al. 2014).
Syntactic features have been leveraged to better identify
the targets and intensity of hate speech, for example sentences where a relevant noun and verb occur (e.g. kill and
Jews) (Gitari et al. 2015), the POS trigram “DT jewish NN”
(Warner and Hirschberg 2012), and the syntactic structure I
<intensity > <user intent > <hate target >, e.g. “I f*cking
hate white people” (Silva et al. 2016).
Other supervised approaches to hate speech classification
have unfortunately conflated hate speech with offensive language, making it difficult to ascertain the extent to which
they are really identifying hate speech (Burnap and Williams
2015; Waseem and Hovy 2016). Neural language models
show promise in the task but existing work has used training
data has a similarly broad definition of hate speech (Djuric et
al. 2015). Non-linguistic features like the gender or ethnicity
of the author can help improve hate speech classification but
this information is often unavailable or unreliable on social
media (Waseem and Hovy 2016).
Data
We begin with a hate speech lexicon containing words and
phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched
for tweets containing terms from the lexicon, resulting in a
sample of tweets from 33,458 Twitter users. We extracted
the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had
them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories:
hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail.
Users were asked to think not just about the words appearing in a given tweet but about the context in which they were
used. They were instructed that the presence of a particular
word, however offensive, did not necessarily indicate a tweet
is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%.
We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no
majority class. This results in a sample of 24,802 labeled
tweets.
Only 5% of tweets were coded as hate speech by the majority of coders and only 1.3% were coded unanimously,
demonstrating the imprecision of the Hatebase lexicon. This
is much lower than a comparable study using Twitter, where
11.6% of tweets were flagged as hate speech (Burnap and
Williams 2015), likely because we use a stricter criteria for
hate speech. The majority of the tweets were considered
to be offensive language (76% at 2/3, 53% at 3/3) and the
remainder were considered to be non-offensive (16.6% at
2/3, 11.8% at 3/3). We then constructed features from these
tweets and used them to train a classifier.
Features
We lowercased each tweet and stemmed it using the Porter
stemmer,3 then create bigram, unigram, and trigram features, each weighted by its TF-IDF. To capture information about the syntactic structure we use NLTK (Bird, Loper,
and Klein 2009) to construct Penn Part-of-Speech (POS) tag
unigrams, bigrams, and trigrams. To capture the quality of
each tweet we use modified Flesch-Kincaid Grade Level and
Flesch Reading Ease scores, where the number of sentences
is fixed at one. We also use a sentiment lexicon designed for
social media to assign sentiment scores to each tweet (Hutto
and Gilbert 2014). We also include binary and count indicators for hashtags, mentions, retweets, and URLs, as well as
features for the number of characters, words, and syllables
in each tweet.
Model
We first use a logistic regression with L1 regularization to
reduce the dimensionality of the data. We then test a variety
of models that have been used in prior work: logistic regression, na¨ ıve Bayes, decision trees, random forests, and linear
SVMs. We tested each model using 5-fold cross validation,
holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the
models and parameters we find that the Logistic Regression
and Linear SVM tended to perform significantly better than
other models. We decided to use a logistic regression with
L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers (Burnap
and Williams 2015; Waseem and Hovy 2016). We trained
the final model using the entire dataset and used it to predict
the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and
the class label with the highest predicted probability across
all classifiers is assigned to each tweet. All modeling was
performing using scikit-learn (Pedregosa and others
2011).
Results
The best performing model has an overall precision 0.91,
recall of 0.90, and F1 score of 0.90. Looking at Figure 1,
however, we see that almost 40% of hate speech is misclassified: the precision and recall scores for the hate class are
0.44 and 0.61 respectively. Most of the misclassification occurs in the upper triangle of this matrix, suggesting that the
3We verified that the stemmer did not remove important information by reducing key terms to the same stem, e.g. f*gs and
f*ggots stem to f*g and f*ggot.
513
Hate Offensive Neither
Predicted categories
Neither Offensive Hate
True categories
0.02 0.03 0.95
0.05 0.91 0.04
0.61 0.31 0.09
Figure 1: True versus predicted categories
model is biased towards classifying tweets as less hateful or
offensive than the human coders. Far fewer tweets are classified as more offensive or hateful than their true category;
approximately 5% of offensive and 2% of innocuous tweets
have been erroneously classified as hate speech. To explore
why these tweets have been misclassified we now look more
closely at the tweets and their predicted classes.
Tweets with the highest predicted probabilities of being
hate speech tend to contain multiple racial or homophobic
slurs, e.g. @JuanYeez shut yo beaner ass up sp*c and hop
your f*ggot ass back across the border little n*gga and RT
@eBeZa: Stupid f*cking n*gger LeBron. You flipping jungle bunny monkey f*ggot. Other tweets tend to be correctly
identified as hate when they contained strongly racist or homophobic terms like n*gger and f*ggot. Interestingly, we
also find cases where people use hate speech to respond
to other hate speakers, such as this tweet where someone
uses a homophobic slur to criticize someone else’s racism:
@MrMoonfrog @RacistNegro86 f*ck you, stupid ass coward b*tch f*ggot racist piece of sh*t.
Turning to true hate speech classified as offensive it appears that tweets with the highest predicted probability of
being offensive are genuinely less hateful and were perhaps mislabeled, for example When you realize how curiosity is a b*tch #CuriosityKilledMe may have been erroneously coded as hate speech if people thought that curiosity was a person, and Why no boycott of racist ”redskins”?
#Redskins #ChangeTheName contains a slur but is actually
against racism. It is likely that coders skimmed these tweets
too quickly, picking out words or phrases that appeared to
be hateful without considering the context. Turning to borderline cases, where the probability of being offensive is
marginally higher than hate speech, it appears that the majority are hate speech, both directed towards other Twitter
users, @MDreyfus @NatFascist88 Sh*t your ass your moms
p*ssy u Jew b*stard. Ur times coming. Heil Hitler! and general hateful statements like My advice of the day: If your a
tranny...go f*ck your self!. These tweets fit our definition of
hate speech but were likely misclassified because they do
not contain any of the terms most strongly associated with
hate speech. Finally, the hateful tweets incorrectly labeled
as neither tend not to contain hate or curse words, for example If some one isn’t an Anglo-Saxon Protestant, they have
no right to be alive in the US. None at all, they are foreign
filth contains a negative term, filth but no slur against a particular group. We also see that rarer types of hate speech,
for example this anti-Chinese statement Every slant in #LA
should be deported. Those scum have no right to be here.
Chinatown should be bulldozed, are incorrectly classified.
While the classifier performs well at prevalent forms of hate
speech, particularly anti-black racism and homophobia, but
is less reliable at detecting types of hate speech that occur
infrequently, a problem noted by Nobata et al. (2016).
A key flaw in much previous work is that offensive language is mislabeled as hate speech due to an overly broad
definition. Our multi-class framework allows us to minimize
these errors; only 5% of our true offensive language was labeled as hate. The tweets correctly labeled as offensive tend
to contain curse words and often sexist language, e.g. Why
you worried bout that other h*e? Cuz that other h*e aint
worried bout another h*e and I knew Kendrick Lamar was
onto something when he said “I call a b*tch a b*tch, a h*e a
h*e, a woman a woman”. Many of these tweets contain sexist terms like b*tch, p*ssy, and h*e. Human coders appear to
consider racists or homophobic terms to be hateful but consider words that are sexist and derogatory towards women
to be only offensive, consistent prior findings (Waseem and
Hovy 2016).
Looking at the tweets misclassified as hate speech we see
that many contain multiple slurs, e.g. @SmogBaby: These
h*es be lyin to all of us n*ggas and My n*gga mister meaner
just hope back in the b*tch. While these tweets contain terms
that can be considered racist and sexist it is apparent than
many Twitter users use this type of language in their everyday communications. When they do contain racist language
they tend to contain the term n*gga rather than n*gger, in
line with the findings of Kwok and Wang (2013). We also
found a few recurring phrases such as these h*es ain’t loyal
that were actually lyrics from rap songs that users were quoting. Classification of such tweets as hate speech leads us
to overestimate the prevalence of the phenomenon. While
our model still misclassifies some offensive language as hate
speech we are able to avoid the vast majority of these errors
by differentiating between the two.
Finally, turning to the neither class, we see that tweets
with the highest predicted probability of belonging to this
class all appear to be innocuous and were included in the
sample because they contained terms included in the Hatebase lexicon such as charlie and bird that are generally not
used in a hateful manner. Tweets with overall positive sentiment and higher readability scores are more likely to belong to this class. The tweets in this category that have
been misclassified as hate or offensive tend to mention race,
sexuality, and other social categories that are targeted by
hate speakers. Most appear to be misclassifications appear
514
to be caused by on the presence of potentially offensive
language, for example He’s a damn good actor. As a gay
man it’s awesome to see an openly queer actor given the
lead role for a major film contains the potentially the offensive terms gay and queer but uses them in a positive sense.
This problem has been encountered in previous research
(Warner and Hirschberg 2012) and illustrates the importance of taking context into account. We also found a small
number of cases where the coders appear to have missed
hate speech that was correctly identified by our model, e.g.
@mayormcgunn @SenFeinstein White people need those
weapons to defend themselves from the subhuman trash your
sort unleashes on us. This finding is consistent with previous work that has found amateur coders to often be unreliable at identifying abusive content (Nobata et al. 2016;
Waseem 2016).
Conclusions
If we conflate hate speech and offensive language then we
erroneously consider many people to be hate speakers (errors in the lower triangle of Figure 1) and fail differentiate between commonplace offensive language and serious
hate speech (errors in the upper triangle of Figure 1). Given
the legal and moral implications of hate speech it is important that we are able to accurately distinguish between
the two. Lexical methods are effective ways to identify potentially offensive terms but are inaccurate at identifying
hate speech; only a small percentage of tweets flagged by
the Hatebase lexicon were considered hate speech by human coders.4 While automated classification methods can
achieve relatively high accuracy at differentiating between
these different classes, close analysis of the results shows
that the presence or absence of particular offensive or hateful terms can both help and hinder accurate classification.
Consistent with previous work, we find that certain
terms are particularly useful for distinguishing between hate
speech and offensive language. While f*g, b*tch, and n*gga
are used in both hate speech and offensive language, the
terms f*ggot and n*gger are generally associated with hate
speech. Many of the tweets considered most hateful contain
multiple racial and homophobic slurs. While this allows us
to easily identify some of the more egregious instances of
hate speech it means that we are more likely to misclassify
hate speech if it doesn’t contain any curse words or offensive
terms. To more accurately classify such cases we should find
sources of training data that are hateful without necessarily
using particular keywords or offensive language.
Our results also illustrate how hate speech can be used in
different ways: it can be directly send to a person or group
of people targeted, it can be espoused to nobody in particular, and it can be used in conversation between people. Future work should distinguish between these different uses
and look more closely at the social contexts and conversa-
4If a lexicon must be used we propose that a smaller lexicon with higher precision is preferable to a larger lexicon with
higher recall. We have made a more restricted version of the
Hatebase lexicon available here: https://github.com/t-davidson/
hate-speech-and-offensive-language.
tions in which hate speech occurs. We must also study more
closely the people who use hate speech, focusing both on
their individual characteristics and motivations and on the
social structures they are embedded in.
Hate speech is a difficult phenomenon to define and is not
monolithic. Our classifications of hate speech tend to reflect
our own subjective biases. People identify racist and homophobic slurs as hateful but tend to see sexist language as
merely offensive. While our results show that people perform well at identifying some of the more egregious instances of hate speech, particularly anti-black racism and homophobia, it is important that we are cognizant of the social
biases that enter into our algorithms and future work should
aim to identify and correct these biases."
7,"Introduction
The Internet has become the means of expressing opinion and view of consumers of
products and services. Information contained in these reviews is of great value to other
consumers as well as the companies that own those products and services. However, consumer
reviews are often unstructured and noisy. Manual analysis of this huge amount of data for
information is impossible. Automatic sentiment analysis of customer reviews has therefore,
become a priority for the research community in the recent years.
Conventional sentiment analysis of text focuses on the opinion of the entire text or the
308
The 2017 Conference on Computational Linguistics and Speech Processing
ROCLING 2017, pp. 308-322
 The Association for Computational Linguistics and Chinese Language Processing
sentence. In the case of consumer reviews, it has been observed that customers often talk about
multiple aspects of an entity and express an opinion on each aspect separately rather than
expressing opinion towards the entity as a whole [1]. Aspect Based Sentiment Analysis (ABSA)
has emerged to tackle this issue. The goal of Aspect Based Sentiment Analysis is to identify
aspects present in the text, and the opinion expressed for each aspect [2].
One of the most crucial tasks of ABSA is to extract aspects from the review text. The state
of the art systems have trouble in working with multiple domains, detecting multiple aspects
in a single sentence, handling a large number of hierarchical aspects and detecting implicit
aspects where the aspect is to be inferred from the context [3]. The objective of our research is
to develop new techniques that would be able to perform aspect extraction from customer
reviews with high accuracy, across multiple domains.
A one-vs-rest Support Vector Machine (SVM) classifier and a list of carefully selected
features are at the core of our supervised machine learning approach for aspect extraction. We
identified that when Mean Embeddings are provided as a feature to the SVM classifier, results
get improved significantly. The system was further enhanced using a clever text pre-processing
pipeline complemented with context sensitive spell correction. Our system is able to
outperform the best results submitted for SemEval-2016 Task 5i, in both restaurant and laptop
domains.
The rest of the paper is organized as follows. In section 2, related work is discussed.
Section 3 explains the SemEval-2016 Task 5 dataset. Section 4 elaborates our system in detail.
Experimental results are discussed in section 5. Finally, section 6 concludes our paper.
309
2. Related Work
Aspect extraction is an important and challenging task in sentiment analysis [4]. There is
previous work on aspect extraction based on different approaches. Frequency based methods
of aspect extraction consider frequent words likely to be aspects. Frequent nouns and noun
phrases are considered as frequent words in this approach [3]. Hu and Liu [5] consider single
nouns and compound nouns to be aspects. However, not each frequent word in a review
sentence refers an aspect, thus this assumption leads to low accuracies in the aspect extraction
process.
Syntax-based methods for aspect extraction use syntactical relations between a sentiment
word and the aspect it is about. The ability to find low frequent aspects is an advantage in this
approach. Still, to have a good recall, many grammatical relations needed to be found. To
address this challenge, the double propagation algorithm is used by Qiu et al. [6] and Zhang et
al. [7]. Yet, the presence of implicit aspects is not addressed in this approach.
It has been observed that machine learning approaches have excelled in aspect extraction
task in the recent literature [3]. Many supervised classifiers have been used for aspect extraction
in the literature [8].
Hercig et al. [9] present a system that uses a maximum entropy classifier for aspect
category detection. The system is fed in with a massive number of features in order to get
competitive results. These features are categorized under semantic, constrained and
unconstrained features. However, despite using many features, this classifier was not able to
outrank the best performing systems at SemEval-2016 Task 5. It is observed that most of the
best performing supervised machine learning models use SVM [10].
In contrast to the supervised machine learning methods, Toh et al. [11] presented a hybrid
approach, which uses deep learning techniques along with a binary classifierii. The model has
been evaluated with restaurant-domain and laptop-domain datasets of SemEval-2016 Task 5.
This model has achieved the best score in the SemEval-2016 Task 5, with an F1 score of 0.7303
310
for restaurant domain and 0.5194 for the laptop domain. We consider these results as our
benchmark results. We try to outperform this complex system using a simple SVM combined
with carefully crafted features.
3. SemEval-2016 Task 5 Dataset
The existence of a dataset such as the one provided by SemEval-2016 Task 5 provides a
standardized evaluation technique to publish our results, and it can be compared fairly with
other systems, which are evaluated on the same dataset. Previously many different researchers
used various datasets in their publications, making it difficult to compare and contrast the
techniques discussed. SemEval-2016 Task 5 consists of several subtasks and slots [12]. Our
system focuses on Slot 1 - Aspect category identification in Subtask 1 - sentence level ABSA.
Details of subtask 1 is as follows,
The task is to identify all opinion tuples when opinionated text is given about a target
entity. Subtask 1 is composed of 3 slots.
• Slot 1 - Aspect category: Identify entity E and attribute A pairs (denoted E#A) in a
given sentence. E and A are chosen from predefined entity types and attribute labels,
respectively.
• Slot 2 - Opinion target extraction: Extraction of expression used in the sentence to refer
to the entity identified in E#A pair.
• Slot 3 - Sentiment polarity: Identify polarity labels (“positive”, “negative”, “neutral”)
for each identified E#A pair.
Our goal is to identify all aspect categories mentioned in each sentence. SemEval-2016
Task 5 dataset of English reviews for restaurant (training: 2000, testing 676 sentences) and
laptop (training: 2500, testing 808 sentences) domains are used to train our SVM classifier.
Training sentences have been annotated for opinions with respective aspect category while
taking the context of the whole review into consideration. The sentences are classified under
311
12 and 81 classes in the restaurant and laptop domains, respectively.
4. System Description
In this section, we present our aspect extraction system. Our goal is to extract all the
relevant aspect categories for a given sentence. We developed an SVM classifier for this task
and evaluated its accuracy. The structure of our system is illustrated in Figure 1.
Figure 1 System Structure
4.1 Preprocessing
The dataset is stripped out of unnecessary content such as HTML, and the encoding of
text is corrected. The pipeline neutralizes incorrect consecutive letters in words entered due to
the excitement of the reviewer (s.a “sooooo”) as done by Machácek [13].
We then present the sentences for spell correction. Both isolated wordiii and context
sensitive spell correctioniv were experimented with. We observed that context sensitive spell
correction performs far superior than isolated word spell correction. For example, consider the
sentence “I lve in the neightborhood and lve their piza”. Individual word spell correction
corrected both occurrences of “lve” by “live” where context sensitive spell correction replaces
the first “lve” by “live” and the second “lve” by “love”.
312
The reason to specifically use this service is that Bing spell check APIiv provides correct
context sensitive spell correction via machine learning and a statistical machine translation
based on a highly contextual algorithm.
After the spell correction, English concatenations present in the sentences were expanded
(i.e. 'can’t' become 'cannot'). Punctuations present in the data were removed and all symbols
were replaced with their word meanings using regular expressions (i.e. - % will be replaced
using percent). Moreover, all occurrences of numerical prices and “$” symbols are replaced
with a price indicator word. Finally, we remove commonly occurring English articles such as
“a” and “an” from the text. Converting all the characters to lowercase is not performed at the
preprocessing stage since text features such as Named Entities are case sensitive. However,
when creating the lemmatized bag of words, the text is lowercased.
4.2 Features
The SVM classifier requires informative and effective features to improve the results. We
came up with following features, which were extracted from the preprocessed text to train and
test the SVM classifier. We identified some of these features from recent publications made on
SemEval-2016 Task 5. Moreover, we introduce some of our own features, which were not tried
out in previous studies for aspect extraction.
Shown below is the feature combination that contributed to the best F1 score of the SVM
classifier according to 5-fold cross validation. The last 3 features on the list are newly
introduced.
4.2.1 Lemmatized bag of words
Used Stanford CoreNLPv to tokenize the text, and the stop words were removed. Then
the tokens were lemmatized and were provided as a feature to the SVM. UWB system [9] and
BUTknot system [13] at SemEval-2016 have lemmatized the text in a similar manner.
Lemmatized bag of words is the base feature of our system.
313
4.2.2 Custom built word lists
Restaurant domain - We manually compiled a collection of restaurant food and drink
names. The food name list contains 1302 items and the drinks list consists of 1400 items.
Laptop domain - We built a collection of laptop manufacturer names, operating systems,
processors, display resolutions, CPU quality, hard disks and laptop model series.
Custom word lists were used in past research and could be observed in the BUTknot
system [13] at SemEval-2016.
4.2.3 Opinion target annotations
We extracted the opinion targets that were annotated in the training dataset. Lemmatized
opinion targets were fed as a feature to the SVM with the respective category of the opinion
target. BUTknot system [13] at SemEval-2016 has taken a similar approach.
4.2.4 Frequent words per category based on tf-idf score
We built a custom list of frequent words per category in the laptop domain. UWB system
[9] at SemEval-2016 has implemented this feature in their approach. We used equation (3) to
extract most important words for each of the categories and manually filtered noise words such
as stop words and numbers. We created a document per category by combining all the sentences
belonging to a particular aspect category together.
𝒕𝒇(𝒘𝒐𝒓𝒅, 𝒄𝒂𝒕𝒆𝒈𝒐𝒓𝒚) = 𝒇𝒘
𝒏
𝒘
(1)
𝒊𝒅𝒇(𝒘𝒐𝒓𝒅, 𝒄𝒂𝒕𝒆𝒈𝒐𝒓𝒊𝒆𝒔) = 𝐥𝐨𝐠 (𝟏+ 𝒄𝒏 𝒄𝒘) (2)
𝒕𝒇 − 𝒊𝒅𝒇 (𝒘𝒐𝒓𝒅, 𝒄𝒂𝒕𝒆𝒈𝒐𝒓𝒚) = 𝒕𝒇(𝒘𝒐𝒓𝒅, 𝒄𝒂𝒕𝒆𝒈𝒐𝒓𝒚) ∗ 𝒊𝒅𝒇(𝒘𝒐𝒓𝒅, 𝒄𝒂𝒕𝒆𝒈𝒐𝒓𝒊𝒆𝒔) (3)
Where,
tf - term frequency score of a word in a given category
idf - inverse document frequency score of a given word among the categories
fw
- number of times a given word appears in a category
314
nw - total number of words in the category
cn - total number of categories
cw - number of categories containing the given word
4.2.5 Presence of price in the text
Presence of price in numeric form in the raw text is fed as a feature. This feature is
important to distinguish the price aspect of the respective entities. BUTknot system [13] at
SemEval-2016 has presented similar feature in their approach.
4.2.6 Presence of exclamation mark in the text
Use of exclamation mark to express excitement is used as a feature. UWB system [9] at
SemEval-2016 has used this feature in their approach.
4.2.7 Bag of five words at the end of sentence
The last five words of a sentence excluding stop words are fed as a feature to the SVM.
UWB system [9] at SemEval-2016 has incorporated this feature in their classifier.
4.2.8 Named Entity Recognition (NER)
Indicated the presence of a person, organization, product or location in the text as a feature.
SpaCyvi was used for NER extraction. Saias system [14] has used a similar feature to extract
opinion target expression for SemEval-2015 Task 12. Ahiladas et al. [15] have used NER to
extract food names in their Ruchi system [15]. IIT-TUDA at SemEval-2016 Task 5 by Kumar
et al. [16] has also used NER for opinion target extraction. In contrast, we provided the
extracted NER tags as direct features to the SVM classifier.
4.2.9 Head Nouns
We extract the head noun per sentence phrase, therefore a given sentence with more than
one phrase would contain multiple head nouns. Part of speech (POS) tag is considered to select
315
a noun. Stanford CoreNLPV is used to parse the sentences and obtain POS tags of the words.
Singular noun (NN), plural noun (NNS), proper noun (NNP), plural proper noun (NNPS) POS
tags are considered when extracting nouns. If multiple nouns are present in the same sentence
phrase, rightmost noun is selected as the head noun. Presence of each extracted head noun is
presented to the SVM as a feature. Therefore, a feature is introduced to the SVM for each head
noun identified.
This feature is not observed in past research. Instead, in past research, a single head noun
per sentence has been used. For example, UWB system [9] at SemEval-2016 Task 5 has
incorporated Bag of head words as a feature to their classifier. They have used the head of the
sentence parse tree as the headword. Consider the sentence “The food was well prepared and
the service impeccable”. The word “food” is the head of the sentence parse tree and thus
considered as the head noun of the sentence. However, our approach would pick up both “food”
and “service” words from the separate sentence phrases of the sentence. This helps to capture
multiple features describing multiple aspect categories present in a single sentence. We found
that getting the head of the sentence from the sentence parse tree does not always provide
correct head word as seen in the ablation results by Toh et al. [11].
4.2.10 Mean Embedding using word2vec
Word embedding represents a class of techniques that represent individual words as realvalued vectors in predefined vector space. Word2vec is a group of related models that are used
to produce word embeddings [17]. Mean embedding vector for each sentence was calculated
using word2vec GoogleNews vector pre-trained modelvii and used as a feature for the SVM.
This feature was not used in past research in aspect extraction. Equation (4) can be used to
obtain the mean embedding vector.
𝑴𝑬𝑽 = ∑ 𝒗𝒆𝒄(𝒘𝒐𝒓𝒅𝒊)
𝒏
𝒏𝒊=
𝟏 (4)
316
Where,
MEV - Mean embedding vector
𝑛 - Number of words in the sentence
𝑣𝑒𝑐(𝑤) - Embedding of word 𝑤
4.3 SVM classifier
𝒈(𝒙) = 𝒘𝑻 𝝓(𝒙) + 𝒃 (5)
A Support Vector Machines (SVM) is a discriminative model used in machine learning. It
uses the discriminant function shown in equation (5), where 𝑤 is the weights vector, 𝑏 is the
bias, and 𝜙(𝑥) denotes nonlinear mapping from input space to high-dimensional feature space.
The parameters 𝑤 and 𝑏 are learnt automatically on the training dataset based on the
principle of maximized margin as indicated in (6).
𝐦𝐢𝐧
𝒘,𝒃
𝟏 𝟐
𝑾𝑻𝑾 + 𝑪 ∑𝑵 𝒊=𝟏 𝝃𝒊 (6)
𝑠. 𝑡. {𝜉 𝑦 𝑖𝑖≥ 𝑔(0 𝑥, 𝑖) 𝑖 = ≥ 1 1,− … ,𝜉 𝑁 𝑖
where 𝜉𝑖 denotes the slack variables and 𝐶 is the penalty coefficient. Instead of solving
this problem directly, it is converted to an equivalent quadratic optimization problem using
Lagrange multipliers.
The training sample (𝑥 ̃𝑖, 𝑦𝑖) is called a support vector when satisfying the Lagrange
multiplier 𝛼𝑖 > 0 . By introducing a kernel function, the discriminant function can be
represented as in equation (7).
𝒈(𝒙) = ∑𝑵 𝒊 ̃ =𝟏 𝜶𝒊𝒚𝒊𝑲(𝒙 ̃𝒊, 𝒙) (7)
We used a one-vs-rest multi-label support vector machine classifier to classify the text
into multiple categories. Therefore, in the restaurant domain, 12 classifiers were used and in
the laptop domain, 81 SVM classifiers were used. A sentence may be categorized into multiple
categories. We used cross-validation for selecting the optimal parameters of the classifier.
According to Joachims [18], most text categorization problems are linearly separable. Due
317
to this reason and the higher dimensionality of the feature vectors, it was more suitable to use
a linear kernel. Furthermore, training an SVM with a linear kernel is faster compared to other
kernels and there is only one parameter (regularization parameter) to be optimized in the Linear
SVM.
5. Experimental Results
Table 1 presents results of ablation experiments on the testing data of the two domains
using the SVM classifier. It is evident that the Mean Embeddings feature contributes
significantly to increase the accuracy of the system compared to other features in the two
domains we considered.
Table 1 Experimental Results for SVM
Feature Restaurant F1 Laptop F1
Lemmatization 0.6034 0.3731
+ Exclamation mark 0.6022 0.3712
+ End words 0.6081 0.4146
+ Named Entities 0.6111 0.4282
+ Has price 0.6134 0.4255
+ Term list 0.6692 0.4774
+ Head nouns 0.685 0.4906
+ Mean embeddings 0.7203 0.4991
+ Preprocessing 0.7418 0.5221
Benchmark 0.7303 0.5194
The benchmark system uses a complex hybrid model with Convolutional Neural Network
(CNN) and Feedforward Neural Network(FNN), whereas we achieve better results using a
simple SVM fed with clever features.
318
Highlighting the significance of preprocessing for the features used with SVM, the F1
score drops to 0.7203 and 0.4991 in restaurant and laptop domain respectively without the
preprocessing pipeline.
The inclusion of context sensitive spell correction during data preprocessing was not
observed in past literature and we emphasize that context sensitive spell correction helps to
perform more accurate aspect extraction. This is because customer reviews are written by
laypeople, and reviews are often written using short-hand versions of words typed in a hurry
using a mobile device. The final F1 result increased by 1.78% when isolated spell correction
(0.7288) was replaced with context sensitive spell correction (0.7418) in the restaurant domain.
6. Conclusion
In this paper, we presented an effective SVM classifier that performs better than the stateof-the-art classifiers for aspect extraction. Moreover, we introduced a pre-processing pipeline
to enhance the accuracy of the classifier. All features to the SVM classifier except the custom
compiled lists can be automatically tuned for a new domain. We were able to outperform the
best F1 score reported for the SemEval-2016 Task 5 in both restaurant and laptop domains
using our classifier. We observed the use of deep learning for aspect extraction as an emerging
trend in the field. Therefore, as future work we hope to perform more research on aspect
extraction using deep learning techniques. Moreover, we would like to experiment the benefits
of a hybrid classifier that uses deep learning and supervised machine learning"
8,"INTRODUCTION
Drought models are useful for water resource
management. A drought is a chronic, albeit a natural climatic
feature in most climates, although it may occur with varying
frequency, intensity or duration that affect agriculture,
economy and environment [1, 2]. A drought generally results
from temporary imbalance of water resources due to
persistently lower than average rainfall [3] with an imbalance
in water availability or low annual rainfall and soil moisture
driven by reduced precipitation, high temperatures and high
evapotranspiration [4].
A recent drought index (DI), Standardized Precipitation
Evapotranspiration Index (SPEI), [5-8] has been experimented
for drought studies [9]. Predictive models of SPEI are useful
for assessing drought impacts as well as permanent dry
features (aridity) due to unexpected changes in rainfall,
temperature and evapotranspiration. They assist in risk
management, developing mitigation, forewarning and
response systems [10-12]. Therefore, in this study, predictive
SPEI have been developed using machine learning techniques
for Wilsons Promontory station in Victoria.
In this study we apply machine learning for drought
modelling, that utilises data-driven algorithm not based on
physical interactions as with the case of fully dynamic
(physical) models but instead employ historical data to deduce
relationships between predictors (inputs) and objective
variables (outputs) [13-16]. Advantage of data driven models
are: the explanation of future trends in climate parameters with
less complexities in setting up compared to a physical model,
easy experimentation or evaluation, low computational cost
and less data requirements than the physical model, efficiency
in training and the testing phase (e.g. shorter execution time),
applicability to specific areas and the competitive performance
relative to physical models [17-19]. More importantly, for the
purpose of predicting temperature or precipitation time-series,
significant improvements in performance of data-driven
models have been noted in recent studies (e.g. [18, 19]).
Artificial Neural Network (ANN) is a computational
paradigm that mimics the biological structure of the brain
[20]. It operates like a black box, and does not require detailed
information about inputs as with the case of physical models.
ANN learns from the relationships between input parameters
and controlled or uncontrolled variables by checking previous
trends in data as non-linear regression. ANN also has the
capability for managing very large and complex datasets with
several interrelated parameters [21]. In this paper, ANN is
used for predicting 1, 3, 6 and 12 monthly SPEI. Model
performance is assessed using Mean Absolute Error, RootMean Square Error and Coefficients of Determination based
on observed and simulated SPEI using the Wilsons
Promontory Station in Victoria as a case study.
II. THEORETICAL OVERVIEWS
A. Theory of the Standardized Precipitation and
Evapotranspiration Index (SPEI)
The first step consists of calculating the standardized
precipitation and evapotranspiration index, using the R
software program [1]. The thoery behind the index is well
known and references are available for the interested reader
[9-12]. Therefore, this section presents a brief description of
the computational approach adopted.
The precipitation and temperature data were used to
calculate the potential evapotranspiration (PET). This
estimator shows the potential amount of evaporation and
transpiration considering that sufficient water is available.
Therefore, it represents the contribution of evapotranspiration
in the hydrologic budget, in the context of a drought situation
[13]. In order to calculate the PET, three different methods
were described in the literature: the Thornthwaite, the
2015 IEEE 14th International Conference on Machine Learning and Applications
978-1-5090-0287-0/15 $31.00 © 2015 IEEE
DOI 10.1109/ICMLA.2015.87
318
2015 IEEE 14th International Conference on Machine Learning and Applications
978-1-5090-0287-0/15 $31.00 © 2015 IEEE
DOI 10.1109/ICMLA.2015.87
318
Hargreaves, and the Penman method [14-16]. Each approach
requires a specific type of data, with the Penman method being
the most data-expensive one [1, 2, 9]. Conversly, the
Thornthwaite method can be used whenever monthly data is
available. Since this study was based on monthly data, the
Thornthwaite method was followed.
m
TI
KPET 
 

=
10
16 (1)
where T is the monthly-mean temperature (°C); I is a heat
index calculated as the sum of 12 monthly index values i, the
latter being derived from mean monthly temperature viz
514.1
5
16 
 

=
T
Ki (2)
and m is deduced empirically (m = 6.75 × 10-5 I3 7.75 × 10-7 I2
+ 1.79 × 10-2 I + 0.492), K is a correction coefficient computed
as a function of the latitude and month




=
3012
NDMN
K (3)
NDM is the sum of days of the month and N is the maximum
number of sun hours calculated using
N ωs
π


=
24 (4)
and ωs = hourly angle of sun rising (ωs ( ) −= ϕ tantanarccos δ ,
ϕ = latitude in radians, 
 

= − 405.1
365
2
δ 4093.0 sen πJ is the solar
declination in radians and J is the average Julian day of the
month.
The next step is to determine the surplus/deficit of
water ( ii −= PETPD i ) as the difference between precipitation
(PCN) and PET. The Di values are aggregated at different time
scales, which is similar to the procedure of the SPI. The
difference Dk i, j in a given month j and year I depends on the
chosen time scale k. For example, the accumulated difference
for one month in a particular year i with a 12-month time scale
is
X kjifDD
j
l
li
jkl
li
k
ji = + <
+−= =
−
1
,
12
13
, ,1
(5)
DX kjif
j
kji
li
k
ji =  ≥
+−= 1
, ,
(6)
where Di,l is the PCN– PET difference in the first month of
year i, in millimeters.
Out of common distributions (Pearson III, lognormal
and general extreme value), the log-logistic distribution f(x) is
the most appropriate for standardizing the D series [22]
2
1
f x ( ) x x 1
β β
β γ γ
α α α
−
−
    − −
= +      
      
 
(7)
where α, β, and γ are the scale, shape, and origin parameter
respectively for D values in the range (γ > D < ∞), calculated
using the L-moment approach [23, 24] where
( )
( ) ( )
1 0 0 1
0
1 0 2
2 2 1 1 1 1
, ,
6 6 1 1/ 1 1/
w w w w
w
w w w
β
β α γ α
β β β β
− −     + −
= = = − Γ Γ    
− − Γ + Γ −    
(8)
and T(β) is the gamma function of β and probability-weighted
moments (PWMs) are given by
( )
1
1
1
N
s
s i
i
w Fi D
N
=
= −  (9)
where Fi is a frequency estimator calculated following the
approach of Hosking [25]
0.35
i
i
F
N
−
= (10)
where i is the range of observations arranged in increasing
order and N is the number of data points.
After the parameters of log-logistic distribution are
determined, the probability distribution function of D series is
calculated as
( )
1
F x 1
x
β
α
γ
−
 
= +    
  
    −
(11)
Finally the distribution F(x) is used to calculate the SPEI
following the classical approximation of Abramowitz and
Stegun [26] viz;
2
0 1 2
2 3
1 1 2 3
C C W C W
SPEI W
d W d W d W
+ +
= −
+ + +
(12)
where W P = −2ln( ) for P ≤ 0.5 and P is the probability of
exceeding a determined D value, P = − 1 ( ) F x and C0 =
2.515517, C1 = 0.802853, C2 = 0.010328, d1 = 1.432788, d2 =
0.189269 and d3 = 0.001308. Note that if P > 0.5, then it is
replaced by 1 – P and the sign of the SPEI is reversed.
B. Multiple Linear Regression
Regression analysis explains changes in the dependent
variable (y) as a function of the independent variable (x). For a
multiple linear regression model, the dependent variable y is
assumed to be a function of k independent explanatory
variables x1, x2, x3,…, xk. The regression model has the form
[22]:
i ,110 i ... , ++++= exbxbby iikk (13)
where, b0, b1, . . ., bk are the intercept (b0) and slope
coefficients (b1…bk) for the first and kth explanatory variables,
respectively, ei is the remaining unexplained noise in the
data, i.e., a random error term representing the remaining
effects on y of variables not explicitly included in the model, y
is the response variable, and yi, x1,i, . . . , xk,i represent the ith
observations of each of the variables y, x1, . . . , xk,i
respectively.
A common procedure for estimating the values of b0,
b1, . . ., and bk is to employ the least squares criterion with the
minimum sum of squares of error terms (S); that is to find the
values of b0, b1, . . ., and bk which minimize S.
( )
=
−−−−=
n
i
iobs i xbxbbyS ikk
1
2
,110 ... ,
(14)
( )
=
−=
n
i
i obs yyS cali
1
2 (15)
( )
=
=
n
i
eS i
1
2 (16)
319 319
As a result, b0, b1, . . ., and bk must satisfy:
kj
eb
e
Sb
n
i j
i
i
j
2 ,...,1,0,0
1
==
∂∂
=
∂∂
=
(17)
and since
−= yye caliobsii , the above equation becomes:
kj
b
y
e
Sb
n
i j
cali
i
j
2 ,...,1,0,0
1
==
∂
∂
−=
∂∂
=
(18)
For a more detailed and formal coverage of the topic, the
reader is directed to [23]. The SPEI indices calculated at 1, 3,
6 and 12 months timescales were set as the dependent y
variable and the mean, maximum and minimum temperature,
precipitation and evapotranspiration were set as the
independent x variables.
The fit of the regression model was judged by minimizing
Mallow’s C
p statistic and maximizing the R2 value. Mallow’s
C
p is a statistical measure used to assess the balance between
explaining as much variability as possible and using the fewest
numbers of coefficients [24]. Using a computer for the large
computations associated with multiple regression models,
while retaining scientific judgment to select the regression
model which explains the greatest variability in the SPEI,
using the fewest easily measured explanatory variables, has
been widely recommended [24].
C. Artificial Neural Networks
ANNs are inspired by the design of the mammalian
cerebral cortex. They are used to estimate unknown functions
that can depend on a large number of inputs. They are
generally presented as systems of interconnected “neurons”
and are characterized by their adaptive nature which allows
them to compute values from inputs, and perform machine
learning and pattern recognition. There are two methods to
train an ANN: (i) the self-organizing ANN (also termed a
Kohonen ANN) is usually used to analyze experimental data,
while a back-propagation ANN is mostly used for cognitive
research and for problem-solving applications [25].
A detailed description of ANN can be found in [26] and a
further explanation of ANNs’ general properties is presented
in [27, 28]. These models are robust even in noisy
environments. The modeling studies adopted a multilayer
feed-forward ANN, called a multilayer perceptron (MLP).
MLPs consist of one input layer, one or more hidden layers of
computational nodes, and one output layer [29]. The hidden
layer is where the input and output layers connect, and is
given by [30]:
0 0
1 1
( ) . ( )
m N
k o kj n ji i j k
j i
y t f w f w x t w w
= =
 
= +     +

      (19)
where N is the number of samples, m is the number of hidden
neurons, xi(t) is the ith output variable at the time-step used,
wji is the weight that connects the ith neuron in the input layer
and the jth neuron in the hidden layer, wjo is the bias for the
hidden jth hidden neuron, fn is the activation function of the
hidden neuron, wkj is the weight the connects jth neuron in the
hidden layer and kth neuron in the output layer, wko is the bias
for the kth neuron, fo is the activation function for the output
neuron and yk(t) is the predicted kth output at time-step t.
In this study, the ANN models had a feed-forward multilayer perceptron architecture and were trained using a
Levenberg-Marquardt (LM) back propagation algorithm. The
Levenberg–Marquardt (LM) algorithm is a trust region based
method which employs a hyper-spherical trust region [31]. It
was chosen for this study because of its efficiency and reduced
computational time in training models [32].
D. Performance of Model Assessment
All forecasts were compared using two prediction score
metrics: the coefficient of determination (R2) and the room
mean squared error (RMSE).
2
1
2
_
1
2
_
1
_ _
2




 −
 

−


 −
 

−
=
 

= =
=
n
i
i p
n
i
o
o
n
i
i o i p
PO PP
PPOO
R
(20)
( )
=
= −
n
i
OP ii
n
RMSE
1
1 2 (21)
where:
= mean observed SPEI, Oi = observed SPEI, Pi = predicted
SPEI, and n =number of data points in the testing period.
Fig 1 Map showing the study station (Wilsons Promontory)
III. MATERIALS AND METHODS
A. Study Area and Model Input Data
This study used 6 input variables (precipitation, maximum
temperature, minimum temperature, mean temperature and
evapotranspiration). All the developed models were based on
input combinations of the calculated SPEI from the current
month (m) as well as the SPEI’s individual components, in
order to investigate whether a specific component improves
the accuracy of the SPEI prediction at (m+1). Fig. 1 shows the
study location (red).
B. MLR Model Development
All models were developed using Statistics and Machine
Learning toolbox in MATLAB R2014a. 97 years of available
Wilsons
Promontory
320 320
data were standardized between 0 and 1 and portioned into
two subsets, 80% for training the model and 20% for testing
the model. The model calibration employed data from 1915 to
1992, while testing employed observed water demand from
1993 to 2012. For the design of all the MLR models, a trial
and error method was adopted. The input variables: the
temperature, the precipitation and the evapotranspiration index
were combined in different ways, and each combination was
tried one by one in order to identify the MLR structure which
lead to the lowest generalization error. The developed models
were then compared using the statistical measures of goodness
of fit. Table 1 shows the input combinations used and model
architectures for the MLR and ANN models. This procedure
was repeated for each of the SPEI indices calculated at the 1,
3, 6 and 12 months timeframes, and it was determined that the
best models to forecast the SPEI at month m+1 were a
function of the SPEI, maximum and minimum temperatures,
precipitation and evapotranspiration at month m.
TABLE I. INPUT PARAMETERS AND MODEL STRUCTURES.
C. ANN Model Development
All simulations were conducted using Neural Network
Toolbox in the Matlab2014a environment. The ANN models
had a feed-forward multi-layer perceptron architecture with a
three layer neurons structure as shown in Fig. 2. The models
were trained using a Levenberg-Marquardt (LM) back
propagation algorithm. Regarding the activation function, the
‘logsig’ function was used for the hidden layer and the
‘purelin’ function was used for the output layer.
TABLE II. PERFORMANCE OF MLR MODEL.
TABLE III. PERFORMANCE OF ANN MODEL.
The data were standardized to a value ranging from 0 to 1.
A cross validation technique [33] was used for all the ANN
models, whereby 80% of the data was used to train the
models, while the remaining 20% was equally divided
between validation and testing sets. Proper selection of input
variables is considered to be one of the most important steps in
the ANN model development process. For SPEI forecasting at
1 month lead time, each ANN model used a number of input
neurons varying between 2 and 6. Information on SPEI,
precipitation, maximum, mean and minimum temperatures
and evapotranspiration at the month m was used to predict the
SPEI value at month m+1. The optimum architecture was
determined by running simulations with different
combinations of input, hidden and output neurons (Table 1).
The optimal structure was identified by trial and error as the
architecture resulting in the lowest RMSE value in validation
set. This way, overfitting of the models was avoided.
IV. RESULTS AND DISCUSSIONS
The models accuracy, as quantified by the R2 and RMSE,
is presented in Table II for the best MLR models, and in Table
III for the best ANN models. A scatter diagram of observed
and predicted SPEI was used (not shown here). For MLR, the
best forecasting models at each SPEI had a testing coefficient
Predicted
SPEI
MLR ANN
Inputs Architecture
(InputOutput)
Inputs Architecture (InputHidden-Output)
SPEI1m+1
speim,
tminm,
prcpm, etm
4-1
speim,
tmaxm,
tminm,
prcpm, etm
5-15-1
SPEI3m+1
speim,
tminm,
prcpm, etm
4-1
speim,
tminm,
tmeanm,
prcpm, etm
5-12-1
SPEI6m+1
speim,
tminm,
prcpm, etm
4-1
speim,
tminm,
tmeanm,
prcpm, etm
5-14-1
SPEI12m+1
speim,
tmaxm,
prcpm, etm
4-1
speim,
tmaxm,
tminm,
prcpm, etm
5-9-1
Predicted SPEI Inputs
Training Testing
R2 RMSE R2 RMSE
SPEI1m+1
speim,
tminm,
prcpm, etm
0.804 0.437 0.838 0.401
SPEI3m+1
speim,
tminm,
prcpm, etm
0.843 0.389 0.884 0.344
SPEI6m+1
speim,
tminm,
prcpm, etm
0.879 0.336 0.918 0.309
SPEI12m+1
speim,
tmaxm,
prcpm, etm
0.995 0.068 0.993 0.095
Predi
cted
SPEI
Input
Combinatio
n
Training Validation Testing
R2 RMS
E R2 RMS E R2 RMS E
SPEI1
m+1
speim, tmaxm,
tminm, prcpm,
etm 0.961 0.196 0.971 0.168 0.934 0.257
SPEI3
m+1
speim, tminm,
tmeanm,
prcpm, etm 0.981 0.136 0.988 0.110 0.980 0.139
SPEI6
m+1
speim, tminm,
tmeanm,
prcpm, etm 0.988 0.106 0.989 0.102 0.987 0.125
SPEI1
2m+1
speim, tmaxm,
tminm, prcpm,
etm 0.999 0.033 0.999 0.032 0.999 0.043
321 321
of determination ranging between 0.8381 and 0.9932 and the
root mean square error of the forecasted SPEI from the
observed was between 0.0952 and 0.4012. When it comes to
ANN, the best forecasting models at each SPEI had a testing
coefficient of determination between 0.9337 and 0.9986 and
the RMSE was between 0.0425 and 0.2574. These values
show a good visual agreement between the observed SPEI
values and the predicted ones.
In Table IV, an assessment for the MLR and ANN models
in forecasting 1, 3, 6 and 12 months SPEI is presented. The
performance consists of the mean absolute error values for the
testing set, and the standard deviation for the observed and
predicted SPEI values. By a close observation, it was found
that the best model predictions were obtained for the 12 month
SPEI using the ANN learning method. For this model, the
smallest value of the RMSE was equal to 0.0425, which also
corresponded to the highest value of R2 (0.9986).
TABLE IV. MEAN ABSOLUTE ERROR (MAE) AND STANDARD DEVIATION
(σ) OF PREDICTED (σ SPEI P) AND OBSERVED (σ SPEI O) VALUES.
The results show that for all the four SPEI timescales,
ANN consistently outperforms MLR. It is interesting to note
that, generally, the performance of the MLR models is only
slightly inferior than that of the ANN models, which indicates
that a multi-linear relationship is a relatively good model for
SPEI estimation. However, ANN helps improving the multilinear model as it is able to extract more information from the
input variables.
Overall, ANN performed better in predicting SPEI
compared to the MLR models as reflected by relatively higher
R2 values and lower RMSE values for all of the 1, 3, 6 and 12
months SPEI. In fact, in the case of the 6 month SPEI, the
RMSE was dramatically lower by 85%. Consequently, the
MAE of the ANN models was lower compared to the MLR
models as shown in Table IV.
Fig. 4 Boxplot of SPEI from observed and predicted data
using MLR and ANN models.
The spread of the predicted and observed SPEI is
illustrated in Fig. 4 using a boxplot. The whiskers on the
boxplot are represented by the horizontal lines extending from
the top and bottom of the box. The upper whisker represents
the largest non-outlier in the dataset, while the lower whisker
represents the smallest non-outlier in the data set. In Fig. 4, it
is shown that the difference between the quartiles distribution
for the predicted and observed data is smaller for ANN
compared to MLR. This smaller shift confirms a better
performance for the ANN models over the MLR models. As
seen in Fig. 4, it is important to note that the median of the
predictions and observations for the ANN models is relatively
similar for all four SPEI, but the top and bottom whiskers are
slightly different for each predicted SPEI.
TABLE V. PERFORMANCE METRICS OF MLR AND ANN MODELS IN
TERMS OF THE MAXIMUM, MINIMUM AND STANDARD DEVIATION OF
PREDICTION ERROR (PE) CALCULATED FOR THE TESTING SET.
Method Predicted
SPEI MAE σ SPEI o σ SPEI p % diff (σ)
MLR
SPEI1m+1 0.3168 0.9903 1.1761 -0.1715
SPEI3m+1 0.2863 0.9912 1.1454 -0.1443
SPEI6m+1 0.2575 0.9941 1.1106 -0.1107
SPEI12m+1 0.0473 0.9938 1.0040 -0.0102
ANN
SPEI1m+1 0.1237 0.9903 0.9637 0.0311
SPEI3m+1 0.0865 0.9912 0.9802 0.0111
SPEI6m+1 0.0728 0.9941 0.9873 0.0069
SPEI12m+1 0.0264 0.9938 0.9910 0.0028
Method Predicted
SPEI
Prediction error (PE)
Maximum Minimum Standard
deviation
MLR
SPEI1m+1 4.624 -1.747 0.430
SPEI3m+1 1.787 -1.289 0.380
SPEI6m+1 1.195 -1.193 0.329
SPEI12m+1 0.325 -0.457 0.067
ANN
SPEI1m+1 1.281 -1.215 0.200
SPEI3m+1 1.073 -0.605 0.134
SPEI6m+1 0.579 -0.949 0.108
SPEI12m+1 0.120 -0.159 0.034
322 322
Fig. 5 Histogram of frequency distribution of the prediction
error (PE) calculated for the test period (2006-2012). The
cumulative frequency representing under-predictions (PE < 0)
and over-predictions (PE > 0) by MLR and ANN models.
Table V presents standard deviations () and maximum
and minimum values of PE, while Fig. 5 illustrates the
frequency distribution of the PE. It was found that, for ANN
models, close to 52% of all the predictions resulted in
prediction errors between -1.2 and 0 and the remaining 48% of
predictions yielded prediction errors relatively larger
prediction errors, ranging between 0 and 1.2.
The prediction metrics obtained in the study for ANN
showed better performance compared to previous studies on
precipitation forecasts conducted in Queensland [34, 35].
They were also better than comparative studies of MLR and
ANN for rainfall forecasts in Central and West Victoria. This
better performance is partly due to the standardized nature of
the SPEI as opposed to direct modeling of precipitations in
previous studies [21]. Overall, the ANN models exhibited
better skill in predicting the SPEI than the MLR models and
therefore can be considered a more useful method for
modeling the SPEI and assessing the risk of droughts.
V. SUMMARY AND CONCLUSIONS
In many fields such as meteorology, hydrology and water
resource management, the forecasting of drought is of crucial
importance. This study investigated the ability of two learning
methods, namely MLR and ANN, to forecast the monthly
standardized Precipitation and Evapotranspiration Index. Six
climate variables over the period of 1915-2012 were used to
describe the Wilsons Promontory station in eastern Australia.
The models were trained using different combinations of the
meteorological variables (mean rainfall, minimum, maximum
and mean temperatures and the evapotranspiration) and the
output variable was the SPEI index.
For the MLR models development, 80% of the dataset was
used for training the models and 20% for testing the developed
models. In developing the ANN models, 80% of the data was
used for training the models, 10% was used for validation, and
the remaining 10% was used for testing. The performance
assessment of the models was based on prediction metrics like
the coefficient of determination, the root mean square error,
and the spread of the predicted SPEI.
The paper presents a comparative study of the performance
of the most optimum MLR and ANN models and shows that
ANN outperforms MLR for the prediction of the 1, 3, 6 and 12
month SPEI. The best ANN models for the 1, 3, 6 and 12
months SPEI had the following architectures: 5-15-1, 5-12-1,
5-14-1 and 5-9-1 as the input-hidden-output neurons. These
models used the SPEI, mean precipitation, mean, minimum
and maximum temperatures as well as the evapotranspiration
at month m, in order to predict for the SPEI at month m+1.
This implies that all the explanatory variables used in this
study are necessary for improved SPEI modeling.
ANN models are practical tools for scientists and
engineers for water stress or water resource assessments. As a
consequence, stakeholders interested in the well-being of the
natural ecosystems will find these predictive models
particularly helpful as they involve less complexity in the
design and yield to high performance in the forecasts. Possible
future studies could investigate the performance of bootstrapbased and wavelet-based ANN and MLR models in
forecasting the monthly SPEI, and compare it with that of
standard ANN and MLR models.
ACKNOWLEDGMENT
The data were obtained from Bureau of Meteorology. R C
Deo thanks University of Southern Queensland Academic
Division “Research Activation Incentive Scheme (RAIS)”
grant to collaborate with S Mouatadid and J F Adamowski.."
9,"Introduction
Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation
for a modest number of steps. A surprising example of the power of DNNs is their ability to sort
N N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are
related to conventional statistical models, they learn an intricate computation. Furthermore, large
DNNs can be trained with supervised backpropagation whenever the labeled training set has enough
information to specify the network’s parameters. Thus, if there exists a parameter setting of a large
DNN that achieves good results (for example, because humans can solve the task very rapidly),
supervised backpropagation will find these parameters and solve the problem.
Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets
can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since
many important problems are best expressed with sequences whose lengths are not known a-priori.
For example, speech recognition and machine translation are sequential problems. Likewise, question answering can also be seen as mapping a sequence of words representing the question to a
1
sequence of words representing the answer. It is therefore clear that a domain-independent method
that learns to map sequences to sequences would be useful.
Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and
outputs is known and fixed. In this paper, we show that a straightforward application of the Long
Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.
The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixeddimensional vector representation, and then to use another LSTM to extract the output sequence
from that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model
[28, 23, 30] except that it is conditioned on the input sequence. The LSTM’s ability to successfully
learn on data with long range temporal dependencies makes it a natural choice for this application
due to the considerable time lag between the inputs and their corresponding outputs (fig. 1).
There have been a number of related attempts to address the general sequence to sequence learning
problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]
who were the first to map the entire input sentence to vector, and is very similar to Cho et al. [5].
Graves [10] introduced a novel differentiable attention mechanism that allows neural networks to
focus on different parts of their input, and an elegant variant of this idea was successfully applied
to machine translation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another
popular technique for mapping sequences to sequences with neural networks, although it assumes a
monotonic alignment between the inputs and the outputs [11].
Figure 1: Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence. The
model stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the
input sentence in reverse, because doing so introduces many short term dependencies in the data that make the
optimization problem much easier.
The main result of this work is the following. On the WMT’14 English to French translation task,
we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep
LSTMs (with 380M parameters each) using a simple left-to-right beam-search decoder. This is
by far the best result achieved by direct translation with large neural networks. For comparison,
the BLEU score of a SMT baseline on this dataset is 33.30 [29]. The 34.81 BLEU score was
achieved by an LSTM with a vocabulary of 80k words, so the score was penalized whenever the
reference translation contained a word not covered by these 80k. This result shows that a relatively
unoptimized neural network architecture which has much room for improvement outperforms a
mature phrase-based SMT system.
Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on
the same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline
by 3.2 BLEU points and is close to the previous state-of-the-art (which is 37.0 [9]).
Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other
researchers with related architectures [26]. We were able to do well on long sentences because we
reversed the order of words in the source sentence but not the target sentences in the training and test
set. By doing so, we introduced many short term dependencies that made the optimization problem
much simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with
long sentences. The simple trick of reversing the words in the source sentence is one of the key
technical contributions of this work.
A useful property of the LSTM is that it learns to map an input sentence of variable length into
a fixed-dimensional vector representation. Given that translations tend to be paraphrases of the
source sentences, the translation objective encourages the LSTM to find sentence representations
that capture their meaning, as sentences with similar meanings are close to each other while different
2
sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model
is aware of word order and is fairly invariant to the active and passive voice.
2 The model
The Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural
networks to sequences. Given a sequence of inputs (x1, . . . , xT ), a standard RNN computes a
sequence of outputs (y1, . . . , yT ) by iterating the following equation:
ht = sigm W hxxt + W hhht−1
yt = W yhht
The RNN can easily map sequences to sequences whenever the alignment between the inputs the
outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose
input and the output sequences have different lengths with complicated and non-monotonic relationships.
A simple strategy for general sequence learning is to map the input sequence to a fixed-sized vector
using one RNN, and then to map the vector to the target sequence with another RNN (this approach
has also been taken by Cho et al. [5]). While it could work in principle since the RNN is provided
with all the relevant information, it would be difficult to train the RNNs due to the resulting long
term dependencies [14, 4] (figure 1) [16, 15]. However, the Long Short-Term Memory (LSTM) [16]
is known to learn problems with long range temporal dependencies, so an LSTM may succeed in
this setting.
The goal of the LSTM is to estimate the conditional probability p(y1, . . . , yT ′|x1, . . . , xT ) where
(x1, . . . , xT ) is an input sequence and y1, . . . , yT ′ is its corresponding output sequence whose length
T ′ may differ from T. The LSTM computes this conditional probability by first obtaining the fixeddimensional representation v of the input sequence (x1, . . . , xT ) given by the last hidden state of the
LSTM, and then computing the probability of y1, . . . , yT ′ with a standard LSTM-LM formulation
whose initial hidden state is set to the representation v of x1, . . . , xT:
p(y1, . . . , yT ′|x1, . . . , xT ) =
T ′
Y t
=1
p(yt|v, y1, . . . , yt−1) (1)
In this equation, each p(yt|v, y1, . . . , yt−1) distribution is represented with a softmax over all the
words in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that
each sentence ends with a special end-of-sentence symbol “<EOS>”, which enables the model to
define a distribution over sequences of all possible lengths. The overall scheme is outlined in figure
1, where the shown LSTM computes the representation of “A”, “B”, “C”, “<EOS>” and then uses
this representation to compute the probability of “W”, “X”, “Y”, “Z”, “<EOS>”.
Our actual models differ from the above description in three important ways. First, we used two
different LSTMs: one for the input sequence and another for the output sequence, because doing
so increases the number model parameters at negligible computational cost and makes it natural to
train the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs
significantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found
it extremely valuable to reverse the order of the words of the input sentence. So for example, instead
of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ,
where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to
β, and so on, a fact that makes it easy for SGD to “establish communication” between the input and
the output. We found this simple data transformation to greatly boost the performance of the LSTM.
3 Experiments
We applied our method to the WMT’14 English to French MT task in two ways. We used it to
directly translate the input sentence without using a reference SMT system and we it to rescore the
n-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample
translations, and visualize the resulting sentence representation.
3
3.1 Dataset details
We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean “selected”
subset from [29]. We chose this translation task and this specific training set subset because of the
public availability of a tokenized training and test set together with 1000-best lists from the baseline
SMT [29].
As typical neural language models rely on a vector representation for each word, we used a fixed
vocabulary for both languages. We used 160,000 of the most frequent words for the source language
and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was
replaced with a special “UNK” token.
3.2 Decoding and Rescoring
The core of our experiments involved training a large deep LSTM on many sentence pairs. We
trained it by maximizing the log probability of a correct translation T given the source sentence S,
so the training objective is
1/|S| X
(T,S)∈S
log p(T |S)
where S is the training set. Once training is complete, we produce translations by finding the most
likely translation according to the LSTM:
T ˆ = arg max
T
p(T |S) (2)
We search for the most likely translation using a simple left-to-right beam search decoder which
maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some
translation. At each timestep we extend each partial hypothesis in the beam with every possible
word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but
the B most likely hypotheses according to the model’s log probability. As soon as the “<EOS>”
symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete
hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system
performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam
search (Table 1).
We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To
rescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took
an even average with their score and the LSTM’s score.
3.3 Reversing the Source Sentences
While the LSTM is capable of solving problems with long term dependencies, we discovered that
the LSTM learns much better when the source sentences are reversed (the target sentences are not
reversed). By doing so, the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU
scores of its decoded translations increased from 25.9 to 30.6.
While we do not have a complete explanation to this phenomenon, we believe that it is caused by
the introduction of many short term dependencies to the dataset. Normally, when we concatenate a
source sentence with a target sentence, each word in the source sentence is far from its corresponding
word in the target sentence. As a result, the problem has a large “minimal time lag” [17]. By
reversing the words in the source sentence, the average distance between corresponding words in
the source and target language is unchanged. However, the first few words in the source language
are now very close to the first few words in the target language, so the problem’s minimal time lag is
greatly reduced. Thus, backpropagation has an easier time “establishing communication” between
the source sentence and the target sentence, which in turn results in substantially improved overall
performance.
Initially, we believed that reversing the input sentences would only lead to more confident predictions in the early parts of the target sentence and to less confident predictions in the later parts. However, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs
4
trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences
results in LSTMs with better memory utilization.
3.4 Training details
We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,
with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary
of 160,000 and an output vocabulary of 80,000. We found deep LSTMs to significantly outperform
shallow LSTMs, where each additional layer reduced perplexity by nearly 10%, possibly due to their
much larger hidden state. We used a naive softmax over 80,000 words at each output. The resulting
LSTM has 380M parameters of which 64M are pure recurrent connections (32M for the “encoder”
LSTM and 32M for the “decoder” LSTM). The complete training details are given below:
• We initialized all of the LSTM’s parameters with the uniform distribution between -0.08
and 0.08
• We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.
After 5 epochs, we begun halving the learning rate every half epoch. We trained our models
for a total of 7.5 epochs.
• We used batches of 128 sequences for the gradient and divided it the size of the batch
(namely, 128).
• Although LSTMs tend to not suffer from the vanishing gradient problem, they can have
exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,
25] by scaling it when its norm exceeded a threshold. For each training batch, we compute
s = kgk2, where g is the gradient divided by 128. If s > 5, we set g = 5 s g .
• Different sentences have different lengths. Most sentences are short (e.g., length 20-30)
but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen
training sentences will have many short sentences and few long sentences, and as a result,
much of the computation in the minibatch is wasted. To address this problem, we made
sure that all sentences within a minibatch were roughly of the same length, which a 2x
speedup.
3.5 Parallelization
A C++ implementation of deep LSTM with the configuration from the previous section on a single GPU processes a speed of approximately 1,700 words per second. This was too slow for our
purposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was
executed on a different GPU and communicated its activations to the next GPU (or layer) as soon
as they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate
GPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible
for multiplying by a 1000 × 20000 matrix. The resulting implementation achieved a speed of 6,300
(both English and French) words per second with a minibatch size of 128. Training took about a ten
days with this implementation.
3.6 Experimental Results
We used the cased BLEU score [24] to evaluate the quality of our translations. We computed our
BLEU scores using multi-bleu.pl1 on the tokenized predictions and ground truth. This way
of evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].
However, if we evaluate the state of the art system of [9] (whose predictions can be downloaded
from statmt.org\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported
by statmt.org\matrix.
The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of
LSTMs that differ in their random initializations and in the random order of minibatches. While the
decoded translations of the LSTM ensemble do not beat the state of the art, it is the first time that
a pure neural translation system outperforms a phrase-based SMT baseline on a large MT task by
1There several variants of the BLEU score, and each variant is defined with a perl script.
5
Method test BLEU score (ntst14)
Bahdanau et al. [2] 28.45
Baseline System [29] 33.30
Single forward LSTM, beam size 12 26.17
Single reversed LSTM, beam size 12 30.59
Ensemble of 5 reversed LSTMs, beam size 1 33.00
Ensemble of 2 reversed LSTMs, beam size 12 33.27
Ensemble of 5 reversed LSTMs, beam size 2 34.50
Ensemble of 5 reversed LSTMs, beam size 12 34.81
Table 1: The performance of the LSTM on WMT’14 English to French test set (ntst14). Note that
an ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of
size 12.
Method test BLEU score (ntst14)
Baseline System [29] 33.30
Cho et al. [5] 34.54
State of the art [9] 37.0
Rescoring the baseline 1000-best with a single forward LSTM 35.61
Rescoring the baseline 1000-best with a single reversed LSTM 35.85
Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs 36.5
Oracle Rescoring of the Baseline 1000-best lists ∼45
Table 2: Methods that use neural networks together with an SMT system on the WMT’14 English
to French test set (ntst14).
a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is within 0.5
BLEU points of the previous state of the art by rescoring the 1000-best list of the baseline system.
3.7 Performance on long sentences
We were surprised to discover that the LSTM did well on long sentences, which is shown quantitatively in figure 3. Table 3 presents several examples of long sentences and their translations.
3.8 Model Analysis
−8 −6 −4 −2 0 2 4 6 8 10
−6
−5
−4
−3
−2
−1
4 3 2 1 0
John respects Mary
Mary respects John
John admires Mary
Mary admires John
Mary is in love with John
John is in love with Mary
−15 −10 −5 0 5 10 15 20
−20
−15
−10
−5
5 0
10
15
I gave her a card in the garden
In the garden , I gave her a card
She was given a card by me in the garden
She gave me a card in the garden
In the garden , she gave me a card
I was given a card by her in the garden
Figure 2: The figure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained
after processing the phrases in the figures. The phrases are clustered by meaning, which in these examples is
primarily a function of word order, which would be difficult to capture with a bag-of-words model. Notice that
both clusters have similar internal structure.
One of the attractive features of our model is its ability to turn a sequence of words into a vector
of fixed dimensionality. Figure 2 visualizes some of the learned representations. The figure clearly
shows that the representations are sensitive to the order of words, while being fairly insensitive to the
6
Type Sentence
Our model Ulrich UNK , membre du conseil d’ administration du constructeur automobile Audi ,
affirme qu’ il s’ agit d’ une pratique courante depuis des annees pour que les t ´ el ´ ephones ´
portables puissent etre collect ˆ es avant les r ´ eunions du conseil d’ administration afin qu’ ils ´
ne soient pas utilises comme appareils d’ ´ ecoute ´ a distance . `
Truth Ulrich Hackenberg , membre du conseil d’ administration du constructeur automobile Audi ,
declare que la collecte des t ´ el ´ ephones portables avant les r ´ eunions du conseil , afin qu’ ils ´
ne puissent pas etre utilis ˆ es comme appareils d’ ´ ecoute ´ a distance , est une pratique courante `
depuis des annees . ´
Our model “ Les tel ´ ephones cellulaires , qui sont vraiment une question , non seulement p ´ arce qu’ ils
pourraient potentiellement causer des interferences avec les appareils de navigation , mais ´
nous savons , selon la FCC , qu’ ils pourraient interferer avec les tours de t ´ el ´ ephone cellulaire ´
lorsqu’ ils sont dans l’ air ” , dit UNK .
Truth “ Les tel ´ ephones portables sont v ´ eritablement un probl ´ eme , non seulement parce qu’ ils `
pourraient eventuellement cr ´ eer des interf ´ erences avec les instruments de navigation , mais ´
parce que nous savons , d’ apres la FCC , qu’ ils pourraient perturber les antennes-relais de `
tel ´ ephonie mobile s’ ils sont utilis ´ es ´ a bord ” , a d ` eclar ´ e Rosenker . ´
Our model Avec la cremation , il y a un “ sentiment de violence contre le corps d’ un ´ etre cher ” , ˆ
qui sera “ reduit ´ a une pile de cendres ” en tr ` es peu de temps au lieu d’ un processus de `
decomposition “ qui accompagnera les ´ etapes du deuil ” . ´
Truth Il y a , avec la cremation , “ une violence faite au corps aim ´ e ” , ´
qui va etre “ r ˆ eduit ´ a un tas de cendres ” en tr ` es peu de temps , et non apr ` es un processus de `
decomposition , qui “ accompagnerait les phases du deuil ” . ´
Table 3: A few examples of long translations produced by the LSTM alongside the ground truth
translations. The reader can verify that the translations are sensible using Google translate.
4 7 8 12 17 22 28 35 79
test sentences sorted by their length
20
25
30
35
40
BLEU score
LSTM (34.8)
baseline (33.3)
0 500 1000 1500 2000 2500 3000 3500
test sentences sorted by average word frequency rank
20
25
30
35
40
BLEU score
LSTM (34.8)
baseline (33.3)
Figure 3: The left plot shows the performance of our system as a function of sentence length, where the
x-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths.
There is no degradation on sentences with less than 35 words, there is only a minor degradation on the longest
sentences. The right plot shows the LSTM’s performance on sentences with progressively more rare words,
where the x-axis corresponds to the test sentences sorted by their “average word frequency rank”.
replacement of an active voice with a passive voice. The two-dimensional projections are obtained
using PCA.
4 Related work
There is a large body of work on applications of neural networks to machine translation. So far,
the simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a
7
Feedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the nbest lists of a strong MT baseline [22], which reliably improves translation quality.
More recently, researchers have begun to look into ways of including information about the source
language into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLM
with a topic model of the input sentence, which improves rescoring performance. Devlin et al. [8]
followed a similar approach, but they incorporated their NNLM into the decoder of an MT system
and used the decoder’s alignment information to provide the NNLM with the most useful words in
the input sentence. Their approach was highly successful and it achieved large improvements over
their baseline.
Our work is closely related to Kalchbrenner and Blunsom [18], who were the first to map the input
sentence into a vector and then back to a sentence, although they map sentences to vectors using
convolutional neural networks, which lose the ordering of the words. Similarly to this work, Cho et
al. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their
primary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also
attempted direct translations with a neural network that used an attention mechanism to overcome
the poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging
results. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et
al. [5] by translating pieces of the source sentence in way that produces smooth translations, which
is similar to a phrase-based approach. We suspect that they could achieve similar improvements by
simply training their networks on reversed source sentences.
End-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs and
outputs by feedforward networks, and map them to similar points in space. However, their approach
cannot generate translations directly: to get a translation, they need to do a look up for closest vector
in the pre-computed database of sentences, or to rescore a sentence.
5 Conclusion
In this work, we showed that a large deep LSTM with a limited vocabulary can outperform a standard SMT-based system whose vocabulary is unlimited on a large-scale MT task. The success of
our simple LSTM-based approach on MT suggests that it should do well on many other sequence
learning problems, provided they have enough training data.
We were surprised by the extent of the improvement obtained by reversing the words in the source
sentences. We conclude that it is important to find a problem encoding that has the greatest number
of short term dependencies, as they make the learning problem much simpler. In particular, while
we were unable to train a standard RNN on the non-reversed translation problem (shown in fig. 1),
we believe that a standard RNN should be easily trainable when the source sentences are reversed
(although we did not verify it experimentally).
We were also surprised by the ability of the LSTM to correctly translate very long sentences. We
were initially convinced that the LSTM would fail on long sentences due to its limited memory,
and other researchers reported poor performance on long sentences with a model similar to ours
[5, 2, 26]. And yet, LSTMs trained on the reversed dataset had little difficulty translating long
sentences.
Most importantly, we demonstrated that a simple, straightforward and a relatively unoptimized approach can outperform a mature SMT system, so further work will likely lead to even greater translation accuracies. These results suggest that our approach will likely do well on other challenging
sequence to sequence problems."
10,"Introduction
Human parsing, which refers to decomposing a human
image into semantic clothes/body regions, is an important
component for general human-centric analysis. It enables
∗Corresponding author is Liang Lin (E-mail: linliang@ieee.org). This
work was done when the first author worked as an intern in National University of Singapore.
many higher level applications, e.g., clothing style recognition and retrieval [5], clothes recognition and retrieval [30],
people re-identification [33], human behavior analysis [29]
and automatic product recommendation [14].
While there has been previous work devoted
to human parsing based on human pose estimation [31] [6] [32] [20] [19], non-parametric label transferring [30][21] and active template regression [15], none of
previous methods has achieved excellent dense prediction
over raw image pixels in a fully end-to-end way. These
previous methods often take complicated preprocessing as
the requisite, such as reliable human pose estimation [4],
bottom-up hypothesis generation [1] and template dictionary learning [23], which makes the system vulnerable to
potential errors of the front-end preprocessing steps.
Convolutional neural network (CNN) facilitates great advances not only in whole-image classification [26], but also
in structure prediction such as object detection [10] [16],
part prediction [27] and general object/scene semantic segmentation [7][8]. However, they usually need supervised
pre-training with a large classification dataset, e.g., ImageNet, and other post-processing steps such as Conditional
Random Field (CRF) [8] and extra discriminative classifiers [24][11]. Besides the above mentioned limitations,
there are still two technical hurdles in the application of
existing CNN architectures to pixel-wise prediction for the
human parsing task. First, diverse contextual information
and mutual relationships among the key components of human parsing (i.e. semantic labels, spatial layouts and shape
priors) should be well addressed during predicting the pixelwise labels. For example, the presence of a skirt will hinder
the probability of labeling any pixel as the dress/pants, and
meanwhile facilitate the pixel prediction of left/right legs.
Second, the predicted label maps are desired to be detailpreserved and of high-resolution, in order to recognize or
highlight very small labels (e.g. sunglass or belt). However,
most of the previous works on semantic segmentation with
CNN can only predict the very low-resolution labeling, such
1 1386
as eight times down-sampled prediction in the fully convolutional network (FCN) [22]. Their prediction is very coarse
and not optimal for the required fine-grained segmentation.
In this paper, we present a novel Contextualized Convolutional Neural Network (Co-CNN) that successfully addresses the above mentioned issues. Given an input human
image, our architecture produces the correspondingly-sized
pixel-wise labeling maps in a fully end-to-end way, as illustrated in Figure 1. Our Co-CNN aims to simultaneously
capture cross-layer context, global image-level context and
local super-pixel contexts by using the local-to-global-tolocal hierarchical structure, global image-level label prediction, within-super-pixel smoothing and cross-super-pixel
neighborhood voting, respectively.
First, our basic local-to-global-to-local structure hierarchically encodes the local details from the early, fine layers
and the global semantic information from the deep, coarse
layers. Four different spatial resolutions are used for capturing different levels of semantic information. The feature
maps from deep layers often focus on the global structure
and are insensitive to local boundaries and spatial displacements. We up-sample the feature maps from deep layers
and then combine them with the feature maps from former
layers under the same resolution.
Second, to utilize the global image-level context and
guarantee the coherence between pixel-wise labeling and
image label prediction, we incorporate global image label
prediction into our pixel-wise categorization network, illustrated as the global image-level context part of Figure 1.
An auxiliary objective defined for the global image label
prediction (i.e. Squared Loss) is used, which focuses on
global semantic information and has no relation with local variants such as pose, illumination or precise location.
We then use the predicted image-level label probabilities to
guide the feature learning from two aspects. First, the predicted image-level label probabilities are utilized to facilitate the feature maps of each intermediate layer to generate
the semantics-aware feature responses, and then the combined feature maps are further convolved by the filters in the
subsequent layers, shown as the image label concatenation
part of Figure 1. Second, the predicted image-level label
probabilities are also used in the prediction layer to explicitly re-weight the pixel-wise label confidences, shown as the
element-wise summation part of Figure 1.
Finally, the within-super-pixel smoothing and crosssuper-pixel neighborhood voting are leveraged to retain the
local boundaries and label consistencies within the superpixels. They are formulated as natural sub-components of
the Co-CNN in both the training and the testing process.
Comprehensive evaluations and comparisons on the ATR
dataset [15] and the Fashionista dataset [30] well demonstrate that our Co-CNN yields results that significantly surpass all previously published methods, boosting the current state-of-the-arts from 64.38% [15] to 76.95%. We also
build a much larger dataset “Chictopia10k”, which contains
10,000 annotated images. By adding the images of “Chictopia10k” into the training, the F-1 score can be further
improved to 80.14%, 15.76% higher than the state-of-thearts [15] [30].
2. Related Work
Human Parsing: Much research has been devoted to
human parsing [31][30][6][32][28][18][25][21]. Most previous works used the low-level over-segmentation, pose estimation and bottom-up hypothesis generation as the building blocks of human parsing. For example, Yamaguchi et
al. [31] performed human pose estimation and attribute labeling sequentially and then improved clothes parsing with
a retrieval-based approach [30]. Dong et al. [6] proposed to
use a group of parselets under the structure learning framework. These traditional hand-crafted pipelines often require many hand-designed processing steps, each of which
needs to be carefully designed and tuned. Recently, Liang
et al. [15] proposed to use two separate convolutional networks to predict the template coefficients for each label
mask and their corresponding locations, respectively. However, their design may lead to sub-optimal results.
Semantic Segmentation with CNN: Our method
works directly on the pixel-level representation, similar
to some recent research on semantic segmentation with
CNN [22] [11] [8]. These pixel-level representations are in
contrast to the common two-stage approaches[24] [10] [12]
which consist of complex bottom-up hypothesis generation
(e.g. bounding box proposals) and CNN-based region classification. For the pixel-wise representation, by directly using CNN, Farabet et al. [7] trained a multi-scale convolutional network from raw pixels and employed the superpixel tree for smoothing. The dense pixel-level CRF was
used as the post-processing step after CNN-based pixelwise prediction [2] [3] [9]. More recently, Long et al. [22]
proposed the fully convolutional network for predicting
pixel-wise labeling.
The main difference between our Co-CNN and these
previous methods is the integration of cross-layer context,
global image-level context, local super-pixel contexts into
a unified network. It should be noted that while the fully
convolutional network [22] also tries to combine coarse and
fine layers, they only aggregate the predictions from different scales in the final output. In contrast, in our localto-global-to-local hierarchical structure , we hierarchically
combine feature maps from cross-layers and further feed
them into several subsequent layers for better feature learning, which is very important in boosting the performance as
demonstrated in the experiments.
1387
FC
FC
Squared Loss
on image-level labels
+ + + +
5*5 convoluions
+ 0
Segmentaion
Goundtruth
Global Image-level Context
Image
150*100
75*50
37*25
18*12
37*25
37*25
75*50
75*50
150*100 150*100 150*100
Local Super-pixel Contexts
150*100
image label
concatenaion
image label
concatenaion
image label
concatenaion
element-wise
summaion
Cross-layer Context
18 18 18 18
210
210 210
192
192
192
192
192
192
192
Figure 1. Our Co-CNN integrates the cross-layer context, global image-level context and local super-pixel contexts into a unified network. It
consists of cross-layer combination, global image-level label prediction, within-super-pixel smoothing and cross-super-pixel neighborhood
voting. First, given an input 150 × 100 image, we extract the feature maps for four resolutions (i.e., 150 × 100, 75 × 50, 37 × 25 and
18 × 12). Then we gradually up-sample the feature maps and combine the corresponding early, fine layers (blue dash line) and deep,
coarse layers (blue circle with plus) under the same resolutions to capture the cross-layer context. Second, an auxiliary objective (shown
as “Squared loss on image-level labels”) is appended after the down-sampling stream to predict global image-level labels. These predicted
probabilities are then aggregated into the subsequent layers after the up-sampling (green line) and used to re-weight pixel-wise prediction
(green circle with plus). Finally, the within-super-pixel smoothing and cross-super-pixel neighborhood voting are performed based on
the predicted confidence maps (orange planes) and the generated super-pixel over-segmentation map to produce the final parsing result.
Only down-sampling, up-sampling, and prediction layers are shown; intermediate convolution layers are omitted. For better viewing of all
figures in this paper, please see original zoomed-in color pdf file.
3. The Proposed Co-CNN Architecture
Our Co-CNN exploits the cross-layer context, global
image context and local super-pixel contexts in a unified
network, consisting of four components, i.e., the localto-global-to-local hierarchy, global image label prediction,
within-super-pixel smoothing and cross-super-pixel neighborhood voting, respectively.
3.1. Local-to-global-to-local Hierarchy
Our basic local-to-global-to-local structure captures the
cross-layer context. It simultaneously considers the local
fine details and global structure information. The input to
our Co-CNN is a 150 × 100 color image and then passed
through a stack of convolutional layers. The feature maps
are down-sampled three times by the max pooling with a
stride of 2 pixels to get three extra spatial resolutions (75 ×
50, 37× 25, 18× 12), shown as the four early convolutional
layers in Figure 1. Except for the stride of 2 pixels for downsampling, the convolution strides are all fixed as 1 pixel.
The spatial padding of convolutional layers is set so that the
spatial resolution is preserved after convolution, e.g., the
padding of 2 pixels for 5 × 5 convolutional filters.
Note that the early convolutional layers with high spatial
resolutions (e.g., 150×100) often capture more local details
while the ones with low spatial resolutions (e.g., 18 × 12)
can capture more structure information with high-level semantics. We combine the local fine details and the highlevel structure information by cross-layer aggregation of
early fine layers and up-sampled deep layers. We transform
the coarse outputs (e.g., with resolution 18 × 12) to dense
outputs (e.g., with resolution 37 × 25) with up-sampling interpolation of factor 2. The feature maps up-sampled from
the low resolutions and those from the high resolutions are
then aggregated with the element-wise summation, shown
as the blue circle with plus in Figure 1. Note that we select the element-wise summation instead of other operations
(e.g. multiplication) by experimenting on the validation set.
After that, the following convolutional layers can be learned
based on the combination of coarse and fine information.
To capture more detailed local boundaries, the input image
is further filtered with the 5 × 5 convolutional filters and
then aggregated into the later feature maps. We perform the
cross-layer combination four times until obtaining the feature maps with the same size as the input image. Finally,
the convolutional layers are utilized to generate the C confidence maps to predict scores for C labels (including background) at each pixel location. Our loss function is the sum
of cross-entropy terms for all pixels in the output map.
1388
Skirt Dress Co-CNN
w/o global label
Co-CNN
skirt
upper-clothes
dress
Global image label
Figure 2. Comparison of label confidence maps between Co-CNN
and that without using global labels. By using the global image
label probabilities to guide feature learning, the confidence maps
for skirt and dress can be corrected.
3.2. Global Image-level Context
An auxiliary objective for multi-label prediction is used
after the intermediate layers with spatial resolution of 18 ×
12, as shown in the pentagon in Figure 1. Following the
fully-connected layer, the C-way softmax which produces
a probability distribution over the C class labels is appended. Squared loss is used during the global image label
prediction. Suppose for each image I in the training set,
y = [y1, y2, · · · , yC] is the ground-truth multi-label vector. yc = 1, (c = 1, · · · , C) if the image is annotated with
class c, and otherwise yc = 0. The ground-truth probability
vector is normalized as pc = ||y yc ||1 and the predictive probability vector is p ˆ = [ˆ p1, p ˆ2, · · · , p ˆC]. The squared loss to be
minimized is defined as J = PC c=1(pc−p ˆ c)2. During training, the loss of image-level labels is added to the total loss
of the network weighted by a discount factor 0.3. To utilize
the predicted global image label probabilities, we perform
two types of combination: concatenating the predicted label probabilities with the intermediate convolutional layers
(image label concatenation in Figure 1) and element-wise
summation with label confidence maps (element-wise summation in Figure 1).
First, consider that the feature maps of the m-th convolutional layer are a three-dimensional array of size hm ×
wm × dm, where hm and wm are spatial dimensions, and
dm is the number of channels. We generate C additional
probability maps {xp c}C 1 with size hm × wm where each
x
p
i,j,c at location (i, j) is set as the predicted probability pc
of the c-th class. By concatenating the feature maps xm of
the m-th layer and the probability maps {xp c}C 1 , we generate the combined feature maps x ˆm = [xm, xp 1, xp 2, · · · , xp C]
of the size hm × wm × (dm + C). The outputs xm i,j+1 at
input Co-CNN
w/o sp Co-CNN input Co-C w/o sp NN Co-CNN
Figure 3. Comparison of example results of using local superpixel contexts. For each image, we show the results from Co-CNN
and “Co-CNN w/o sp”, i.e. no local super-pixel information used.
location (i, j) in the next layer are computed by
xm+1
i,j = fk({x ˆm i+δi,j+δj}0≤δi,δj≤k), (1)
where k is the kernel size, and fk is the corresponding convolution filters. We perform this concatenation after each
combination of coarse and fine layers in Section 3.1, as
shown in Figure 1.
Second, we element-wisely sum the predicted confidence maps with the global image label probabilities. If
the class c has a low probability of appearing in the image, the corresponding pixel-wise probability will be suppressed. Given the probability ri,j,c of the c-th confidence
map at location (i, j), the resulting probability r ˆi,j,c is calculated by r ˆi,j,c = ri,j,c + ˆ pc for the c-th channel. The
incorporation of global image-level context into label confidence maps can help reduce the confusion of competing
labels.
3.3. Local Super-pixel Context
We further integrate the within-super-pixel smoothing
and the cross-super-pixel neighborhood voting into the
training and testing process to respect the local detailed information. They are only performed on the prediction layer
(i.e. C confidence maps) instead of all convolutional layers.
It is advantageous that super-pixel guidance is used at the
later stage, which avoids making premature decisions and
thus learning unsatisfactory convolution filters.
Within-super-pixel Smoothing: For each input image
I, we first compute the over-segmentation of I using the
entropy rate based segmentation algorithm [17] and obtain 500 super-pixels per image. Given the C confidence
maps {xc}C 1 in the prediction layer, the within-super-pixel
smoothing is performed on each map xc. Let us denote the
super-pixel covering the pixel at location (i, j) by sij, the
smoothed confidence maps x ˜c can be computed by
x ˜i,j,c =
1
||sij||
X
(i′,j′)∈sij
xi′,j′,c, (2)
where ||sij|| is the number of pixels within the super-pixel
sij and (i′, j′) represents all pixels within sij.
1389
Cross-super-pixel Neighborhood Voting: After
smoothing confidences within each super-pixel, we can
take the neighboring larger regions into account for better
inference, and exploit more statistical structures and
correlations between different super-pixels. For classes
with non-uniform appearance (e.g., the common clothes
items), the inference within larger regions may better
capture the characteristic distribution for this class. For
simplicity, let x ˜s, x ˜s′ denote the smoothed responses of the
super-pixel s and s′ on each confidence map, respectively.
For each super-pixel s, we first compute a concatenation
of bag-of-words from RGB, Lab and HOG descriptor for
each super-pixel, and the feature of each super-pixel can be
denoted as bs. The cross neighborhood voted response x ¯s
of the super-pixel s is calculated by
x ¯s = (1 − α)˜ xs + α X
s′∈Ds
exp(−||bs − bs′||2)
Pˆ s∈Ds exp(−||bs − bˆ s||2)x ˜s′.
(3)
Here, Ds denotes the neighboring super-pixel set of the
super-pixel s. We weight the voting of each neighboring super-pixel s′ with the normalized appearance similarities. If the pair of super-pixels (s, s′) shares higher appearance similarity, the corresponding weight of neighborhood voting will be higher. Our within-super-pixel smoothing and cross-super-pixel neighborhood voting can be seen
as two types of pooling methods, which are performed on
the local responses within the irregular regions depicted by
super-pixels. When back-propagating through the network,
the gradients are back-propagated through each super-pixel.
Some results with/without incorporating the local superpixel contexts are shown in Figure 3.
3.4. Parameter details of Co-CNN
Our detailed Co-CNN configuration is listed in Table 1.
We use the small 3×3 and 5×5 receptive fields throughout
the whole network, and the non-linear rectification layers
after every convolutional layer. The network has 21 layers
if only the layers with parameters are counted, or 27 layers
if we also count max pooling and up-sampling. The dropout
(30%) of fully-connected layer in the image-level label prediction is set by the validation set.
4. Experiments
4.1. Experimental Settings
Dataset: We evaluate the human parsing performance
of our Co-CNN on the large ATR dataset [15] and the small
Fashionista dataset [31]. Human parsing is to predict every
pixel with 18 labels: face, sunglass, hat, scarf, hair, upperclothes, left-arm, right-arm, belt, pants, left-leg, right-leg,
skirt, left-shoe, right-shoe, bag, dress and null. Totally,
Table 1. The detailed configuration of our Co-CNN.
component type kernel size/stride output size
convolution 5 × 5/1 150 × 100 × 128
convolution 5 × 5/1 150 × 100 × 192
max pool 3 × 3/2 75 × 50 × 192
convolution 5 × 5/1 75 × 50 × 192
convolution 5 × 5/1 75 × 50 × 192
local-to-global max pool 3 × 3/2 37 × 25 × 192
convolution 5 × 5/1 37 × 25 × 192
convolution 5 × 5/1 37 × 25 × 192
max pool 3 × 3/2 18 × 12 × 192
convolution 5 × 5/1 18 × 12 × 192
convolution 5 × 5/1 18 × 12 × 192
convolution 1 × 1/1 18 × 12 × 96
image-level label FC (dropout 30%) 1 × 1 × 1024
prediction FC 1 × 1 × 18
Squared Loss 1 × 1 × 18
upsampling 2 × 2/2 37 × 25 × 192
convolution 5 × 5/1 37 × 25 × 192
element sum 37 × 25 × 192
concat 37 × 25 × 210
convolution 5 × 5/1 37 × 25 × 192
upsampling 2 × 2/2 75 × 50 × 192
convolution 3 × 3/1 75 × 50 × 192
element sum 75 × 50 × 192
global-to-local concat 75 × 50 × 210
convolution 5 × 5/1 75 × 50 × 192
upsampling 2 × 2/2 150 × 100 × 192
convolution 5 × 5/1 150 × 100 × 192
element sum 150 × 100 × 192
concat 150 × 100 × 210
convolution 5 × 5/1 150 × 100 × 192
convolution (image) 5 × 5/1 150 × 100 × 192
element sum 150 × 100 × 192
convolution 3 × 3/1 150 × 100 × 256
convolution 1 × 1/1 150 × 100 × 18
prediction element sum 150 × 100 × 18
convolution 1 × 1/1 150 × 100 × 18
within-S-P smoothing 150 × 100 × 18
super-pixel cross-S-P voting 150 × 100 × 18
Softmax Loss 150 × 100 × 18
7,700 images are included in the ATR dataset [15], 6,000
for training, 1,000 for testing and 700 for validation1. The
Fashionista dataset contains 685 images, in which 229 images are used for testing and the rest for training. We use
the Fashionista dataset after transforming the original labels
to 18 categories as in [15] for fair comparison. We use the
same evaluation criterion as in [30] and [15], including accuracy, average precision, average recall, and average F-1
score over pixels. The images in these two datasets are near
frontal-view and have little cluttered background, and are
insufficient for real-world applications with arbitrary postures, views and backgrounds. We collect 10,000 real-world
human pictures from a social network, chictopia.com, to
construct a much larger dataset “Chictopia10k”, and annotate pixel-level labels following [15]. Our new dataset
mainly contains images in the wild (e.g., more challenging
poses, occlusion and clothes), which will be released upon
publication to promote future research on human parsing.
Implementation Details: We augment the training images with the horizontal reflections, which improves about
4% in terms of F-1 scores. Given a test image, we use
the human detection algorithm [10] to detect the human
1We sincerely thank the authors of [15] for sharing the dataset.
1390
Table 2. Comparison of human parsing performances with several architectural variants of our model and four state-of-the-arts when
evaluating on ATR [15]. The ⋆ indicates the method is not a fully end-to-end framework.
Method Accuracy F.g. accuracy Avg. precision Avg. recall Avg. F-1 score
⋆ Yamaguchi et al. [31] 84.38 55.59 37.54 51.05 41.80
⋆ PaperDoll [30] 88.96 62.18 52.75 49.43 44.76
⋆M-CNN [21] 89.57 73.98 64.56 65.17 62.81
⋆ ATR [15] 91.11 71.04 71.69 60.25 64.38
baseline (150-75) 92.77 68.66 67.98 62.85 63.88
baseline (150-75-37) 92.91 76.29 78.48 65.42 69.32
baseline (150-75-37-18) 94.41 78.54 76.62 71.24 72.72
baseline (150-75-37-18, w/o fusion) 92.57 70.76 67.17 64.34 65.25
Co-CNN (concatenate with global label) 94.90 80.80 78.35 73.14 74.56
Co-CNN (summation with global label) 94.28 76.43 79.62 71.34 73.98
Co-CNN (concatenate, summation with global label) 94.87 79.86 78.00 73.94 75.27
Co-CNN (w-s-p) 95.09 80.50 79.22 74.38 76.17
Co-CNN (full) 95.23 80.90 81.55 74.42 76.95
Co-CNN (+Chictopia10k) 96.02 83.57 84.95 77.66 80.14
Table 3. Per-Class Comparison of F-1 scores with several variants of our versions and four state-of-the-art methods on ATR [15].
Method Hat Hair S-gls U-cloth Skirt Pants Dress Belt L-shoe R-shoe Face L-leg R-leg L-arm R-arm Bag Scarf
⋆ Yamaguchi et al. [31] 8.44 59.96 12.09 56.07 17.57 55.42 40.94 14.68 38.24 38.33 72.10 58.52 57.03 45.33 46.65 24.53 11.43
⋆ PaperDoll [30] 1.72 63.58 0.23 71.87 40.20 69.35 59.49 16.94 45.79 44.47 61.63 52.19 55.60 45.23 46.75 30.52 2.95
⋆M-CNN [21] 80.77 65.31 35.55 72.58 77.86 70.71 81.44 38.45 53.87 48.57 72.78 63.25 68.24 57.40 51.12 57.87 43.38
⋆ ATR [15] 77.97 68.18 29.20 79.39 80.36 79.77 82.02 22.88 53.51 50.26 74.71 69.07 71.69 53.79 58.57 53.66 57.07
baseline (150-75) 28.94 81.96 63.04 74.71 50.91 70.18 53.87 37.32 64.87 60.49 86.02 72.55 72.40 78.54 72.43 63.94 18.86
baseline (150-75-37) 63.12 80.08 36.55 83.12 63.17 81.10 65.38 28.36 65.75 69.94 82.88 82.03 81.55 75.68 76.31 77.36 37.15
baseline (150-75-37-18) 59.41 84.67 69.59 82.75 65.52 80.30 65.29 43.50 75.85 72.71 88.00 85.11 84.35 80.61 80.27 72.25 22.87
baseline (150-75-37-18, w/o fusion) 57.93 79.15 54.01 78.08 65.27 73.25 50.73 20.63 63.00 63.57 82.48 68.20 73.02 73.39 73.37 72.79 27.05
Co-CNN (concatenate with global label) 62.96 85.09 70.42 84.20 70.36 83.02 70.67 45.71 74.26 74.23 88.14 87.09 85.99 81.94 80.73 73.91 24.39
Co-CNN (summation with global label) 69.77 87.91 78.05 79.31 61.81 80.53 57.51 28.16 74.87 73.22 91.34 82.15 83.98 84.37 84.23 79.78 35.35
Co-CNN (concatenate, summation with global label) 65.05 85.11 70.92 84.02 73.20 81.49 69.61 45.44 73.59 73.40 88.73 83.25 83.51 82.74 82.15 77.88 35.75
Co-CNN (w-s-p) 71.25 85.52 71.37 84.70 74.98 82.23 71.18 46.28 74.83 75.04 88.76 84.39 83.38 82.84 82.62 78.97 33.66
Co-CNN (full) 72.07 86.33 72.81 85.72 70.82 83.05 69.95 37.66 76.48 76.80 89.02 85.49 85.23 84.16 84.04 81.51 44.94
Co-CNN (+Chictopia10k) 75.88 89.97 81.26 87.38 71.94 84.89 71.03 40.14 81.43 81.49 92.73 88.77 88.48 89.00 88.71 83.81 46.24
body. The resulting human centric image is then rescaled
into 150×100 and fed into our Co-CNN for pixel-wise prediction. We choose the resolution of 150× 100 for each image, to balance computational efficiency, practicality (e.g.,
GPU memory) and accuracy. To evaluate the performance,
we re-scale the output pixel-wise prediction back to the size
of the original ground-truth labeling. All models in our experiment are trained and tested based on Caffe [13] on a
single NVIDIA Tesla K40c. We set the weight parameter
α in cross-super-pixel voting as 0.3 by using the validation
set. The network is trained from scratch using the annotated
training images. The weights of all network parameters are
initialized with Gaussian distribution with standard deviation as 0.001. We train Co-CNN using stochastic gradient
descent with a batch size of 12 images, momentum of 0.9,
and weight decay of 0.0005. The learning rate is initialized
at 0.001 and divided by 10 after 30 epochs. We train the networks for roughly 90 epochs, which takes 4 to 5 days. Our
Co-CNN can rapidly process one 150 × 100 image within
about 0.0015 second. After incorporating the super-pixel
extraction [17], we test one image within about 0.15 second.
This compares much favorably to other state-of-the-art approaches, as current state-of-the-art approaches have higher
complexity: [30] runs in about 10 to 15 seconds, [6] runs
in 1 to 2 minutes and [15] runs in 0.5 second.
Figure 4. Exemplar images of our “Chictopia10k” dataset.
4.2. Results and Comparisons
We compare our proposed Co-CNN with five state-ofthe-art approaches [31] [30] [21] [15] [25] on two datasets.
All results of the competing methods and our methods are
obtained by using the same training and testing setting described in the paper [15].
ATR dataset [15]: Table 2 and Table 3 show the performance of our models and comparisons with four stateof-the-arts on overall metrics and F-1 scores of foreground
semantic labels, respectively. Our “Co-CNN (full)” can
1391
Table 4. Comparison of parsing performance with three state-ofthe-arts on the test images of Fashionista [31].
.
Method Acc, F.g. acc. Avg. prec. Avg. recall Avg. F-1 score
⋆ Yamaguchi et al. [31] 87.87 58.85 51.04 48.05 42.87
⋆ PaperDoll [30] 89.98 65.66 54.87 51.16 46.80
⋆ ATR [15] 92.33 76.54 73.93 66.49 69.30
Co-CNN (full) 96.08 84.71 82.98 77.78 79.37
Co-CNN (+Chictopia10k) 97.06 89.15 87.83 81.73 83.78
significantly outperform four baselines: 35.15% over Yamaguchi et al. [31], 32.19% over PaperDoll [30], 14.14%
over M-CNN [21] and 12.57% over ATR [15] in terms of
average F-1 score. Since the code of ATR [15] is not publicly available, we only take our “Chictopia10k” dataset as
the supplementary dataset to the training set and report the
results as “Co-CNN (+Chictopia10K)”. After training with
more realistic images in our newly collected dataset “Chictopia10k”, our “Co-CNN (+Chictopia10k)” can further improve the average F-1 score by 3.19% and the average precision by 3.4%. This indicates that our “Chictopia10k”
dataset can introduce greater data diversity and improve the
network generality. We show the F-1 scores for each label
in Table 3. Generally, our Co-CNN shows much higher performance than other methods. In terms of predicting small
labels such as hat, belt, bag and scarf, our method achieves
a very large gain, e.g. 72.81% vs 29.20% [15] for sunglass,
81.51% vs 53.66% [15] for bag. We also achieve much
better performance on human body parts, e.g. 84.16% vs
53.79% [15] for left-arm. It demonstrates that Co-CNN performs very well on various poses (e.g. human body parts),
fine details (e.g. small labels) and diverse clothing styles.
Fashionista dataset [31]: Table 4 gives the comparison
results on the 229 test images of the Fashionista dataset. All
results of the state-of-the-art methods were reported in [15].
Note that deep learning based algorithm requires enough
training samples. Following [15], we only report the performance by training on the same large ATR dataset[15], and
then testing on the 229 images on Fashionista dataset. Our
method “Co-CNN (full)” can substantially outperform the
baselines by 10.07%, 32.57% and 36.5% over “ATR [15]”,
“PaperDoll [30]” and “Yamaguchi et al. [31]” in terms of
average F-1 score, respectively. We cannot compare all metrics with the CRF model proposed in [25], since it only reported the average pixel-wise accuracy, and only achieved
84.88%, which only slightly improved the results 84.68%
of PaperDoll [30] on Fashionista, as reported in [25].
The qualitative comparison of parsing results is visualized in Figure 5. Our Co-CNN outputs more meaningful
and precise predictions than PaperDoll [30] and ATR [15]
despite the large appearance and position variations.
4.3. Discussion on Our Network
We further evaluate the different network settings for our
three components, presented in Table 2 and Table 3.
Local-to-Global-to-Local Hierarchy: We explore different variants of our basic network structure. Note that all
the following results are obtained without combining the
global image-level label context and the local super-pixel
contexts. First, different down-sampled spatial resolutions
are tested. The “baseline (150-75)”, “baseline (150-75-37)”
and “baseline (150-75-37-18)” are the versions with downsampling up to 75 × 50, 37 × 25 and 18 × 12, respectively.
When only convolving the input image with two resolutions
(“baseline (150-75)”), the performance is worse than the
state-of-the-arts [15]. After further increasing the depth of
the network by down-sampling up to 37 × 25 (“baseline
(150-75-37)”), the F-1 score can be significantly increased
by 5.44%, compared to “baseline (150-75)”. The “baseline
(150-75-37-18)” can further improve the F-1 score by 3.4%,
compared to “baseline (150-75-37)”. We do not report results by further down-sampling the feature maps since only
slight improvement is achieved with smaller resolutions.
Second, we also evaluate the effectiveness of the crosslayer context combination. The “baseline (150-75-37-18,
w/o fusion)” represents the version without cross-layer
combinations. The large decrease 7.47% in F-1 score compared with the “baseline (150-75-37-18)” demonstrates the
great advantage of the cross-layer combination. Combining the cross-layer information enables the network to make
precise local predictions and respect global semantic information.
Finally, we also test the FCN architecture [22] on semantic segmentation in the human parsing task, i.e., fine-tuning
the pre-trained classification network with the human parsing dataset and only performing the combination for the
pixel-wise predictions. Its performance is much worse than
our network (i.e. 64.63% vs 72.72% of “baseline (150-75-
37-18)” in average F-1 score).
Global Image-level Context: We also explore different architectures to demonstrate the effectiveness of utilizing the global image label context. All the following
results are obtained without using local super-pixel contexts. After the summation of global image label probabilities (“Co-CNN (summation with global label)”), the performance can be increased by 1.26%, compared to “baseline (150-75-37-18)”. After concatenating the global image label probabilities with each subsequent convolutional
layer, “Co-CNN (concatenate with global label)”, the performance can be improved by 1.84% in F-1 score, compared to the version without using global label (“baseline
(150-75-37-18)”). The further summation of global image
label probabilities can bring 0.71% increase in F-1 score,
shown as “Co-CNN (concatenate, summation with global
label)”. The most significant improvements over “baseline
1392
Image PaperDoll ATR
up glass skirt scarf r-shoe r-arm pants l-shoe
l-leg l-arm hat face dress belt bag hair null
r-leg
Co-CNN (full) Image PaperDoll ATR Co-CNN (full) Image PaperDoll ATR Co-CNN (full)
Figure 5. Result comparison of our Co-CNN and two state-of-the-art methods. For each image, we show the parsing results by PaperDoll [30], ATR [15] and our Co-CNN sequentially.
(150-75-37-18)” can be observed from the F-1 scores for
clothing items, e.g., 7.68% for skirt and 4.32% for dress.
The main reason for these improvements may be that by accounting for the global image-level label probabilities, the
label exclusiveness and occurrences can be well captured
during dense pixel-wise prediction.
Local Super-pixel Contexts: Extensive evaluations are
conducted on the effectiveness of using local super-pixel
contexts. The average F-1 score increases by 0.9% by embedding the within-super-pixel smoothing into our network
(“Co-CNN (w-s-p)”), compared to the version “Co-CNN
(concatenate, summation with global label)”. Our full network “Co-CNN (full)” leads to 1.68% increase. For the F-1
score for each semantic label, the significant improvements
are obtained for the labels of small regions (e.g. hat, sunglasses and scarf). For instance, the F-1 score for hat is increased by 7.02%, and 9.19% for scarf, compared with “CoCNN (concatenate, summation with global label)”. This
demonstrates that the local super-pixel contexts can help
preserve the local boundaries and generate more precise
classification for small regions. Previous works ofter apply
the super-pixel smoothing as the post-processing step.
5. Conclusions and Future Work
In this work, we proposed a novel Co-CNN architecture for human parsing task, which integrates the crosslayer context, global image label context and local superpixel contexts into a unified network. For each input
image, our Co-CNN produces the correspondingly-sized
pixel-wise prediction in a full end-to-end way. The localto-global-to-local hierarchy is used to combine the local detailed information and global semantic information. The
global image label prediction, within-super-pixel smoothing and cross-super-pixel neighborhood voting are formulated as the natural components of our Co-CNN. Extensive experimental results clearly demonstrated the effectiveness of the proposed Co-CNN. A new large dataset “Chictopia10k” has been built. In the future, we will further extend our Co-CNN architecture for generic image parsing
tasks, e.g., object semantic segmentation and scene parsing.
Our online demo website will be released upon publication
to demonstrate the efficiency and effectiveness."
