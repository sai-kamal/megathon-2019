{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in ./q_3.7/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/kamal/q_3.7/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/home/kamal/q_3.7/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!python3 -m spacy download en\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# from gensim.summarization.summarizer import summarize\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# ps=SnowballStemmer('english')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = './summaries.csv'\n",
    "f = './fulltext.csv'\n",
    "\n",
    "# s = sys.argv[1]\n",
    "# f = sys.argv[2]\n",
    "\n",
    "ds = pd.read_csv(s)['abstract'].values.tolist()\n",
    "df = pd.read_csv(f)['paper text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts, stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def preprocessing(data):  # data is a list of sentences/abstracts\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "    data_words = list(sent_to_words(data))\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases. \n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words, stop_words)\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops, bigram_mod)\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    return data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = preprocessing(ds)\n",
    "df = preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = []\n",
    "# for i in range(len(df)):\n",
    "# #     print(df[i])\n",
    "#     x = '_'.join(df[i])\n",
    "#     x = summarize(x,words=500)\n",
    "#     print(x)\n",
    "#     new_df.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = ds + df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove stop words and punctuation followed by stemming\n",
    "# def foo_filter(sentence):\n",
    "#     sentence=sentence.lower()\n",
    "#     word_tokens=word_tokenize(sentence)\n",
    "#     word_tokens = [w.lower() for w in word_tokens if not (w in stop_words or w in string.punctuation)]\n",
    "# #     filtered_sentence = [ps.stem(w) for w in word_tokens]\n",
    "#     return word_tokens\n",
    "# #     return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter(\"This is a sample sentence, showing off the stop words filtration 10 .\")\n",
    "# filter_sent=[]\n",
    "# for i in data1:\n",
    "#     filter_sent.append(foo_filter(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# embed = hub.Module(\"./sentence_wise_email/module/module_useT\")\n",
    "# sent_messages = [' '.join(i) for i in filter_sent]# Reduce logging output.\n",
    "# just_sent_messages = [' '.join(i) for i in filter_just_sent]# Reduce logging output.\n",
    "# with tf.Session() as session:\n",
    "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#     sent_embeddings = session.run(embed(sent_messages))\n",
    "#     just_sent_embeddings = session.run(embed(just_sent_messages))\n",
    "# print(sent_embeddings[0],just_sent_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('./dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "from gensim.models import Word2Vec\n",
    "from fse.models import Average\n",
    "# from fse.models import Sentence2Vec\n",
    "# from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "# model = Doc2Vec(size=vec_size,\n",
    "#                 alpha=alpha, \n",
    "#                 min_alpha=0.00025,\n",
    "#                 min_count=1,\n",
    "#                 dm =1)\n",
    "\n",
    "\n",
    "model = Word2Vec(arr, min_count=0)\n",
    "se = Average(model)\n",
    "# sentences_emb = se.train(filter_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "givenSent=\"We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.\"\n",
    "filter_sent=foo_filter(givenSent)\n",
    "# given_emb=se.train(filter_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# max_i=-1\n",
    "# max_v=-1e9\n",
    "# for i in range(len(sentences_emb)):\n",
    "#     c=cosine_similarity([sentences_emb[i]],[given_emb[0]])\n",
    "#     if c[0][0] > max_v:\n",
    "#         max_v=c[0][0]\n",
    "#         max_i=i\n",
    "# print(max_i,data1[max_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similarity(score):\n",
    "    score*=1.5\n",
    "    if score > 1:\n",
    "        score=1\n",
    "    if score < -1:\n",
    "        score=-1\n",
    "    score=(score+1)/2\n",
    "    if score < 0.5:\n",
    "        score*=0.75\n",
    "    return score\n",
    "\n",
    "# f_s0=foo_filter(s0)\n",
    "# f_s1=foo_filter(s1)\n",
    "# f_s3=foo_filter(s3)\n",
    "# f_s5=foo_filter(s5)\n",
    "# f_s7=foo_filter(s7)\n",
    "# # print(' '.join(foo_filter(s2)))\n",
    "\n",
    "# s2=' '.join(foo_filter(s2))\n",
    "# s4=' '.join(foo_filter(s4))\n",
    "# s6=' '.join(foo_filter(s6))\n",
    "# s8=' '.join(foo_filter(s8))\n",
    "# # print(s2,'\\n\\n',s4)\n",
    "# s2=summarizer.summarize(s2,words=500)\n",
    "# s4=summarizer.summarize(s4,words=500)\n",
    "# s6=summarizer.summarize(s6,words=len(s1.split(' ')))\n",
    "# s8=summarizer.summarize(s8,words=len(s1.split(' ')))\n",
    "# # print(s4)\n",
    "# # print(s1,'\\n\\n\\n',s2)\n",
    "# f_s2=foo_filter(s2)\n",
    "# f_s4=foo_filter(s4)\n",
    "# f_s6=foo_filter(s8)\n",
    "# f_s8=foo_filter(s8)\n",
    "# # sentences=[f_s0,f_s1,f_s2,f_s3,f_s4]\n",
    "# sentences=[f_s0,f_s1,f_s2,f_s3,f_s4,f_s5,f_s6,f_s7,f_s8]\n",
    "# emb=se.train(sentences)\n",
    "# print(emb.shape)\n",
    "# emb_f_s0=[emb[0,:]]\n",
    "# emb_f_s1=[emb[1,:]]\n",
    "# emb_f_s2=[emb[2,:]]\n",
    "# emb_f_s3=[emb[3,:]]\n",
    "# emb_f_s4=[emb[4,:]]\n",
    "# emb_f_s5=[emb[5,:]]\n",
    "# emb_f_s6=[emb[6,:]]\n",
    "# emb_f_s7=[emb[7,:]]\n",
    "# emb_f_s8=[emb[8,:]]\n",
    "\n",
    "\n",
    "# c02=cosine_similarity(emb_f_s0,emb_f_s2)\n",
    "# c12=cosine_similarity(emb_f_s1,emb_f_s2)\n",
    "# c32=cosine_similarity(emb_f_s3,emb_f_s2)\n",
    "# c52=cosine_similarity(emb_f_s5,emb_f_s2)\n",
    "# c72=cosine_similarity(emb_f_s7,emb_f_s2)\n",
    "\n",
    "# c04=cosine_similarity(emb_f_s0,emb_f_s4)\n",
    "# c14=cosine_similarity(emb_f_s1,emb_f_s4)\n",
    "# c34=cosine_similarity(emb_f_s3,emb_f_s4)\n",
    "# c54=cosine_similarity(emb_f_s5,emb_f_s4)\n",
    "# c74=cosine_similarity(emb_f_s7,emb_f_s4)\n",
    "\n",
    "# # print(get_similarity(c02[0][0]),get_similarity(c12[0][0]),get_similarity(c32[0][0]))\n",
    "# print(get_similarity(c02[0][0]),get_similarity(c12[0][0]),get_similarity(c32[0][0]),get_similarity(c52[0][0]),get_similarity(c72[0][0]))\n",
    "# # print(get_similarity(c04[0][0]),get_similarity(c14[0][0]),get_similarity(c34[0][0]))\n",
    "# print(get_similarity(c04[0][0]),get_similarity(c14[0][0]),get_similarity(c34[0][0]),get_similarity(c54[0][0]),get_similarity(c74[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "q_3.7",
   "language": "python",
   "name": "q_3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
